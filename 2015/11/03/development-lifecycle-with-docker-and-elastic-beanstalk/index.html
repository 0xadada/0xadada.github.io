<!DOCTYPE html><html lang="en-US"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="https://assets-cdn.github.com/images/icons/emoji/unicode/1f6a2.png"/><link rel="stylesheet" href="/_next/static/css/362901d03019bc4c.css" crossorigin="" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/01f63581d77a7b07.css" crossorigin="" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-85364f91ad1195ff.js" crossorigin=""/><script src="/_next/static/chunks/fd9d1056-cc48c28d170fddc2.js" async="" crossorigin=""></script><script src="/_next/static/chunks/69-0a6331c18b0d37aa.js" async="" crossorigin=""></script><script src="/_next/static/chunks/main-app-715e3a652bc6b546.js" async="" crossorigin=""></script><script src="/_next/static/chunks/250-d7e0a94ebe194dac.js" async=""></script><script src="/_next/static/chunks/app/page-84b663e6d1625d16.js" async=""></script><script src="/_next/static/chunks/749-1aefd436964833c3.js" async=""></script><script src="/_next/static/chunks/app/layout-18f161c3adf8f2fa.js" async=""></script><title>Development Lifecycle with Docker and Elastic Beanstalk</title><meta name="description" content="This article explains the advantages of using Docker over automated configuration management tools, and describes a workflow from development through QA and into production deployment using Amazons&#x27; Elastic Beanstalk."/><link rel="author" href="https://0xadada.pub"/><meta name="author" content="0xADADA"/><meta name="keywords" content="essays,software-engineering"/><link rel="alternate" type="application/rss+xml" href="https://0xadada.pub/rss.xml"/><meta property="og:title" content="Development Lifecycle with Docker and Elastic Beanstalk"/><meta property="og:description" content="This article explains the advantages of using Docker over automated configuration management tools, and describes a workflow from development through QA and into production deployment using Amazons&#x27; Elastic Beanstalk."/><meta property="og:url" content="https://0xadada.pub/2015/11/03/development-lifecycle-with-docker-and-elastic-beanstalk/"/><meta property="og:site_name" content="0xADADA"/><meta property="og:locale" content="en_US"/><meta property="og:image" content="https://0xadada.pub/static/images/meta/avatar.svg"/><meta property="og:image:width" content="660"/><meta property="og:image:height" content="660"/><meta property="og:type" content="article"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Development Lifecycle with Docker and Elastic Beanstalk"/><meta name="twitter:description" content="This article explains the advantages of using Docker over automated configuration management tools, and describes a workflow from development through QA and into production deployment using Amazons&#x27; Elastic Beanstalk."/><meta name="twitter:image" content="https://0xadada.pub/static/images/meta/avatar.svg"/><meta name="twitter:image:width" content="660"/><meta name="twitter:image:height" content="660"/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="48x48"/><link rel="apple-touch-icon" href="/apple-icon.png?b764b3a1dbf00a82" type="image/png" sizes="180x180"/><script src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js" crossorigin="" noModule=""></script></head><body><main class="layout_layout__dCqca"><article class="hentry h-entry"><header><h1 class="entry-title p-name">Development Lifecycle with Docker and Elastic Beanstalk</h1><time class="display-date_published__A_L_d dt-published" dateTime="2015-11-03T00:00:00.000Z">Tuesday November 3, 2015</time><span class="h-card page_byline__wbXsN"> by <span class="fn p-author p-name">0xADADA</span></span></header><div class="entry-content e-content"><p>Docker is getting a lot of hype these days, for good reason. There are plenty of
articles touting the merits of Docker but most are written without context and
are limited to examining the the benefits of Docker independently of its’
practical everyday use in a software development project lifecycle.</p>
<p>This article aims to examine the benefits of Docker within the context of a
software company with multiple developers working on multiple projects, having
to manage these projects deployed in a cloud production environment. In this
context the benefits of Docker become more readily apparent.</p>
<p>With the rise of cloud computing, the number of systems that need to be
maintained has exploded. Manual <em><a href="#provisioning">provisioning</a></em> of an
increasingly large number of systems becomes impossible for a small team, given
platforms like Amazon EC2 provide <em><a href="#autoscaling">auto-scaling</a></em> when additional
load is detected.</p>
<p>Tools like Ansible, Chef, Puppet and Salt are great solutions towards achieving
automated provisioning of virtual machines. The industry has responded by
quickly adopting these tools, but even more agility and performance can be
achieved by using <em><a href="#docker-container">Docker containers</a></em>.</p>
<p>Combining Docker with a deployment tool like AWS Elastic Beanstalk can provide
even greater efficiencies for developing and deploying cloud applications.</p>
<h2 id="dockdj">Dockdj</h2>
<figure class="rehype-figure"><img src="https://assets-cdn.github.com/images/icons/emoji/unicode/1f6a2.png" alt="Dockdj"/><figcaption>Dockdj</figcaption></figure>
<p>This article will be using <a href="https://github.com/0xadada/dockdj">Dockdj</a> to
illustrate using Docker and Elastic Beanstalk in the context of a real-world web
project. Dockdj is a recipe for building 12-factor Python / Django web apps with
multi-container Docker and deploying to Amazon AWS using Elastic Beanstalk.
<a href="https://github.com/0xadada/dockdj">Dockdj is available on GitHub</a>.</p>
<h3 id="manual-provisioning">Manual Provisioning</h3>
<p>The naive approach is <em>manual provisioning</em>: the developer installs Apache and
associated system libraries directly on the local development machine, configure
it according to the WordPress documentation.</p>
<p>These manual steps will need to be repeated for every additional member of the
development team, and again for the production web server. When provisioned
software is updated or configurations change. All members of the development
team and the production systems need to be updated accordingly. Larger teams
inevitably begin experiencing the <em>“works on my machine”</em> problems between
developers when some developers haven’t updated their configurations to match
coworkers who have.</p>
<p>Manual provisioning quickly becomes a frequent and resource-intensive process,
with the side-effect of prolonging the deployment of important vendor bug fixes
and security patches across both development and productions systems.</p>
<p>Additionally, when increased traffic hits productions systems, new systems need
to be allocated and scaled horizontally to support the new traffic. All the
provisioning needs to repeated. This system doesn’t scale as more production
servers are added to serve additional traffic load.</p>
<p>Even worse is when differences between developer-systems and production-systems
result in hard-to-reproduce bugs once the app is deployed from development into
production.</p>
<h3 id="automated-provisioning">Automated Provisioning</h3>
<p>An improvement over manual provisioning is <em>automated provisioning</em> using a
<em><a href="#confg-mgmt">configuration management</a></em> tool like Ansible, Chef, Puppet, Salt,
etc. These tools have been developed to address the problems of provisioning at
large scale.</p>
<p>The aim of these tools are:</p>
<ol>
<li>Initialize and start virtual machines</li>
<li>Automate the provisioning process in a repeatable way</li>
<li>Manage changes to provisioning in a version control system</li>
<li>Establishing and maintaining consistency of system dependencies and
configuration throughout an applications life</li>
</ol>
<p>Configuration management tools are wonderful for automated provisioning, but <em>in
practice</em><sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="true" aria-describedby="footnote-label">1</a></sup> they tend to
split management of the stack-app into two parts:</p>
<ol>
<li>the software Stack</li>
</ol>
<ul>
<li>Operating system</li>
<li>System libraries</li>
<li>Provisioned software</li>
<li>Configuration</li>
</ul>
<ol>
<li>the Application</li>
</ol>
<ul>
<li>Source code &amp; binaries</li>
<li>Dependencies</li>
<li>Runtime environment</li>
</ul>
<p>The result is that the stack (#1) is initially allocated and provisioned using
one of the configuration management tools. The application (#2) is then deployed
on the stack— resulting in a running application. When subsequent
application versions (#2) are released and deployed, they are deployed onto the
(unchanged) stack. <strong>The problem with this model is that the stack and the
application are managed independently.</strong> Changes to the stack are managed as a
unit separate from changes to the application. No data is recorded that
describes the compatibility of the integrated whole.</p>
<p>This results in increased complexity during rollbacks or simultaneous updates to
both stack and application. More importantly <strong>version numbers of the
application are not tied to versions of the stack.</strong></p>
<p>Under this model, the stack version and application version aren’t
coupled— which increases the likelihood of integration failures.</p>
<p>An example will illustrate where this model will fail:</p>
<blockquote>
<p>Our production web server is provisioned with Apache 3.3.0 and the application
(WordPress) was at version 0.7.0 last week, and have just released version
0.8.0 this past week.</p>
<p>Apache announces a security vulnerability fix at version 3.3.1. Under the
(typical) automated provisioning model, the configuration management tool
would be updated to provision the new version of Apache. The tool runs against
all production server systems. Here the application doesn’t change, it simply
rides on top of the Apache stack without change. No problems occur with the
rollout of the new Apache release.</p>
<p>Next the application updates and releases a new version for deployment- 0.9.0.
The deploy process runs, and for some reason the application fails, it isn’t
compatible with version 3.3.1 of Apache.</p>
<p>The decision is made to rollback the application to 0.8.0, which runs
successfully with Apache 3.3.1. The system is working again.</p>
<p>A critical security vulnerability is discovered in application 0.8.0 and the
decision is made to roll application back to version 0.7.0. (Keep in mind the
previous app version 0.7.0 was running Apache 3.3.0, and the stack is
currently 3.3.1).</p>
<p>The application fails— because 0.7.0 was never integration tested
against Apache 3.3.1. What do you do?</p>
</blockquote>
<p>In this example the devops team <strong>failed to remember</strong> to rollback Apache,
simply because the integrated dependencies were not internally coupled. The
compatible coupling existed <strong>only as institutional knowledge</strong> outside the
scope of the configuration management system, as Stack and Application were
managed separately.</p>
<h2 id="docker-for-configuration-management">Docker for Configuration Management</h2>
<p>One major advantage of Docker is that it does not necessitate running a unique
VM<sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="true" aria-describedby="footnote-label">2</a></sup> for every project a team works on.
If developers work on multiple projects, each with its own customized VM,
switching between projects becomes a time-consuming context shift for
developers.</p>
<p>Docker containers run directly on the Linux operating system and yet each
container is isolated. This eliminates the slowness of booting and the overhead
of a VM. Docker containers start up as quickly as running a normal process, and
eliminate VM “booting” for every Docker project the developer works on.
Deploying changes to the environment for every developer working on the project
is as easy as publishing a new <em><a href="#docker-image">Docker image</a></em>. Next time a
developer starts the container, he/she will get the new image.</p>
<p>Another advantage of Docker over an automated configuration management tool is
that it <strong>does not</strong> bifurcate the stack and the application into independent
segments.</p>
<p>A stack using Docker containers has the same benefit of configuration
management, but can couple the stack and the application into a single managed
component. The application is deployed along with its stack— and the
complete stack-app component is deployed together as a single Docker image or a
bundle of Docker images that have already been integration tested at least on a
developers machine.</p>
<p>As opposed the the “automated provisioning” model, the Docker model of the
stack-app looks more like this:</p>
<ol>
<li>Set of Docker images</li>
</ol>
<ul>
<li>Operating system (the software Stack)</li>
<li>Provisioned software (the software Stack)</li>
<li>Configuration (the software Stack)</li>
<li>System libraries (the software Stack)</li>
<li>Source code &amp; binaries (Application)</li>
<li>Dependencies (Application)</li>
<li>Runtime environment (Application)</li>
</ul>
<p>With every deployment, the entire stack-app (1) will be deployed. Docker uses
hashes (like Git) to minimize the amount of data that will be downloaded for any
update. This means only the differences are downloaded rather than the entire
stack.</p>
<p><strong>The greatest advantage of using Docker is that developers can run the
application in the very same environment as production</strong>. According to
<a href="http://12factor.net/dev-prod-parity">Twelve-Factor Methodology</a> this is called
achieving “Dev/Prod Parity”. This is a huge benefit in that it eliminates an
entire class of bugs that result from differences between
developers-and-developers as well as bugs that result from differences between
developers-and-production.</p>
<h2 id="elastic-beanstalk-for-deployment">Elastic Beanstalk for Deployment</h2>
<p>If you know <a href="https://www.heroku.com/">Heroku</a>, than Amazons’ Elastic Beanstalk
will be extremely familiar. EB borrows many ideas from Heroku, but the killer
feature is its’ ability to dynamically run, deploy and scale Docker containers
on a cluster of servers. It handles hardware allocation, network configuration,
load balancing, auto-scaling, health monitoring and rolling deployments.</p>
<p>EB doesn’t do everything, but it’s good enough to adopt early and use until your
team understands its deployment use-cases more clearly and understands
limitations of EB and its trade-offs.</p>
<h2 id="django-specific-structure">Django specific structure</h2>
<p>The core structure of the Docker / Elastic Beanstalk app can be explained by
describing the directory structure.</p>
<p>These comments describe the application-specific file structure:</p>
<pre><code>.dockerignore
.ebextensions/
  01_envvars.config
.ebignore
.elasticbeanstalk/
Dockerrun.aws.json
.gitignore               # Describes which files git ignores
.bowerrc                 # Configures where web frontend dependencies live
.csslintrc.json          # Describes CSS syntax rules
.jshintrc                # Describes JavaScript syntax rules
bower.json               # Describes web frontend dependencies
gulpfile.js              # Describes app build and dev tasks
package.json             # Describes NPM dependencies
app/                     # Our python app
  apps/*                 # python app modules
  project/*              # App-specific settings
  dist/*                 # App static assets (served via Nginx)
bin/*
docker/
  django/
    dev/
      docker-compose.yml
      Dockerfile
    prod/
      docker-compose.yml
      Dockerfile
      gunicorn.conf.py   # Settings for production app-server
    start.sh             # Script to start app-server
  nginx/*                # Nginx config files
environments/            # Environment-specific settings
  dev/                   # Development-only environment settings
    .env                 # Actual environment vars (Excluded from git)
    .env.example         # Example environment vars
    Procfile             # Configures how Honcho starts app-servers
    requirements.txt     # Describes dev Python dependencies
  prod/                  # Development-only environment settings
    .env                 # Actual environment vars (Excluded from git)
    .env.example         # Example environment vars
    Procfile             # Configures how Honcho starts app-servers
    requirements.txt     # Describes prod Python dependencies
</code></pre>
<p>Some of these directories and files are described in more depth below:</p>
<h3 id="gitignore">.gitignore</h3>
<pre><code>environments/*/.env
node_modules
.elasticbeanstalk/*
!.elasticbeanstalk/*.cfg.yml
!.elasticbeanstalk/*.global.yml
# Built testing and static asset artifacts
app/dist
</code></pre>
<p>Files matching the name <code>environments/*/.env</code> contain sensitive information
(usernames, passwords, etc) about per-deployment environments that shouldn’t be
included in version control.</p>
<p>The <code>node_modules</code> directory and will be created when the developer installs NPM
packages. These are dependencies and should not be committed into the source
code repository.</p>
<p>The <code>.elasticbeanstalk/*</code> directory is excluded from Git because it contains
files that are generated by EB command-line during environment creation and
version deployment that shouldn’t be written to the repository. It also contains
temporary configuration files (written by the EB cli).</p>
<p>Both <code>!.elasticbeanstalk/*.cfg.yml</code> and <code>!.elasticbeanstalk/*.global.yml</code>
entries use the “NOT” operator to re- include themselves into the repo. These
files can be useful to have in version control, as they contain useful
environment configuration settings.</p>
<h3 id="bowerjson">bower.json</h3>
<p>Bower is a web frontend package management system. The application declares its
frontend dependencies in this file.</p>
<p>During docker image creation, these dependencies are installed.</p>
<h3 id="gulpfilejs">gulpfile.js</h3>
<p>Gulp.js is a task runner for Node.js. <code>gulpfile.js</code> defines common tasks and
utilities related to this application:</p>
<ul>
<li>Running code Syntax checking &amp; automated testing</li>
<li>SASS and CSS compilation and minification</li>
<li>Frontend asset building</li>
</ul>
<h3 id="packagejson">package.json</h3>
<p>NPM is a package management system for Node.js applications. <code>package.json</code> The
application declares its Node.js dependencies in this file.</p>
<h3 id="app">app/</h3>
<p>The <code>app/</code> directory contains all source code related to the Django python web
application.</p>
<h3 id="dockerdjangoprodgunicornconfpy">docker/django/prod/gunicorn.conf.py</h3>
<p>This project uses two application servers, <code>runserver_plus</code> during development
and <code>gunicorn</code> in production.</p>
<p>These are settings related to the Gunicorn application server. In production, a
more performant application is used, requiring this configuration file.</p>
<h3 id="dockernginxsites-enabledconf">docker/nginx/sites-enabled.conf</h3>
<p>On production systems, where nginx acts as a reverse-proxy for the Gunicorn web
application, we use Docker links to connect the two containers together. This
configuration is best for reducing latency. Inside our nginx config file, we can
use a named entry for the <code>proxy_pass</code> value to reference our Django application
server running in another container on port 8080.</p>
<pre><code># ...
location / {
  proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
  proxy_set_header Host $http_host;
  proxy_redirect   off;
  proxy_pass       http://django:8080;
  # ...
</code></pre>
<h3 id="dockerdjangostartsh">docker/django/start.sh</h3>
<pre><code>cd /var/app
# ...
honcho --procfile &quot;environments/$ENV_NAME/Procfile&quot; \
       --env &quot;environments/$ENV_NAME/.env&quot; start
</code></pre>
<p><code>start.sh</code> is used during both development and production as a single task to
bootstrap the application server. It uses the <code>honcho</code> task runner to start the
server according to a set of tasks in a <code>Procfile</code> for development and another
for production.</p>
<p>The <code>--env</code> parameter is used to pass environment variables sourced from the
environments subdirectory. At runtime, the <code>$ENV_NAME</code> variable will be set:
<code>dev</code> for development and <code>prod</code> for production. This way a separate Procfile
and separate set of environment variables are available to configure the modes
independently.</p>
<h3 id="environmentsdevprocfile">environments/dev/Procfile</h3>
<pre><code># The webserver: Python
webserver: cd app &amp;&amp; ./manage.py runserver_plus 0.0.0.0:8080
# The CDN assets emulation server
cdnserver: cd /var/app/app/dist &amp;&amp; python -m http.server 8010
</code></pre>
<p>During development, the Django <code>runserver_plus</code> application server interprets
Python, while a simple HTTP server serves assets (images, CSS, etc).</p>
<h3 id="environmentsprodprocfile">environments/prod/Procfile</h3>
<pre><code class="language-bash">webserver: cd app &amp;&amp; gunicorn \
  -c /etc/gunicorn/gunicorn.conf.py project.wsgi:application
</code></pre>
<p>In production, we use Gunicorn to serve the python application, so the only task
run is the gunicorn app server. Static assets aren’t handled here because Nginx
will be reverse-proxying the application and also serving static assets.</p>
<p>This decision was made because Nginx is optimized to serve static assets and it
will reduce load on the application server.</p>
<h3 id="environmentsdev--prodenvexample">environments/[dev | prod]/.env.example</h3>
<p>Both <code>environments/dev/.env.example</code> and <code>environments/prod/.env.example</code> are
committed into git to provide hints to developers that they should create a
<code>.env</code> file in the same directory as a place to store environment variables that
are passed into the application.</p>
<p>The <code>.env</code> file is excluded from the git repository as it contains sensitive
usernames passwords and cryptographic information.</p>
<h3 id="environmentsdev--prodrequirementstxt">environments/[dev | prod]/requirements.txt</h3>
<p>The python utility <code>pip</code> is a package management utility. It uses a file named
<code>requirements.txt</code> to install all package dependencies. The development
environment adds some useful debugging utilities that shouldn’t be included on
production systems, so production has its own file.</p>
<p>During docker image creation, these dependencies are installed.</p>
<h2 id="docker-specific-structure">Docker specific structure</h2>
<pre><code>.dockerignore            # Describes which files Docker ignores
.ebextensions/
  01_envvars.config
.ebignore
.elasticbeanstalk/
Dockerrun.aws.json
.gitignore
.bowerrc
.csslintrc.json
.jshintrc
bower.json
gulpfile.js
package.json
app/
  apps/*
  project/*
  dist/*
bin/*                    # Scripts for Docker, build and deployment
docker/                  # Configuration files required by docker
  django/                # Django related settings
    dev/                 # Development-only settings
      docker-compose.yml # Orchestrates dev containers
      Dockerfile         # Builds dev Docker image
    prod/                # Production-only settings
      docker-compose.yml # Orchestrates prod containers
      Dockerfile         # Builds prod Docker image
      gunicorn.conf.py
    start.sh
  nginx/*
environments/
  dev/
    .env
    .env.example
    Procfile
    requirements.txt
  prod/
    .env
    .env.example
    Procfile
    requirements.txt
</code></pre>
<h3 id="dockerignore">.dockerignore</h3>
<p>The <code>.dockerignore</code> file specifies a list of patterns to exclude from the build
context during creation of the Docker image. These files are not required by the
execution of the container, and should be removed to reduce the size of the
final image.</p>
<pre><code>.coverage
.ebextensions/*
.elasticbeanstalk/*
.ebignore
.dockerignore
.git
.gitignore
.DS_Store
node_modules
app/dist
docs
htmlcov
README.md
ghostdriver.log
</code></pre>
<p>Some notable entries are described below:</p>
<p>The <code>node_modules</code> directory and will be created when the developer installs NPM
packages. If the developer is using on OS X, packages compiled on OS X will not
work when the container is running in the Linux VM, so this entry ensures that
node modules are installed on the host OS independently from the container OS.</p>
<p><code>.ebextensions/*</code>, <code>.elasticbeanstalk/*</code> and <code>.ebignore</code> are required by the
Elastic Beanstalk deploy process, and are outside the scope of execution of the
Docker container, and are not required.</p>
<p>The <code>app/dist</code> directory contains frontend assets served by both Django and
Nginx, and are required by both Django and Nginx containers. Because Amazon ECS
cannot (currently) directly mount a single volume from one container into
another container, we need these files to be deployed directly on the host OS.
Docker can mount the directory on both Django and Nginx containers as a shared
volume. This directory will be deployed by EB, and is thus excluded from Docker.</p>
<h3 id="binimage">bin/image</h3>
<p><code>bin/image</code> is a shell script that wraps common Docker commands used to create
Docker images. There are three major subcommands: <code>build</code>, <code>destroy</code> and
<code>update</code> all take a single argument, the name of the environment subdirectory of
the <code>environments/</code> directory. This will spawn Docker and build, delete or
rebuild the image as specified by the <code>Dockerfile</code> in the directory
corresponding to the final argument.</p>
<h3 id="binstevedore">bin/stevedore</h3>
<p><code>bin/stevedore</code> is a shell script that wraps common Docker commands used to
start and stop Docker containers. There are many subcommands, but the most
useful are: <code>start</code>, <code>stop</code> and either of the two <code>build</code> commands. All
subcommands take take a single argument, the name of the environment
subdirectory of the <code>environments/</code> directory. This will spawn Docker and start,
stop or run the corresponding build process.</p>
<h3 id="dockerdjangodevdocker-composeyml">docker/django/dev/docker-compose.yml</h3>
<p>This file provides configuration for Docker to orchestrate the management of the
development Docker container for the local dev environment.</p>
<pre><code>django:
  build: ../../..
  dockerfile: docker/django/dev/Dockerfile
  env_file: ../../../environments/dev/.env
  volumes:
    - &quot;../../../app/apps:/var/app/app/apps&quot;
    - &quot;../../../app/dist:/var/app/app/dist&quot;
    - &quot;../../../app/project:/var/app/app/project&quot;
    - &quot;../../../app/manage.py:/var/app/app/manage.py&quot;
    - &quot;../../../environments:/var/app/environments&quot;
    - &quot;../../../gulpfile.js:/var/app/gulpfile.js&quot;
  ports:
    - &quot;80:8080&quot;
    - &quot;8010:8010&quot;
</code></pre>
<p>It defines one container “django”, specifying a path to the <code>build</code>-context as
well as a path to load the <code>Dockerfile</code>. <code>env_file</code> specifies the path the a
file containing all environment variables. A set of volumes to share from the
host OS to the container are listed in <code>volumes</code>. Finally <code>ports</code> tells Docker
which ports on the host to map to the container.</p>
<h3 id="dockerdjangodevdockerfile">docker/django/dev/Dockerfile</h3>
<p>The <code>Dockerfile</code> is a set of instructions for Docker to execute in order to
produce a Docker image— a file used to create a Docker container running
your application code.</p>
<pre><code class="language-bash"># ...
# Install apt, Python then NodeJS dependencies.
RUN             apt-get update &amp;&amp; \
                curl -sL https://deb.nodesource.com/setup_0.12 | bash - &amp;&amp; \
                apt-get install -y nodejs &amp;&amp; \
                pip install --upgrade pip &amp;&amp; \
                pip install -r \
                    environments/dev/requirements.txt &amp;&amp; \
                npm update &amp;&amp; \
                npm install -g gulp &amp;&amp; \
                npm install &amp;&amp; \
                gulp
# Add our initialization script to the image and run it upon startup.
ADD             docker/django/start.sh /
CMD             [&quot;/start.sh&quot;]
</code></pre>
<p>In the development Dockerfile, <code>pip</code> and <code>npm</code> commands install the necessary
dependencies from the <code>environments/dev</code> folder. Finally <code>start.sh</code> is called to
start the Django application server.</p>
<h3 id="dockerdjangoproddocker-composeyml">docker/django/prod/docker-compose.yml</h3>
<p>This file provides configuration for Docker to orchestrate the management of the
production Docker containers. This configuration can be used for testing locally
prior to deployment to Amazon AWS.</p>
<pre><code class="language-yml">django:
  build: ../../..
  dockerfile: docker/django/prod/Dockerfile
  env_file: ../../../environments/prod/.env
  volumes:
    - &quot;../../../docker/django/prod/gunicorn.conf.py:/etc/gunicorn/gunicorn.conf.py:ro&quot;
    - &quot;/var/app/app/dist&quot;

nginx:
  image: nginx
  links:
    - django
  volumes:
    - &quot;../../../docker/nginx/nginx.conf:/etc/nginx/nginx.conf:ro&quot;
    - &quot;../../../docker/nginx/sites-enabled.conf:/etc/nginx/conf.d/default.conf:ro&quot;
  volumes_from:
    - django
  ports:
    - &quot;80:80&quot;
</code></pre>
<p>It defines two containers “django” and “nginx”. “Django” is configured quite
similar to the development setup, but doesn’t map as many volumes from the host
OS to the container. In this configuration “django” doesn’t need to expose a
port externally, as the “nginx” container will expose port 80 externally.</p>
<p>Importantly, the “nginx” container uses <code>links</code> to connect the “django”
container to “nginx”. This way nginx config files can refer to “django” as-if it
was another host on the same network with the name “nginx”.</p>
<p>Finally, the “nginx” container will mount all volumes from the “django”
container with the <code>volumes_from</code> directive, and expose port 80 to the host OS.</p>
<h3 id="dockerdjangoproddockerfile">docker/django/prod/Dockerfile</h3>
<p>The major difference between the development <code>Dockerfile</code> and the production
version is:</p>
<ul>
<li>It exposes port 8080 for other containers</li>
<li>It runs <code>gulp build</code> during image creation</li>
</ul>
<p>This Dockerfile also installs production-only Python pip dependencies in
requirements.txt.</p>
<pre><code class="language-bash"># Install apt, Python then NodeJS dependencies.
RUN             apt-get update &amp;&amp; \
                curl -sL https://deb.nodesource.com/setup_0.12 | bash - &amp;&amp; \
                apt-get install -y nodejs &amp;&amp; \
                pip install --upgrade pip &amp;&amp; \
                pip install -r \
                    environments/prod/requirements.txt &amp;&amp; \
                npm update &amp;&amp; \
                npm install -g gulp &amp;&amp; \
                npm install &amp;&amp; \
                gulp build
# Exposes port 8080
EXPOSE          8080
</code></pre>
<p>When Docker runs the image build, it runs <code>gulp build</code>, which runs code-quality,
unit tests and produces production-ready web frontend assets. This allows for
testing prior to deployment, and gives the team an opportunity to fix errors
before they go into the wild.</p>
<h2 id="elastic-beanstalk-specific-structure">Elastic Beanstalk specific structure</h2>
<p>Finally, these comments describe files related to Amazons’ Elastic Beanstalk:</p>
<p>.dockerignore
.ebextensions/           # Describes how EB builds environments
01_envvars.config      # Describes env vars for AWS Docker containers
.ebignore                # Describes how Amazon EB ignores some files
.elasticbeanstalk/       # Location Amazon EB stores its cli settings
Dockerrun.aws.json       # Describes how to run our containers in AWS
.gitignore
.bowerrc
.csslintrc.json
.jshintrc
bower.json
gulpfile.js
package.json
app/
apps/*
project/*
dist/*
bin/*
docker/
django/
dev/
docker-compose.yml
Dockerfile
prod/
docker-compose.yml
Dockerfile
gunicorn.conf.py
start.sh
nginx/*
environments/
dev/
.env
.env.example
Procfile
requirements.txt
prod/
.env
.env.example
Procfile
requirements.txt</p>
<h3 id="ebextensions01_envvarsconfig">.ebextensions/01_envvars.config</h3>
<p>This file is used by the Elastic Beanstalk command line utilities to pass
key-value parameters to Amazon EC2 and ECS. This file is used to store all
production environment variables that are provided to running containers. This
variables often vary between deployments.</p>
<h3 id="ebignore">.ebignore</h3>
<p>When Elastic Beanstalk does a deployment, it creates a zip file of the current
directory, uploads it to Amazon S3, and deploys the files to running EC2
instances. Since this project is using Docker images to package the app, we can
ignore most files; with the exception of the <code>app/dist</code> directory— which
both “Django” and “Nginx” containers will need access too.</p>
<p>The <code>.ebignore</code> file is used to ignore certain files in a project directory.
This file works like a <code>.gitignore</code> file.</p>
<h1 id="ignore-everything">Ignore everything!</h1>
<ul>
<li></li>
</ul>
<h1 id="except-for-these-exclusion-patterns-required-by-amazon-ecs">Except for these exclusion patterns required by Amazon ECS</h1>
<p>!Dockerrun.aws.json
!.ebextensions/<em>.config
!.elasticbeanstalk/</em>.cfg.yml
!.elasticbeanstalk/*.global.yml
!app/dist/**
!docker/**</p>
<p>The only files our EB package should contain are those required by Docker,
Elastic Beanstalk itself, or any files shared between both containers (such as
the <code>app/dist</code> directory).</p>
<p>When you deploy your project directory to Elastic Beanstalk and create a new
application version, the EB CLI will not include files specified by the
<code>.ebignore</code> in the source bundle that it creates. This is useful for creating
smaller packages by excluding files that aren’t required for running
production-only code.</p>
<h3 id="elasticbeanstalk">.elasticbeanstalk/</h3>
<p>Elastic Beanstalk uses this directory to store temp files and configuration
information about the current AWS account, EB Application name and IAM
credentials to utilize.</p>
<h3 id="dockerrunawsjson">Dockerrun.aws.json</h3>
<p><code>Dockerrun.aws.json</code> is a proprietary Amazon-specific JSON format called a
“<a href="#task-def">Task Definition</a>” used to configure how to manage Docker containers
running on Amazon EC2 Container Service (ECS) platform.</p>
<pre><code class="language-json">&quot;containerDefinitions&quot;: [
{
    &quot;name&quot;: &quot;django&quot;,
    &quot;image&quot;: &quot;0xadada/dockdj:latest&quot;,
    &quot;essential&quot;: true,
    &quot;memory&quot;: 512,
    &quot;mountPoints&quot;: [
        {
            &quot;sourceVolume&quot;: &quot;gunicorn-conf&quot;,
            &quot;containerPath&quot;: &quot;/etc/gunicorn/gunicorn.conf.py&quot;,
            &quot;readOnly&quot;: true
        }
    ]
}
</code></pre>
<p>The JSON format is very similar to the docker-compose Yaml format, having a
nearly 1-to-1 mapping of <code>image</code>, <code>mountPoints</code> to volumes and ports all
defined.</p>
<p>This file is functionally identical to <code>docker/prod/docker-compose.yml</code> in that
it runs, configures and connects the “Django” and “Nginx” Docker containers. As
such, changes to the <code>docker-compose.yml</code> file should be mirrored in the
<code>Dockerrun.aws.json</code> file.</p>
<h2 id="lifecycle">Lifecycle</h2>
<p>New developers to this project simply clone the project from GitHub, install
Docker (and boot2docker/docker-machine on OS X) and can begin running the app.
There is no need to setup a developer environment or create (yet) another VM.</p>
<h3 id="development">Development</h3>
<p>When the developer starts working on the project from scratch, the only
requirement is Docker and a machine capable of running Docker containers (Linux
3+ or boot2docker/docker-machine).</p>
<p>Once an organization or developer has adopted Docker for a single project,
startup time for other docker projects is drastically reduced as this core
requirement has already been met. From that point forward, the projects
themselves can define and provision their own dependencies.</p>
<p>For this project, the next steps required of the developer are as follows:</p>
<pre><code class="language-bash">git clone &lt;PROJECT&gt;
&lt;create .env file&gt;
.bin/stevedore dev start
</code></pre>
<p>The developer is now running the app. Any internal OS configuration, system
libraries, software dependencies and provisioning are all handled by the project
and Docker— transparently to the developer.</p>
<p>Subsequent context-switches between other projects and this project have been
reduced to a single command:</p>
<pre><code class="language-bash">.bin/stevedore dev start
</code></pre>
<p>The developer doesn’t need to boot up a VM, nor does she/he need to understand
or start any internal processes or run any commands internal to the VM.</p>
<p><strong>In development, Docker can be used to lower cognitive load on developers
switching between multiple projects.</strong></p>
<h3 id="qa">QA</h3>
<p>In this particular project, QA tests are run during build of the the production
Docker image via the <code>gulp build</code> task. See
<a href="#dockerdjangoproddockerfile">the production Dockerfile</a> to view how it calls
the gulp task.</p>
<p>In effect, this will prevent developers or continuous Integration systems from
publishing a production Docker image to Docker Hub, as the build will trigger a
Docker image build failure.</p>
<p>More generally, development teams could create different tags for “production”
releases and “development” Docker image releases. Lets say “prod” vs “dev”.</p>
<p>Development teams could publish images tagged with “dev”, to Docker Hub. Other
developers on the team or members of the QA team could <code>docker pull</code> that tagged
image and run their suite of tests on it.</p>
<p>Using Docker in this manner, dev and QA teams no longer have to keep VM
configurations synchronized, as the OS and other dependencies have been pushed
down from VM directly into dependencies within the scope of the project. <strong>This
has the effect of reducing manual synchronization and de-necessitating
out-of-channel communication between development and QA teams about the state of
the runtime environment.</strong> This allows for faster, less-error-prone iteration of
the runtime environment.</p>
<h3 id="production">Production</h3>
<p>Developers have iterated on functionality, QA has run tests against the code,
and the projects is ready for deployment to production.</p>
<p>At this point, a working Docker image has been run on developers local machines,
and QA has passed. These three phases could’ve gone through multiple iterations
while bugs were identified and fixed. The end result is a working Docker image
exists that has been deemed “ready” for production.</p>
<p>Either manually or as part of a continuous integration tool, the
production-ready Docker image can now be tagged with a release version and
published to Docker Hub (or other compatible Docker image repository). Finally,
the deploy process needs to update the production servers running our working
application stack and run the latest application code.</p>
<p>These tasks are handled by our <code>bin/deploy</code> script, a wrapper for Docker, Git
and Elastic Beanstalk. It will tag the latest Docker image, publish the tag to
Docker Hub, tag the publish the tags to GitHub and use Elastic Beanstalk to
deploy both the latest stack and application code:</p>
<p>(Lets use 1.2.3 as an arbitrary version number for this example)</p>
<pre><code class="language-bash">bin/deploy release 1.2.3 # Create a release branch and tag the image
bin/deploy publish 1.2.3 # Publish the Docker image and git branch
                         # to Docker Hub and GitHub
bin/deploy deploy 1.2.3  # Use EB to deploy the latest release
</code></pre>
<p>The deploy script is a light bash wrapper that automates Git, Docker and Elastic
Beanstalk commands in an easy-to-reproduce set of short commands.</p>
<p>Once complete, the Amazon environment will be running your latest application
code, as well as any new changes to the container OS, system libraries and
dependencies. Most importantly, <strong>any changes in provisioning to the stack have
been deployed along with the Docker image</strong>, thus enabling seamless roll-backs.
<strong>Rolling back the application version will also rollback the stack version</strong>.
The application and stack are deployed together.</p>
<p>Happy cloud computing!</p>
<h2 id="terms">Terms</h2>
<ul>
<li>
<p><a name="autoscaling"><em>Auto-scaling</em></a> A method of setting a threshold that
detects when the load on a server cluster necessitates adding or removing
servers in order to optimize the number of servers servicing that load.
Auto-scaling allows an organization to decrease operating costs by running the
minimum number of servers required to service its load, and eliminating the
need to accurately predict future traffic patterns.</p>
</li>
<li>
<p><a name="config-mgmt"><em>Configuration Management</em></a> Software tools that are
designed to automatically start, provision and configure software on virtual
machines rather than have engineers run these steps manually on each server.
These tools can be used both locally to create development VMs (virtual
machines) as well as in the cloud to create staging and production VMs.</p>
</li>
<li>
<p><a name="docker-image"><em>Docker image</em></a> A docker image is like an executable
program binary. It takes source files and other assets and bundles them
together, and the resulting bundle can be run/executed as a single process on
a Linux machine.</p>
</li>
<li>
<p><a name="docker-container"><em>Docker container</em></a> A docker container is like a
running executable program. It is a running instance of a docker image. Like a
running program, it has a PID, and it is appropriate to call it a process. It
can be started and stopped. One docker image can be run many times on one or
more machines.</p>
</li>
<li>
<p><a name="provisioning"><em>Provisioning</em></a> The installation and configuration
of software needed to run an application. E.g. Installing and configuring
Apache and its system libraries in order to run WordPress.</p>
</li>
<li>
<p><a name="task-def"><em>Task definition</em></a> A proprietary JSON format for
describing how Docker containers are run within the Amazon EC2 Cloud Service.
Read more about
<a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_defintions.html">Amazon ECS Task Definitions</a>.
Docker uses the <a href="https://docs.docker.com/compose/yml/">docker-compose Yaml</a>
file to do the same thing.</p>
</li>
</ul>
<section data-footnotes="true" class="footnotes"><h2 class="sr-only" id="footnote-label">Footnotes</h2>
<ol>
<li id="user-content-fn-1">
<p>Configuration management tools can be used to couple both Stack and
Application, but experience has has shown that over time, these tools
are not strongly opinionated, and therefore Stack-App decoupling occurs
organically over the lifetime of a project. <a href="#user-content-fnref-1" data-footnote-backref="true" class="data-footnote-backref" aria-label="Back to content">↩</a></p>
</li>
<li id="user-content-fn-2">
<p>Docker runs on Linux version 3. In the case where the developer is
using OS X, Windows or another non-Linux OS, they’ll need to run a Linux
VM in order to use Docker. However, this single VM will be able to run
all Docker containers for all Docker projects they use. Tools like
Docker Machine make working with the Docker VM much simpler. <a href="#user-content-fnref-2" data-footnote-backref="true" class="data-footnote-backref" aria-label="Back to content">↩</a></p>
</li>
</ol>
</section></div><footer class="license_copyright__EpMnZ">This is licensed under a Creative Commons <!-- -->cc-by-nc-sa<!-- --> International License</footer></article></main><footer class="footer_footer__woFMY layout_layout__dCqca"><nav><p>© 2003-<!-- -->2024<!-- --> 0xADADA (unless otherwise noted.)<br/><span class="h-card"><span class="p-note" hidden="">0xADADA is a Software Engineer / Writer / Motorsports Driver exploring the impact of the attention economy on idleness, time, and lived experience. 🧑‍💻🔧🚗✍️🔒</span><a class="u-url u-uid" title="0xADADA" href="https://0xadada.pub/">0xADADA</a> <a class="c-Meta u-email" rel="me" title="0xADADA" href="mailto:0xadada.pub@protonmail.com">Email</a> <a class="u-url" rel="me nofollow external noopener" title="0xADADA on Warpcast" href="https://warpcast.com/0xadada">Farcaster</a> <a class="u-url" rel="me nofollow external noopener" title="0xADADA on Mastodon" href="https://mastodon.cloud/@0xADADA">Mastodon</a> <a class="u-url" rel="me nofollow external noopener" title="0xADADA on Bluesky" href="https://bsky.app/profile/0xadada.bsky.social">Bluesky</a> <a class="u-url" rel="me nofollow external noopener" title="0xADADA on Twitter" href="https://twitter.com/0xadada"><del>Twitter</del></a> <a class="u-url" rel="me nofollow external noopener" title="0xADADA on GitHub" href="https://github.com/0xadada">GitHub</a> <a class="u-url" rel="me nofollow external noopener" title="0xADADA on Goodreads" href="https://www.goodreads.com/review/list/60524683-0xadada?shelf=wanted">Goodreads</a> <a href="/colophon/">Colophon</a> <a href="/rss.xml">RSS</a> <img alt="0xADADA icon" loading="lazy" width="20" height="20" decoding="async" data-nimg="1" class="u-photo" style="color:transparent;margin-bottom:-0.125rem" src="/static/images/meta/avatar.svg"/></span></p></nav><form action="https://buttondown.email/api/emails/embed-subscribe/0xadada" method="post" target="popupwindow"><label for="email">Sign up to get emailed when I write new things:<!-- --> </label><input type="email" id="email" name="email"/><input type="submit" value="Subscribe"/></form></footer><script src="/_next/static/chunks/webpack-85364f91ad1195ff.js" crossorigin="" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/css/362901d03019bc4c.css\",\"style\",{\"crossOrigin\":\"\"}]\n0:\"$L2\"\n"])</script><script>self.__next_f.push([1,"3:HL[\"/_next/static/css/01f63581d77a7b07.css\",\"style\",{\"crossOrigin\":\"\"}]\n"])</script><script>self.__next_f.push([1,"4:I[7690,[],\"\"]\n7:I[5613,[],\"\"]\n9:I[1778,[],\"\"]\na:I[5250,[\"250\",\"static/chunks/250-d7e0a94ebe194dac.js\",\"931\",\"static/chunks/app/page-84b663e6d1625d16.js\"],\"\"]\nb:I[1749,[\"250\",\"static/chunks/250-d7e0a94ebe194dac.js\",\"749\",\"static/chunks/749-1aefd436964833c3.js\",\"185\",\"static/chunks/app/layout-18f161c3adf8f2fa.js\"],\"Image\"]\nd:I[8955,[],\"\"]\n8:[\"slug\",\"2015/11/03/development-lifecycle-with-docker-and-elastic-beanstalk\",\"c\"]\ne:[]\n"])</script><script>self.__next_f.push([1,"2:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/362901d03019bc4c.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]],[\"$\",\"$L4\",null,{\"buildId\":\"cN9ZgLN6aT9MrEZmAOKFL\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/2015/11/03/development-lifecycle-with-docker-and-elastic-beanstalk/\",\"initialTree\":[\"\",{\"children\":[[\"slug\",\"2015/11/03/development-lifecycle-with-docker-and-elastic-beanstalk\",\"c\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":[\\\"2015\\\",\\\"11\\\",\\\"03\\\",\\\"development-lifecycle-with-docker-and-elastic-beanstalk\\\"]}\",{}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[[\"slug\",\"2015/11/03/development-lifecycle-with-docker-and-elastic-beanstalk\",\"c\"],{\"children\":[\"__PAGE__\",{},[\"$L5\",\"$L6\",null]]},[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"$8\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L9\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/01f63581d77a7b07.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]]}]]},[null,[\"$\",\"html\",null,{\"lang\":\"en-US\",\"children\":[\"$\",\"body\",null,{\"children\":[[\"$\",\"main\",null,{\"className\":\"layout_layout__dCqca\",\"children\":[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L9\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[],\"styles\":null}]}],[\"$\",\"footer\",null,{\"className\":\"footer_footer__woFMY layout_layout__dCqca\",\"children\":[[\"$\",\"nav\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[\"© 2003-\",\"2024\",\" 0xADADA (unless otherwise noted.)\",[\"$\",\"br\",null,{}],[\"$\",\"span\",null,{\"className\":\"h-card\",\"children\":[[\"$\",\"span\",null,{\"className\":\"p-note\",\"hidden\":true,\"children\":\"0xADADA is a Software Engineer / Writer / Motorsports Driver exploring the impact of the attention economy on idleness, time, and lived experience. 🧑‍💻🔧🚗✍️🔒\"}],[\"$\",\"$La\",null,{\"className\":\"u-url u-uid\",\"href\":\"https://0xadada.pub/\",\"title\":\"0xADADA\",\"children\":\"0xADADA\"}],\" \",[\"$\",\"$La\",null,{\"href\":\"mailto:0xadada.pub@protonmail.com\",\"className\":\"c-Meta u-email\",\"rel\":\"me\",\"title\":\"0xADADA\",\"children\":\"Email\"}],\" \",[\"$\",\"$La\",null,{\"href\":\"https://warpcast.com/0xadada\",\"className\":\"u-url\",\"rel\":\"me nofollow external noopener\",\"title\":\"0xADADA on Warpcast\",\"children\":\"Farcaster\"}],\" \",[\"$\",\"$La\",null,{\"href\":\"https://mastodon.cloud/@0xADADA\",\"className\":\"u-url\",\"rel\":\"me nofollow external noopener\",\"title\":\"0xADADA on Mastodon\",\"children\":\"Mastodon\"}],\" \",[\"$\",\"$La\",null,{\"href\":\"https://bsky.app/profile/0xadada.bsky.social\",\"className\":\"u-url\",\"rel\":\"me nofollow external noopener\",\"title\":\"0xADADA on Bluesky\",\"children\":\"Bluesky\"}],\" \",[\"$\",\"$La\",null,{\"href\":\"https://twitter.com/0xadada\",\"className\":\"u-url\",\"rel\":\"me nofollow external noopener\",\"title\":\"0xADADA on Twitter\",\"children\":[\"$\",\"del\",null,{\"children\":\"Twitter\"}]}],\" \",[\"$\",\"$La\",null,{\"href\":\"https://github.com/0xadada\",\"className\":\"u-url\",\"rel\":\"me nofollow external noopener\",\"title\":\"0xADADA on GitHub\",\"children\":\"GitHub\"}],\" \",[\"$\",\"$La\",null,{\"href\":\"https://www.goodreads.com/review/list/60524683-0xadada?shelf=wanted\",\"className\":\"u-url\",\"rel\":\"me nofollow external noopener\",\"title\":\"0xADADA on Goodreads\",\"children\":\"Goodreads\"}],\" \",[\"$\",\"$La\",null,{\"href\":\"/colophon/\",\"children\":\"Colophon\"}],\" \",[\"$\",\"$La\",null,{\"href\":\"/rss.xml\",\"children\":\"RSS\"}],\" \",[\"$\",\"$Lb\",null,{\"className\":\"u-photo\",\"src\":\"/static/images/meta/avatar.svg\",\"alt\":\"0xADADA icon\",\"width\":\"20\",\"height\":\"20\",\"style\":{\"marginBottom\":\"-0.125rem\"}}]]}]]}]}],[\"$\",\"form\",null,{\"action\":\"https://buttondown.email/api/emails/embed-subscribe/0xadada\",\"method\":\"post\",\"target\":\"popupwindow\",\"children\":[[\"$\",\"label\",null,{\"htmlFor\":\"email\",\"children\":[\"Sign up to get emailed when I write new things:\",\" \"]}],[\"$\",\"input\",null,{\"type\":\"email\",\"name\":\"email\",\"id\":\"email\"}],[\"$\",\"input\",null,{\"type\":\"submit\",\"value\":\"Subscribe\"}]]}]]}]]}]}],null]],\"initialHead\":[false,\"$Lc\"],\"globalErrorComponent\":\"$d\",\"missingSlots\":\"$We\"}]]\n"])</script><script>self.__next_f.push([1,"f:T6ac,"])</script><script>self.__next_f.push([1,".dockerignore\n.ebextensions/\n  01_envvars.config\n.ebignore\n.elasticbeanstalk/\nDockerrun.aws.json\n.gitignore               # Describes which files git ignores\n.bowerrc                 # Configures where web frontend dependencies live\n.csslintrc.json          # Describes CSS syntax rules\n.jshintrc                # Describes JavaScript syntax rules\nbower.json               # Describes web frontend dependencies\ngulpfile.js              # Describes app build and dev tasks\npackage.json             # Describes NPM dependencies\napp/                     # Our python app\n  apps/*                 # python app modules\n  project/*              # App-specific settings\n  dist/*                 # App static assets (served via Nginx)\nbin/*\ndocker/\n  django/\n    dev/\n      docker-compose.yml\n      Dockerfile\n    prod/\n      docker-compose.yml\n      Dockerfile\n      gunicorn.conf.py   # Settings for production app-server\n    start.sh             # Script to start app-server\n  nginx/*                # Nginx config files\nenvironments/            # Environment-specific settings\n  dev/                   # Development-only environment settings\n    .env                 # Actual environment vars (Excluded from git)\n    .env.example         # Example environment vars\n    Procfile             # Configures how Honcho starts app-servers\n    requirements.txt     # Describes dev Python dependencies\n  prod/                  # Development-only environment settings\n    .env                 # Actual environment vars (Excluded from git)\n    .env.example         # Example environment vars\n    Procfile             # Configures how Honcho starts app-servers\n    requirements.txt     # Describes prod Python dependencies\n"])</script><script>self.__next_f.push([1,"6:[\"$\",\"article\",null,{\"className\":\"hentry h-entry\",\"children\":[[\"$\",\"header\",null,{\"children\":[[\"$\",\"h1\",null,{\"className\":\"entry-title p-name\",\"children\":\"Development Lifecycle with Docker and Elastic Beanstalk\"}],\"$undefined\",[\"$\",\"time\",null,{\"className\":\"display-date_published__A_L_d dt-published\",\"dateTime\":\"2015-11-03T00:00:00.000Z\",\"children\":\"Tuesday November 3, 2015\"}],[\"$\",\"span\",null,{\"className\":\"h-card page_byline__wbXsN\",\"children\":[\" by \",[\"$\",\"span\",null,{\"className\":\"fn p-author p-name\",\"children\":\"0xADADA\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"entry-content e-content\",\"children\":[[\"$\",\"p\",null,{\"children\":\"Docker is getting a lot of hype these days, for good reason. There are plenty of\\narticles touting the merits of Docker but most are written without context and\\nare limited to examining the the benefits of Docker independently of its’\\npractical everyday use in a software development project lifecycle.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"This article aims to examine the benefits of Docker within the context of a\\nsoftware company with multiple developers working on multiple projects, having\\nto manage these projects deployed in a cloud production environment. In this\\ncontext the benefits of Docker become more readily apparent.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"With the rise of cloud computing, the number of systems that need to be\\nmaintained has exploded. Manual \",[\"$\",\"em\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"#provisioning\",\"children\":\"provisioning\"}]}],\" of an\\nincreasingly large number of systems becomes impossible for a small team, given\\nplatforms like Amazon EC2 provide \",[\"$\",\"em\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"#autoscaling\",\"children\":\"auto-scaling\"}]}],\" when additional\\nload is detected.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Tools like Ansible, Chef, Puppet and Salt are great solutions towards achieving\\nautomated provisioning of virtual machines. The industry has responded by\\nquickly adopting these tools, but even more agility and performance can be\\nachieved by using \",[\"$\",\"em\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"#docker-container\",\"children\":\"Docker containers\"}]}],\".\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Combining Docker with a deployment tool like AWS Elastic Beanstalk can provide\\neven greater efficiencies for developing and deploying cloud applications.\"}],\"\\n\",[\"$\",\"h2\",null,{\"id\":\"dockdj\",\"children\":\"Dockdj\"}],\"\\n\",[\"$\",\"figure\",null,{\"className\":\"rehype-figure\",\"children\":[[\"$\",\"img\",null,{\"src\":\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f6a2.png\",\"alt\":\"Dockdj\"}],[\"$\",\"figcaption\",null,{\"children\":\"Dockdj\"}]]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"This article will be using \",[\"$\",\"a\",null,{\"href\":\"https://github.com/0xadada/dockdj\",\"children\":\"Dockdj\"}],\" to\\nillustrate using Docker and Elastic Beanstalk in the context of a real-world web\\nproject. Dockdj is a recipe for building 12-factor Python / Django web apps with\\nmulti-container Docker and deploying to Amazon AWS using Elastic Beanstalk.\\n\",[\"$\",\"a\",null,{\"href\":\"https://github.com/0xadada/dockdj\",\"children\":\"Dockdj is available on GitHub\"}],\".\"]}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"manual-provisioning\",\"children\":\"Manual Provisioning\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The naive approach is \",[\"$\",\"em\",null,{\"children\":\"manual provisioning\"}],\": the developer installs Apache and\\nassociated system libraries directly on the local development machine, configure\\nit according to the WordPress documentation.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"These manual steps will need to be repeated for every additional member of the\\ndevelopment team, and again for the production web server. When provisioned\\nsoftware is updated or configurations change. All members of the development\\nteam and the production systems need to be updated accordingly. Larger teams\\ninevitably begin experiencing the \",[\"$\",\"em\",null,{\"children\":\"“works on my machine”\"}],\" problems between\\ndevelopers when some developers haven’t updated their configurations to match\\ncoworkers who have.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Manual provisioning quickly becomes a frequent and resource-intensive process,\\nwith the side-effect of prolonging the deployment of important vendor bug fixes\\nand security patches across both development and productions systems.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Additionally, when increased traffic hits productions systems, new systems need\\nto be allocated and scaled horizontally to support the new traffic. All the\\nprovisioning needs to repeated. This system doesn’t scale as more production\\nservers are added to serve additional traffic load.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Even worse is when differences between developer-systems and production-systems\\nresult in hard-to-reproduce bugs once the app is deployed from development into\\nproduction.\"}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"automated-provisioning\",\"children\":\"Automated Provisioning\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"An improvement over manual provisioning is \",[\"$\",\"em\",null,{\"children\":\"automated provisioning\"}],\" using a\\n\",[\"$\",\"em\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"#confg-mgmt\",\"children\":\"configuration management\"}]}],\" tool like Ansible, Chef, Puppet, Salt,\\netc. These tools have been developed to address the problems of provisioning at\\nlarge scale.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The aim of these tools are:\"}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Initialize and start virtual machines\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Automate the provisioning process in a repeatable way\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Manage changes to provisioning in a version control system\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Establishing and maintaining consistency of system dependencies and\\nconfiguration throughout an applications life\"}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Configuration management tools are wonderful for automated provisioning, but \",[\"$\",\"em\",null,{\"children\":\"in\\npractice\"}],[\"$\",\"sup\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"#user-content-fn-1\",\"id\":\"user-content-fnref-1\",\"data-footnote-ref\":true,\"aria-describedby\":\"footnote-label\",\"children\":\"1\"}]}],\" they tend to\\nsplit management of the stack-app into two parts:\"]}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"the software Stack\"}],\"\\n\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Operating system\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"System libraries\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Provisioned software\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Configuration\"}],\"\\n\"]}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"the Application\"}],\"\\n\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Source code \u0026 binaries\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Dependencies\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Runtime environment\"}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The result is that the stack (#1) is initially allocated and provisioned using\\none of the configuration management tools. The application (#2) is then deployed\\non the stack— resulting in a running application. When subsequent\\napplication versions (#2) are released and deployed, they are deployed onto the\\n(unchanged) stack. \",[\"$\",\"strong\",null,{\"children\":\"The problem with this model is that the stack and the\\napplication are managed independently.\"}],\" Changes to the stack are managed as a\\nunit separate from changes to the application. No data is recorded that\\ndescribes the compatibility of the integrated whole.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"This results in increased complexity during rollbacks or simultaneous updates to\\nboth stack and application. More importantly \",[\"$\",\"strong\",null,{\"children\":\"version numbers of the\\napplication are not tied to versions of the stack.\"}]]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Under this model, the stack version and application version aren’t\\ncoupled— which increases the likelihood of integration failures.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"An example will illustrate where this model will fail:\"}],\"\\n\",[\"$\",\"blockquote\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"Our production web server is provisioned with Apache 3.3.0 and the application\\n(WordPress) was at version 0.7.0 last week, and have just released version\\n0.8.0 this past week.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Apache announces a security vulnerability fix at version 3.3.1. Under the\\n(typical) automated provisioning model, the configuration management tool\\nwould be updated to provision the new version of Apache. The tool runs against\\nall production server systems. Here the application doesn’t change, it simply\\nrides on top of the Apache stack without change. No problems occur with the\\nrollout of the new Apache release.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Next the application updates and releases a new version for deployment- 0.9.0.\\nThe deploy process runs, and for some reason the application fails, it isn’t\\ncompatible with version 3.3.1 of Apache.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The decision is made to rollback the application to 0.8.0, which runs\\nsuccessfully with Apache 3.3.1. The system is working again.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"A critical security vulnerability is discovered in application 0.8.0 and the\\ndecision is made to roll application back to version 0.7.0. (Keep in mind the\\nprevious app version 0.7.0 was running Apache 3.3.0, and the stack is\\ncurrently 3.3.1).\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The application fails— because 0.7.0 was never integration tested\\nagainst Apache 3.3.1. What do you do?\"}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"In this example the devops team \",[\"$\",\"strong\",null,{\"children\":\"failed to remember\"}],\" to rollback Apache,\\nsimply because the integrated dependencies were not internally coupled. The\\ncompatible coupling existed \",[\"$\",\"strong\",null,{\"children\":\"only as institutional knowledge\"}],\" outside the\\nscope of the configuration management system, as Stack and Application were\\nmanaged separately.\"]}],\"\\n\",[\"$\",\"h2\",null,{\"id\":\"docker-for-configuration-management\",\"children\":\"Docker for Configuration Management\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"One major advantage of Docker is that it does not necessitate running a unique\\nVM\",[\"$\",\"sup\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"#user-content-fn-2\",\"id\":\"user-content-fnref-2\",\"data-footnote-ref\":true,\"aria-describedby\":\"footnote-label\",\"children\":\"2\"}]}],\" for every project a team works on.\\nIf developers work on multiple projects, each with its own customized VM,\\nswitching between projects becomes a time-consuming context shift for\\ndevelopers.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Docker containers run directly on the Linux operating system and yet each\\ncontainer is isolated. This eliminates the slowness of booting and the overhead\\nof a VM. Docker containers start up as quickly as running a normal process, and\\neliminate VM “booting” for every Docker project the developer works on.\\nDeploying changes to the environment for every developer working on the project\\nis as easy as publishing a new \",[\"$\",\"em\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"#docker-image\",\"children\":\"Docker image\"}]}],\". Next time a\\ndeveloper starts the container, he/she will get the new image.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Another advantage of Docker over an automated configuration management tool is\\nthat it \",[\"$\",\"strong\",null,{\"children\":\"does not\"}],\" bifurcate the stack and the application into independent\\nsegments.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"A stack using Docker containers has the same benefit of configuration\\nmanagement, but can couple the stack and the application into a single managed\\ncomponent. The application is deployed along with its stack— and the\\ncomplete stack-app component is deployed together as a single Docker image or a\\nbundle of Docker images that have already been integration tested at least on a\\ndevelopers machine.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"As opposed the the “automated provisioning” model, the Docker model of the\\nstack-app looks more like this:\"}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Set of Docker images\"}],\"\\n\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Operating system (the software Stack)\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Provisioned software (the software Stack)\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Configuration (the software Stack)\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"System libraries (the software Stack)\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Source code \u0026 binaries (Application)\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Dependencies (Application)\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Runtime environment (Application)\"}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"With every deployment, the entire stack-app (1) will be deployed. Docker uses\\nhashes (like Git) to minimize the amount of data that will be downloaded for any\\nupdate. This means only the differences are downloaded rather than the entire\\nstack.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"The greatest advantage of using Docker is that developers can run the\\napplication in the very same environment as production\"}],\". According to\\n\",[\"$\",\"a\",null,{\"href\":\"http://12factor.net/dev-prod-parity\",\"children\":\"Twelve-Factor Methodology\"}],\" this is called\\nachieving “Dev/Prod Parity”. This is a huge benefit in that it eliminates an\\nentire class of bugs that result from differences between\\ndevelopers-and-developers as well as bugs that result from differences between\\ndevelopers-and-production.\"]}],\"\\n\",[\"$\",\"h2\",null,{\"id\":\"elastic-beanstalk-for-deployment\",\"children\":\"Elastic Beanstalk for Deployment\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"If you know \",[\"$\",\"a\",null,{\"href\":\"https://www.heroku.com/\",\"children\":\"Heroku\"}],\", than Amazons’ Elastic Beanstalk\\nwill be extremely familiar. EB borrows many ideas from Heroku, but the killer\\nfeature is its’ ability to dynamically run, deploy and scale Docker containers\\non a cluster of servers. It handles hardware allocation, network configuration,\\nload balancing, auto-scaling, health monitoring and rolling deployments.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"EB doesn’t do everything, but it’s good enough to adopt early and use until your\\nteam understands its deployment use-cases more clearly and understands\\nlimitations of EB and its trade-offs.\"}],\"\\n\",[\"$\",\"h2\",null,{\"id\":\"django-specific-structure\",\"children\":\"Django specific structure\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The core structure of the Docker / Elastic Beanstalk app can be explained by\\ndescribing the directory structure.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"These comments describe the application-specific file structure:\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"$f\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Some of these directories and files are described in more depth below:\"}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"gitignore\",\"children\":\".gitignore\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"environments/*/.env\\nnode_modules\\n.elasticbeanstalk/*\\n!.elasticbeanstalk/*.cfg.yml\\n!.elasticbeanstalk/*.global.yml\\n# Built testing and static asset artifacts\\napp/dist\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Files matching the name \",[\"$\",\"code\",null,{\"children\":\"environments/*/.env\"}],\" contain sensitive information\\n(usernames, passwords, etc) about per-deployment environments that shouldn’t be\\nincluded in version control.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The \",[\"$\",\"code\",null,{\"children\":\"node_modules\"}],\" directory and will be created when the developer installs NPM\\npackages. These are dependencies and should not be committed into the source\\ncode repository.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The \",[\"$\",\"code\",null,{\"children\":\".elasticbeanstalk/*\"}],\" directory is excluded from Git because it contains\\nfiles that are generated by EB command-line during environment creation and\\nversion deployment that shouldn’t be written to the repository. It also contains\\ntemporary configuration files (written by the EB cli).\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Both \",[\"$\",\"code\",null,{\"children\":\"!.elasticbeanstalk/*.cfg.yml\"}],\" and \",[\"$\",\"code\",null,{\"children\":\"!.elasticbeanstalk/*.global.yml\"}],\"\\nentries use the “NOT” operator to re- include themselves into the repo. These\\nfiles can be useful to have in version control, as they contain useful\\nenvironment configuration settings.\"]}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"bowerjson\",\"children\":\"bower.json\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Bower is a web frontend package management system. The application declares its\\nfrontend dependencies in this file.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"During docker image creation, these dependencies are installed.\"}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"gulpfilejs\",\"children\":\"gulpfile.js\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Gulp.js is a task runner for Node.js. \",[\"$\",\"code\",null,{\"children\":\"gulpfile.js\"}],\" defines common tasks and\\nutilities related to this application:\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Running code Syntax checking \u0026 automated testing\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"SASS and CSS compilation and minification\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Frontend asset building\"}],\"\\n\"]}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"packagejson\",\"children\":\"package.json\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"NPM is a package management system for Node.js applications. \",[\"$\",\"code\",null,{\"children\":\"package.json\"}],\" The\\napplication declares its Node.js dependencies in this file.\"]}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"app\",\"children\":\"app/\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The \",[\"$\",\"code\",null,{\"children\":\"app/\"}],\" directory contains all source code related to the Django python web\\napplication.\"]}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"dockerdjangoprodgunicornconfpy\",\"children\":\"docker/django/prod/gunicorn.conf.py\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"This project uses two application servers, \",[\"$\",\"code\",null,{\"children\":\"runserver_plus\"}],\" during development\\nand \",[\"$\",\"code\",null,{\"children\":\"gunicorn\"}],\" in production.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"These are settings related to the Gunicorn application server. In production, a\\nmore performant application is used, requiring this configuration file.\"}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"dockernginxsites-enabledconf\",\"children\":\"docker/nginx/sites-enabled.conf\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"On production systems, where nginx acts as a reverse-proxy for the Gunicorn web\\napplication, we use Docker links to connect the two containers together. This\\nconfiguration is best for reducing latency. Inside our nginx config file, we can\\nuse a named entry for the \",[\"$\",\"code\",null,{\"children\":\"proxy_pass\"}],\" value to reference our Django application\\nserver running in another container on port 8080.\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"# ...\\nlocation / {\\n  proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\\n  proxy_set_header Host $http_host;\\n  proxy_redirect   off;\\n  proxy_pass       http://django:8080;\\n  # ...\\n\"}]}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"dockerdjangostartsh\",\"children\":\"docker/django/start.sh\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"cd /var/app\\n# ...\\nhoncho --procfile \\\"environments/$ENV_NAME/Procfile\\\" \\\\\\n       --env \\\"environments/$ENV_NAME/.env\\\" start\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"start.sh\"}],\" is used during both development and production as a single task to\\nbootstrap the application server. It uses the \",[\"$\",\"code\",null,{\"children\":\"honcho\"}],\" task runner to start the\\nserver according to a set of tasks in a \",[\"$\",\"code\",null,{\"children\":\"Procfile\"}],\" for development and another\\nfor production.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The \",[\"$\",\"code\",null,{\"children\":\"--env\"}],\" parameter is used to pass environment variables sourced from the\\nenvironments subdirectory. At runtime, the \",[\"$\",\"code\",null,{\"children\":\"$$ENV_NAME\"}],\" variable will be set:\\n\",[\"$\",\"code\",null,{\"children\":\"dev\"}],\" for development and \",[\"$\",\"code\",null,{\"children\":\"prod\"}],\" for production. This way a separate Procfile\\nand separate set of environment variables are available to configure the modes\\nindependently.\"]}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"environmentsdevprocfile\",\"children\":\"environments/dev/Procfile\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"# The webserver: Python\\nwebserver: cd app \u0026\u0026 ./manage.py runserver_plus 0.0.0.0:8080\\n# The CDN assets emulation server\\ncdnserver: cd /var/app/app/dist \u0026\u0026 python -m http.server 8010\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"During development, the Django \",[\"$\",\"code\",null,{\"children\":\"runserver_plus\"}],\" application server interprets\\nPython, while a simple HTTP server serves assets (images, CSS, etc).\"]}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"environmentsprodprocfile\",\"children\":\"environments/prod/Procfile\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-bash\",\"children\":\"webserver: cd app \u0026\u0026 gunicorn \\\\\\n  -c /etc/gunicorn/gunicorn.conf.py project.wsgi:application\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"In production, we use Gunicorn to serve the python application, so the only task\\nrun is the gunicorn app server. Static assets aren’t handled here because Nginx\\nwill be reverse-proxying the application and also serving static assets.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"This decision was made because Nginx is optimized to serve static assets and it\\nwill reduce load on the application server.\"}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"environmentsdev--prodenvexample\",\"children\":\"environments/[dev | prod]/.env.example\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Both \",[\"$\",\"code\",null,{\"children\":\"environments/dev/.env.example\"}],\" and \",[\"$\",\"code\",null,{\"children\":\"environments/prod/.env.example\"}],\" are\\ncommitted into git to provide hints to developers that they should create a\\n\",[\"$\",\"code\",null,{\"children\":\".env\"}],\" file in the same directory as a place to store environment variables that\\nare passed into the application.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The \",[\"$\",\"code\",null,{\"children\":\".env\"}],\" file is excluded from the git repository as it contains sensitive\\nusernames passwords and cryptographic information.\"]}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"environmentsdev--prodrequirementstxt\",\"children\":\"environments/[dev | prod]/requirements.txt\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The python utility \",[\"$\",\"code\",null,{\"children\":\"pip\"}],\" is a package management utility. It uses a file named\\n\",[\"$\",\"code\",null,{\"children\":\"requirements.txt\"}],\" to install all package dependencies. The development\\nenvironment adds some useful debugging utilities that shouldn’t be included on\\nproduction systems, so production has its own file.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"During docker image creation, these dependencies are installed.\"}],\"\\n\",[\"$\",\"h2\",null,{\"id\":\"docker-specific-structure\",\"children\":\"Docker specific structure\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\".dockerignore            # Describes which files Docker ignores\\n.ebextensions/\\n  01_envvars.config\\n.ebignore\\n.elasticbeanstalk/\\nDockerrun.aws.json\\n.gitignore\\n.bowerrc\\n.csslintrc.json\\n.jshintrc\\nbower.json\\ngulpfile.js\\npackage.json\\napp/\\n  apps/*\\n  project/*\\n  dist/*\\nbin/*                    # Scripts for Docker, build and deployment\\ndocker/                  # Configuration files required by docker\\n  django/                # Django related settings\\n    dev/                 # Development-only settings\\n      docker-compose.yml # Orchestrates dev containers\\n      Dockerfile         # Builds dev Docker image\\n    prod/                # Production-only settings\\n      docker-compose.yml # Orchestrates prod containers\\n      Dockerfile         # Builds prod Docker image\\n      gunicorn.conf.py\\n    start.sh\\n  nginx/*\\nenvironments/\\n  dev/\\n    .env\\n    .env.example\\n    Procfile\\n    requirements.txt\\n  prod/\\n    .env\\n    .env.example\\n    Procfile\\n    requirements.txt\\n\"}]}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"dockerignore\",\"children\":\".dockerignore\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The \",[\"$\",\"code\",null,{\"children\":\".dockerignore\"}],\" file specifies a list of patterns to exclude from the build\\ncontext during creation of the Docker image. These files are not required by the\\nexecution of the container, and should be removed to reduce the size of the\\nfinal image.\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\".coverage\\n.ebextensions/*\\n.elasticbeanstalk/*\\n.ebignore\\n.dockerignore\\n.git\\n.gitignore\\n.DS_Store\\nnode_modules\\napp/dist\\ndocs\\nhtmlcov\\nREADME.md\\nghostdriver.log\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Some notable entries are described below:\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The \",[\"$\",\"code\",null,{\"children\":\"node_modules\"}],\" directory and will be created when the developer installs NPM\\npackages. If the developer is using on OS X, packages compiled on OS X will not\\nwork when the container is running in the Linux VM, so this entry ensures that\\nnode modules are installed on the host OS independently from the container OS.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\".ebextensions/*\"}],\", \",[\"$\",\"code\",null,{\"children\":\".elasticbeanstalk/*\"}],\" and \",[\"$\",\"code\",null,{\"children\":\".ebignore\"}],\" are required by the\\nElastic Beanstalk deploy process, and are outside the scope of execution of the\\nDocker container, and are not required.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The \",[\"$\",\"code\",null,{\"children\":\"app/dist\"}],\" directory contains frontend assets served by both Django and\\nNginx, and are required by both Django and Nginx containers. Because Amazon ECS\\ncannot (currently) directly mount a single volume from one container into\\nanother container, we need these files to be deployed directly on the host OS.\\nDocker can mount the directory on both Django and Nginx containers as a shared\\nvolume. This directory will be deployed by EB, and is thus excluded from Docker.\"]}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"binimage\",\"children\":\"bin/image\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"bin/image\"}],\" is a shell script that wraps common Docker commands used to create\\nDocker images. There are three major subcommands: \",[\"$\",\"code\",null,{\"children\":\"build\"}],\", \",[\"$\",\"code\",null,{\"children\":\"destroy\"}],\" and\\n\",[\"$\",\"code\",null,{\"children\":\"update\"}],\" all take a single argument, the name of the environment subdirectory of\\nthe \",[\"$\",\"code\",null,{\"children\":\"environments/\"}],\" directory. This will spawn Docker and build, delete or\\nrebuild the image as specified by the \",[\"$\",\"code\",null,{\"children\":\"Dockerfile\"}],\" in the directory\\ncorresponding to the final argument.\"]}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"binstevedore\",\"children\":\"bin/stevedore\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"bin/stevedore\"}],\" is a shell script that wraps common Docker commands used to\\nstart and stop Docker containers. There are many subcommands, but the most\\nuseful are: \",[\"$\",\"code\",null,{\"children\":\"start\"}],\", \",[\"$\",\"code\",null,{\"children\":\"stop\"}],\" and either of the two \",[\"$\",\"code\",null,{\"children\":\"build\"}],\" commands. All\\nsubcommands take take a single argument, the name of the environment\\nsubdirectory of the \",[\"$\",\"code\",null,{\"children\":\"environments/\"}],\" directory. This will spawn Docker and start,\\nstop or run the corresponding build process.\"]}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"dockerdjangodevdocker-composeyml\",\"children\":\"docker/django/dev/docker-compose.yml\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"This file provides configuration for Docker to orchestrate the management of the\\ndevelopment Docker container for the local dev environment.\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"django:\\n  build: ../../..\\n  dockerfile: docker/django/dev/Dockerfile\\n  env_file: ../../../environments/dev/.env\\n  volumes:\\n    - \\\"../../../app/apps:/var/app/app/apps\\\"\\n    - \\\"../../../app/dist:/var/app/app/dist\\\"\\n    - \\\"../../../app/project:/var/app/app/project\\\"\\n    - \\\"../../../app/manage.py:/var/app/app/manage.py\\\"\\n    - \\\"../../../environments:/var/app/environments\\\"\\n    - \\\"../../../gulpfile.js:/var/app/gulpfile.js\\\"\\n  ports:\\n    - \\\"80:8080\\\"\\n    - \\\"8010:8010\\\"\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"It defines one container “django”, specifying a path to the \",[\"$\",\"code\",null,{\"children\":\"build\"}],\"-context as\\nwell as a path to load the \",[\"$\",\"code\",null,{\"children\":\"Dockerfile\"}],\". \",[\"$\",\"code\",null,{\"children\":\"env_file\"}],\" specifies the path the a\\nfile containing all environment variables. A set of volumes to share from the\\nhost OS to the container are listed in \",[\"$\",\"code\",null,{\"children\":\"volumes\"}],\". Finally \",[\"$\",\"code\",null,{\"children\":\"ports\"}],\" tells Docker\\nwhich ports on the host to map to the container.\"]}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"dockerdjangodevdockerfile\",\"children\":\"docker/django/dev/Dockerfile\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The \",[\"$\",\"code\",null,{\"children\":\"Dockerfile\"}],\" is a set of instructions for Docker to execute in order to\\nproduce a Docker image— a file used to create a Docker container running\\nyour application code.\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-bash\",\"children\":\"# ...\\n# Install apt, Python then NodeJS dependencies.\\nRUN             apt-get update \u0026\u0026 \\\\\\n                curl -sL https://deb.nodesource.com/setup_0.12 | bash - \u0026\u0026 \\\\\\n                apt-get install -y nodejs \u0026\u0026 \\\\\\n                pip install --upgrade pip \u0026\u0026 \\\\\\n                pip install -r \\\\\\n                    environments/dev/requirements.txt \u0026\u0026 \\\\\\n                npm update \u0026\u0026 \\\\\\n                npm install -g gulp \u0026\u0026 \\\\\\n                npm install \u0026\u0026 \\\\\\n                gulp\\n# Add our initialization script to the image and run it upon startup.\\nADD             docker/django/start.sh /\\nCMD             [\\\"/start.sh\\\"]\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"In the development Dockerfile, \",[\"$\",\"code\",null,{\"children\":\"pip\"}],\" and \",[\"$\",\"code\",null,{\"children\":\"npm\"}],\" commands install the necessary\\ndependencies from the \",[\"$\",\"code\",null,{\"children\":\"environments/dev\"}],\" folder. Finally \",[\"$\",\"code\",null,{\"children\":\"start.sh\"}],\" is called to\\nstart the Django application server.\"]}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"dockerdjangoproddocker-composeyml\",\"children\":\"docker/django/prod/docker-compose.yml\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"This file provides configuration for Docker to orchestrate the management of the\\nproduction Docker containers. This configuration can be used for testing locally\\nprior to deployment to Amazon AWS.\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-yml\",\"children\":\"django:\\n  build: ../../..\\n  dockerfile: docker/django/prod/Dockerfile\\n  env_file: ../../../environments/prod/.env\\n  volumes:\\n    - \\\"../../../docker/django/prod/gunicorn.conf.py:/etc/gunicorn/gunicorn.conf.py:ro\\\"\\n    - \\\"/var/app/app/dist\\\"\\n\\nnginx:\\n  image: nginx\\n  links:\\n    - django\\n  volumes:\\n    - \\\"../../../docker/nginx/nginx.conf:/etc/nginx/nginx.conf:ro\\\"\\n    - \\\"../../../docker/nginx/sites-enabled.conf:/etc/nginx/conf.d/default.conf:ro\\\"\\n  volumes_from:\\n    - django\\n  ports:\\n    - \\\"80:80\\\"\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"It defines two containers “django” and “nginx”. “Django” is configured quite\\nsimilar to the development setup, but doesn’t map as many volumes from the host\\nOS to the container. In this configuration “django” doesn’t need to expose a\\nport externally, as the “nginx” container will expose port 80 externally.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Importantly, the “nginx” container uses \",[\"$\",\"code\",null,{\"children\":\"links\"}],\" to connect the “django”\\ncontainer to “nginx”. This way nginx config files can refer to “django” as-if it\\nwas another host on the same network with the name “nginx”.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Finally, the “nginx” container will mount all volumes from the “django”\\ncontainer with the \",[\"$\",\"code\",null,{\"children\":\"volumes_from\"}],\" directive, and expose port 80 to the host OS.\"]}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"dockerdjangoproddockerfile\",\"children\":\"docker/django/prod/Dockerfile\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The major difference between the development \",[\"$\",\"code\",null,{\"children\":\"Dockerfile\"}],\" and the production\\nversion is:\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"It exposes port 8080 for other containers\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"It runs \",[\"$\",\"code\",null,{\"children\":\"gulp build\"}],\" during image creation\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"This Dockerfile also installs production-only Python pip dependencies in\\nrequirements.txt.\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-bash\",\"children\":\"# Install apt, Python then NodeJS dependencies.\\nRUN             apt-get update \u0026\u0026 \\\\\\n                curl -sL https://deb.nodesource.com/setup_0.12 | bash - \u0026\u0026 \\\\\\n                apt-get install -y nodejs \u0026\u0026 \\\\\\n                pip install --upgrade pip \u0026\u0026 \\\\\\n                pip install -r \\\\\\n                    environments/prod/requirements.txt \u0026\u0026 \\\\\\n                npm update \u0026\u0026 \\\\\\n                npm install -g gulp \u0026\u0026 \\\\\\n                npm install \u0026\u0026 \\\\\\n                gulp build\\n# Exposes port 8080\\nEXPOSE          8080\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"When Docker runs the image build, it runs \",[\"$\",\"code\",null,{\"children\":\"gulp build\"}],\", which runs code-quality,\\nunit tests and produces production-ready web frontend assets. This allows for\\ntesting prior to deployment, and gives the team an opportunity to fix errors\\nbefore they go into the wild.\"]}],\"\\n\",[\"$\",\"h2\",null,{\"id\":\"elastic-beanstalk-specific-structure\",\"children\":\"Elastic Beanstalk specific structure\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Finally, these comments describe files related to Amazons’ Elastic Beanstalk:\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\".dockerignore\\n.ebextensions/           # Describes how EB builds environments\\n01_envvars.config      # Describes env vars for AWS Docker containers\\n.ebignore                # Describes how Amazon EB ignores some files\\n.elasticbeanstalk/       # Location Amazon EB stores its cli settings\\nDockerrun.aws.json       # Describes how to run our containers in AWS\\n.gitignore\\n.bowerrc\\n.csslintrc.json\\n.jshintrc\\nbower.json\\ngulpfile.js\\npackage.json\\napp/\\napps/*\\nproject/*\\ndist/*\\nbin/*\\ndocker/\\ndjango/\\ndev/\\ndocker-compose.yml\\nDockerfile\\nprod/\\ndocker-compose.yml\\nDockerfile\\ngunicorn.conf.py\\nstart.sh\\nnginx/*\\nenvironments/\\ndev/\\n.env\\n.env.example\\nProcfile\\nrequirements.txt\\nprod/\\n.env\\n.env.example\\nProcfile\\nrequirements.txt\"}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"ebextensions01_envvarsconfig\",\"children\":\".ebextensions/01_envvars.config\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"This file is used by the Elastic Beanstalk command line utilities to pass\\nkey-value parameters to Amazon EC2 and ECS. This file is used to store all\\nproduction environment variables that are provided to running containers. This\\nvariables often vary between deployments.\"}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"ebignore\",\"children\":\".ebignore\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"When Elastic Beanstalk does a deployment, it creates a zip file of the current\\ndirectory, uploads it to Amazon S3, and deploys the files to running EC2\\ninstances. Since this project is using Docker images to package the app, we can\\nignore most files; with the exception of the \",[\"$\",\"code\",null,{\"children\":\"app/dist\"}],\" directory— which\\nboth “Django” and “Nginx” containers will need access too.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The \",[\"$\",\"code\",null,{\"children\":\".ebignore\"}],\" file is used to ignore certain files in a project directory.\\nThis file works like a \",[\"$\",\"code\",null,{\"children\":\".gitignore\"}],\" file.\"]}],\"\\n\",[\"$\",\"h1\",null,{\"id\":\"ignore-everything\",\"children\":\"Ignore everything!\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{}],\"\\n\"]}],\"\\n\",[\"$\",\"h1\",null,{\"id\":\"except-for-these-exclusion-patterns-required-by-amazon-ecs\",\"children\":\"Except for these exclusion patterns required by Amazon ECS\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"!Dockerrun.aws.json\\n!.ebextensions/\",[\"$\",\"em\",null,{\"children\":\".config\\n!.elasticbeanstalk/\"}],\".cfg.yml\\n!.elasticbeanstalk/*.global.yml\\n!app/dist/**\\n!docker/**\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The only files our EB package should contain are those required by Docker,\\nElastic Beanstalk itself, or any files shared between both containers (such as\\nthe \",[\"$\",\"code\",null,{\"children\":\"app/dist\"}],\" directory).\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"When you deploy your project directory to Elastic Beanstalk and create a new\\napplication version, the EB CLI will not include files specified by the\\n\",[\"$\",\"code\",null,{\"children\":\".ebignore\"}],\" in the source bundle that it creates. This is useful for creating\\nsmaller packages by excluding files that aren’t required for running\\nproduction-only code.\"]}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"elasticbeanstalk\",\"children\":\".elasticbeanstalk/\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Elastic Beanstalk uses this directory to store temp files and configuration\\ninformation about the current AWS account, EB Application name and IAM\\ncredentials to utilize.\"}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"dockerrunawsjson\",\"children\":\"Dockerrun.aws.json\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"Dockerrun.aws.json\"}],\" is a proprietary Amazon-specific JSON format called a\\n“\",[\"$\",\"a\",null,{\"href\":\"#task-def\",\"children\":\"Task Definition\"}],\"” used to configure how to manage Docker containers\\nrunning on Amazon EC2 Container Service (ECS) platform.\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-json\",\"children\":\"\\\"containerDefinitions\\\": [\\n{\\n    \\\"name\\\": \\\"django\\\",\\n    \\\"image\\\": \\\"0xadada/dockdj:latest\\\",\\n    \\\"essential\\\": true,\\n    \\\"memory\\\": 512,\\n    \\\"mountPoints\\\": [\\n        {\\n            \\\"sourceVolume\\\": \\\"gunicorn-conf\\\",\\n            \\\"containerPath\\\": \\\"/etc/gunicorn/gunicorn.conf.py\\\",\\n            \\\"readOnly\\\": true\\n        }\\n    ]\\n}\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The JSON format is very similar to the docker-compose Yaml format, having a\\nnearly 1-to-1 mapping of \",[\"$\",\"code\",null,{\"children\":\"image\"}],\", \",[\"$\",\"code\",null,{\"children\":\"mountPoints\"}],\" to volumes and ports all\\ndefined.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"This file is functionally identical to \",[\"$\",\"code\",null,{\"children\":\"docker/prod/docker-compose.yml\"}],\" in that\\nit runs, configures and connects the “Django” and “Nginx” Docker containers. As\\nsuch, changes to the \",[\"$\",\"code\",null,{\"children\":\"docker-compose.yml\"}],\" file should be mirrored in the\\n\",[\"$\",\"code\",null,{\"children\":\"Dockerrun.aws.json\"}],\" file.\"]}],\"\\n\",[\"$\",\"h2\",null,{\"id\":\"lifecycle\",\"children\":\"Lifecycle\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"New developers to this project simply clone the project from GitHub, install\\nDocker (and boot2docker/docker-machine on OS X) and can begin running the app.\\nThere is no need to setup a developer environment or create (yet) another VM.\"}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"development\",\"children\":\"Development\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"When the developer starts working on the project from scratch, the only\\nrequirement is Docker and a machine capable of running Docker containers (Linux\\n3+ or boot2docker/docker-machine).\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Once an organization or developer has adopted Docker for a single project,\\nstartup time for other docker projects is drastically reduced as this core\\nrequirement has already been met. From that point forward, the projects\\nthemselves can define and provision their own dependencies.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"For this project, the next steps required of the developer are as follows:\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-bash\",\"children\":\"git clone \u003cPROJECT\u003e\\n\u003ccreate .env file\u003e\\n.bin/stevedore dev start\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The developer is now running the app. Any internal OS configuration, system\\nlibraries, software dependencies and provisioning are all handled by the project\\nand Docker— transparently to the developer.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Subsequent context-switches between other projects and this project have been\\nreduced to a single command:\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-bash\",\"children\":\".bin/stevedore dev start\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The developer doesn’t need to boot up a VM, nor does she/he need to understand\\nor start any internal processes or run any commands internal to the VM.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"In development, Docker can be used to lower cognitive load on developers\\nswitching between multiple projects.\"}]}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"qa\",\"children\":\"QA\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"In this particular project, QA tests are run during build of the the production\\nDocker image via the \",[\"$\",\"code\",null,{\"children\":\"gulp build\"}],\" task. See\\n\",[\"$\",\"a\",null,{\"href\":\"#dockerdjangoproddockerfile\",\"children\":\"the production Dockerfile\"}],\" to view how it calls\\nthe gulp task.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"In effect, this will prevent developers or continuous Integration systems from\\npublishing a production Docker image to Docker Hub, as the build will trigger a\\nDocker image build failure.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"More generally, development teams could create different tags for “production”\\nreleases and “development” Docker image releases. Lets say “prod” vs “dev”.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Development teams could publish images tagged with “dev”, to Docker Hub. Other\\ndevelopers on the team or members of the QA team could \",[\"$\",\"code\",null,{\"children\":\"docker pull\"}],\" that tagged\\nimage and run their suite of tests on it.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Using Docker in this manner, dev and QA teams no longer have to keep VM\\nconfigurations synchronized, as the OS and other dependencies have been pushed\\ndown from VM directly into dependencies within the scope of the project. \",[\"$\",\"strong\",null,{\"children\":\"This\\nhas the effect of reducing manual synchronization and de-necessitating\\nout-of-channel communication between development and QA teams about the state of\\nthe runtime environment.\"}],\" This allows for faster, less-error-prone iteration of\\nthe runtime environment.\"]}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"production\",\"children\":\"Production\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Developers have iterated on functionality, QA has run tests against the code,\\nand the projects is ready for deployment to production.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"At this point, a working Docker image has been run on developers local machines,\\nand QA has passed. These three phases could’ve gone through multiple iterations\\nwhile bugs were identified and fixed. The end result is a working Docker image\\nexists that has been deemed “ready” for production.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Either manually or as part of a continuous integration tool, the\\nproduction-ready Docker image can now be tagged with a release version and\\npublished to Docker Hub (or other compatible Docker image repository). Finally,\\nthe deploy process needs to update the production servers running our working\\napplication stack and run the latest application code.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"These tasks are handled by our \",[\"$\",\"code\",null,{\"children\":\"bin/deploy\"}],\" script, a wrapper for Docker, Git\\nand Elastic Beanstalk. It will tag the latest Docker image, publish the tag to\\nDocker Hub, tag the publish the tags to GitHub and use Elastic Beanstalk to\\ndeploy both the latest stack and application code:\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"(Lets use 1.2.3 as an arbitrary version number for this example)\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-bash\",\"children\":\"bin/deploy release 1.2.3 # Create a release branch and tag the image\\nbin/deploy publish 1.2.3 # Publish the Docker image and git branch\\n                         # to Docker Hub and GitHub\\nbin/deploy deploy 1.2.3  # Use EB to deploy the latest release\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The deploy script is a light bash wrapper that automates Git, Docker and Elastic\\nBeanstalk commands in an easy-to-reproduce set of short commands.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Once complete, the Amazon environment will be running your latest application\\ncode, as well as any new changes to the container OS, system libraries and\\ndependencies. Most importantly, \",[\"$\",\"strong\",null,{\"children\":\"any changes in provisioning to the stack have\\nbeen deployed along with the Docker image\"}],\", thus enabling seamless roll-backs.\\n\",[\"$\",\"strong\",null,{\"children\":\"Rolling back the application version will also rollback the stack version\"}],\".\\nThe application and stack are deployed together.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Happy cloud computing!\"}],\"\\n\",[\"$\",\"h2\",null,{\"id\":\"terms\",\"children\":\"Terms\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"a\",null,{\"name\":\"autoscaling\",\"children\":[\"$\",\"em\",null,{\"children\":\"Auto-scaling\"}]}],\" A method of setting a threshold that\\ndetects when the load on a server cluster necessitates adding or removing\\nservers in order to optimize the number of servers servicing that load.\\nAuto-scaling allows an organization to decrease operating costs by running the\\nminimum number of servers required to service its load, and eliminating the\\nneed to accurately predict future traffic patterns.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"a\",null,{\"name\":\"config-mgmt\",\"children\":[\"$\",\"em\",null,{\"children\":\"Configuration Management\"}]}],\" Software tools that are\\ndesigned to automatically start, provision and configure software on virtual\\nmachines rather than have engineers run these steps manually on each server.\\nThese tools can be used both locally to create development VMs (virtual\\nmachines) as well as in the cloud to create staging and production VMs.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"a\",null,{\"name\":\"docker-image\",\"children\":[\"$\",\"em\",null,{\"children\":\"Docker image\"}]}],\" A docker image is like an executable\\nprogram binary. It takes source files and other assets and bundles them\\ntogether, and the resulting bundle can be run/executed as a single process on\\na Linux machine.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"a\",null,{\"name\":\"docker-container\",\"children\":[\"$\",\"em\",null,{\"children\":\"Docker container\"}]}],\" A docker container is like a\\nrunning executable program. It is a running instance of a docker image. Like a\\nrunning program, it has a PID, and it is appropriate to call it a process. It\\ncan be started and stopped. One docker image can be run many times on one or\\nmore machines.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"a\",null,{\"name\":\"provisioning\",\"children\":[\"$\",\"em\",null,{\"children\":\"Provisioning\"}]}],\" The installation and configuration\\nof software needed to run an application. E.g. Installing and configuring\\nApache and its system libraries in order to run WordPress.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"a\",null,{\"name\":\"task-def\",\"children\":[\"$\",\"em\",null,{\"children\":\"Task definition\"}]}],\" A proprietary JSON format for\\ndescribing how Docker containers are run within the Amazon EC2 Cloud Service.\\nRead more about\\n\",[\"$\",\"a\",null,{\"href\":\"http://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_defintions.html\",\"children\":\"Amazon ECS Task Definitions\"}],\".\\nDocker uses the \",[\"$\",\"a\",null,{\"href\":\"https://docs.docker.com/compose/yml/\",\"children\":\"docker-compose Yaml\"}],\"\\nfile to do the same thing.\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"section\",null,{\"data-footnotes\":true,\"className\":\"footnotes\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"sr-only\",\"id\":\"footnote-label\",\"children\":\"Footnotes\"}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"id\":\"user-content-fn-1\",\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Configuration management tools can be used to couple both Stack and\\nApplication, but experience has has shown that over time, these tools\\nare not strongly opinionated, and therefore Stack-App decoupling occurs\\norganically over the lifetime of a project. \",[\"$\",\"a\",null,{\"href\":\"#user-content-fnref-1\",\"data-footnote-backref\":true,\"className\":\"data-footnote-backref\",\"aria-label\":\"Back to content\",\"children\":\"↩\"}]]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"id\":\"user-content-fn-2\",\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Docker runs on Linux version 3. In the case where the developer is\\nusing OS X, Windows or another non-Linux OS, they’ll need to run a Linux\\nVM in order to use Docker. However, this single VM will be able to run\\nall Docker containers for all Docker projects they use. Tools like\\nDocker Machine make working with the Docker VM much simpler. \",[\"$\",\"a\",null,{\"href\":\"#user-content-fnref-2\",\"data-footnote-backref\":true,\"className\":\"data-footnote-backref\",\"aria-label\":\"Back to content\",\"children\":\"↩\"}]]}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}]]}],[\"$\",\"footer\",null,{\"className\":\"license_copyright__EpMnZ\",\"children\":[\"This is licensed under a Creative Commons \",\"cc-by-nc-sa\",\" International License\"]}]]}]\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Development Lifecycle with Docker and Elastic Beanstalk\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"This article explains the advantages of using Docker over automated configuration management tools, and describes a workflow from development through QA and into production deployment using Amazons' Elastic Beanstalk.\"}],[\"$\",\"link\",\"4\",{\"rel\":\"author\",\"href\":\"https://0xadada.pub\"}],[\"$\",\"meta\",\"5\",{\"name\":\"author\",\"content\":\"0xADADA\"}],[\"$\",\"meta\",\"6\",{\"name\":\"keywords\",\"content\":\"essays,software-engineering\"}],[\"$\",\"link\",\"7\",{\"rel\":\"alternate\",\"type\":\"application/rss+xml\",\"href\":\"https://0xadada.pub/rss.xml\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:title\",\"content\":\"Development Lifecycle with Docker and Elastic Beanstalk\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:description\",\"content\":\"This article explains the advantages of using Docker over automated configuration management tools, and describes a workflow from development through QA and into production deployment using Amazons' Elastic Beanstalk.\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:url\",\"content\":\"https://0xadada.pub/2015/11/03/development-lifecycle-with-docker-and-elastic-beanstalk/\"}],[\"$\",\"meta\",\"11\",{\"property\":\"og:site_name\",\"content\":\"0xADADA\"}],[\"$\",\"meta\",\"12\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"13\",{\"property\":\"og:image\",\"content\":\"https://0xadada.pub/static/images/meta/avatar.svg\"}],[\"$\",\"meta\",\"14\",{\"property\":\"og:image:width\",\"content\":\"660\"}],[\"$\",\"meta\",\"15\",{\"property\":\"og:image:height\",\"content\":\"660\"}],[\"$\",\"meta\",\"16\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"17\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"18\",{\"name\":\"twitter:title\",\"content\":\"Development Lifecycle with Docker and Elastic Beanstalk\"}],[\"$\",\"meta\",\"19\",{\"name\":\"twitter:description\",\"content\":\"This article explains the advantages of using Docker over automated configuration management tools, and describes a workflow from development through QA and into production deployment using Amazons' Elastic Beanstalk.\"}],[\"$\",\"meta\",\"20\",{\"name\":\"twitter:image\",\"content\":\"https://0xadada.pub/static/images/meta/avatar.svg\"}],[\"$\",\"meta\",\"21\",{\"name\":\"twitter:image:width\",\"content\":\"660\"}],[\"$\",\"meta\",\"22\",{\"name\":\"twitter:image:height\",\"content\":\"660\"}],[\"$\",\"link\",\"23\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"48x48\"}],[\"$\",\"link\",\"24\",{\"rel\":\"apple-touch-icon\",\"href\":\"/apple-icon.png?b764b3a1dbf00a82\",\"type\":\"image/png\",\"sizes\":\"180x180\"}]]\n"])</script><script>self.__next_f.push([1,"5:null\n"])</script><script>self.__next_f.push([1,""])</script></body></html>