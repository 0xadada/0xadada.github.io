<!DOCTYPE html><html><head><meta charSet="utf-8"/><title>Notes on Programming Phoenix for Elixir</title><meta property="og:site_name" content="0xADADA"/><meta property="og:title" content="Notes on Programming Phoenix for Elixir"/><meta name="twitter:title" content="Notes on Programming Phoenix for Elixir"/><link rel="canonical" href="https://0xadada.pub/2017/12/22/programming-phoenix/"/><link type="application/rss+xml" rel="alternate" href="/rss.xml" title="0xADADA"/><meta property="og:url" content="https://0xadada.pub/2017/12/22/programming-phoenix/"/><meta name="twitter:url" content="https://0xadada.pub/2017/12/22/programming-phoenix/"/><meta name="twitter:card" content="summary"/><meta name="twitter:site" content="@0xadada"/><meta name="author" content="0xADADA"/><meta property="og:type" content="article"/><meta name="description" content="Notes and observations on the Phoenix web framework for Elixir"/><meta name="og:description" content="Notes and observations on the Phoenix web framework for Elixir"/><meta name="twitter:description" content="Notes and observations on the Phoenix web framework for Elixir"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="icon" href="https://0xadada.pub/favicon.ico"/><link rel="home" href="https://0xadada.pub/"/><link rel="manifest" href="https://0xadada.pub/static/images/meta/0xadada.webmanifest"/><link rel="apple-touch-icon" href="https://0xadada.pub/static/images/meta/apple-touch-icon.png"/><link rel="icon" type="image/png" href="https://0xadada.pub/static/images/meta/favicon-32x32.png" sizes="32x32"/><link rel="icon" type="image/png" href="https://0xadada.pub/static/images/meta/favicon-16x16.png" sizes="16x16"/><link rel="mask-icon" href="https://0xadada.pub/static/images/meta/safari-pinned-tab.svg" color="#5bbad5"/><meta name="theme-color" content="#FDF9F0"/><meta name="pocket-site-verification" content="7431f135e23a84de547e5b79dab406"/><meta name="next-head-count" content="26"/><link rel="preload" href="/_next/static/css/49969f6af05533cc.css" as="style"/><link rel="stylesheet" href="/_next/static/css/49969f6af05533cc.css" data-n-g=""/><link rel="preload" href="/_next/static/css/e491c5ad83d0116a.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e491c5ad83d0116a.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-514908bffb652963.js" defer=""></script><script src="/_next/static/chunks/framework-91d7f78b5b4003c8.js" defer=""></script><script src="/_next/static/chunks/main-22b044904a3f81e0.js" defer=""></script><script src="/_next/static/chunks/pages/_app-929aade252cc0f2b.js" defer=""></script><script src="/_next/static/chunks/392-e44e3e95fe0288b1.js" defer=""></script><script src="/_next/static/chunks/pages/%5Byear%5D/%5Bmonth%5D/%5Bday%5D/%5Bslug%5D-a818a8866f202bc1.js" defer=""></script><script src="/_next/static/8lVdZxFQRcnwEza8kfMUH/_buildManifest.js" defer=""></script><script src="/_next/static/8lVdZxFQRcnwEza8kfMUH/_ssgManifest.js" defer=""></script><script src="/_next/static/8lVdZxFQRcnwEza8kfMUH/_middlewareManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><main class="layout_layout__Xf50c"><article class="hentry h-entry" lang="en-US"><header><h1 class="entry-title p-name">Notes on Programming Phoenix for Elixir</h1><time class="byline_published__8ePA_ dt-published" dateTime="2017-12-22T15:54:00.000Z">Friday December 22, 2017</time><br/><span class="byline_byline__jNP6Q h-card">by: <span class="author fn p-author p-name">0xADADA</span></span></header><div class="entry-content e-content"><p>A summary of my notes taken while reading
<a href="https://pragprog.com/book/phoenix/programming-phoenix">Programming Phoenix</a>.</p>
<h2>Ch.1</h2>
<p>The layers of phoenix, the endpoint is where the HTTP connection contacts
phoenix, from there it goes to the router which directs a request to the
appropriate controller, passing through a series of pipelines. Pipelines chain
functions together to handle tasks that span across multiple controllers, such
as browser requests vs API requests.</p>
<pre><code>connection
|> endpoint
|> router
|> pipelines
|> controller
</code></pre>
<p>The purpose of the web server is to route a request to a function that performs
the right task, called an action. These functions are grouped into controllers.
A controller is also a series of pipelines</p>
<pre><code>connection
|> controller
|> common_services
|> action
</code></pre>
<p>An action to show a user would look like this:</p>
<pre><code>connection
|> find_user
|> view
|> template
</code></pre>
<p>Run phoenix inside an interactive shell: iex -S mix phx.server</p>
<p>In controllers, parameters come in as a Map of keys with string names:</p>
<pre><code>def world(conn, %{"name" => name}) do    # string key
  render conn, "world.html", name: name  # atom key
end
</code></pre>
<p>Phoenix specifically converts these from String keys to atoms once inside the
application boundry, because external data can't safely be converted to atoms.</p>
<p>Phoenix encourages breaking big functions down into smaller ones. It provides a
place to explicitly register each smaller function in a way that makes it easy
to understand and replace. These small pieces are orchestrated with the Plug
library. The Plug library is a specification for building applications that
connect with the web. Plug produces and consumes a struct called Plug.Conn,
describing the entire request/response cycle of a web application, containing
the request, protocol, parameters, etc. Each individual Plug should do one-thing
and do it well, take the conn struct, transform it, and return it.</p>
<p>Plugs are just functions, and Phoenix web applications are just pipelines of
Plugs.</p>
<p>If you look at endpoint.ex you'll see that its essentially a pipeline of
functions chained together at the beginning of each request:</p>
<pre><code>connection
|> Plug.Static.call
|> Plug.RequestId.call
|> Plug.Logger.call
|> Plug.Parsers.call
|> Plug.MethodOverride.call
|> Plug.Head.call
|> Plug.Session.call
|> &#x3C;your app>.Router.call
</code></pre>
<p>The last call, to <your app>.Router.call is where endpoint.ex defers to your
applications router.ex to define which controller, and subsequently, which
function is invoked next in the pipeline chain. The router defines a set of
pipelines for dealing with which Plugs to use prior to your conroller being
called, typically:</p>
<pre><code>connection
|> endpoint
|> router
|> pipeline(s)
|> controller
</code></pre>
<p>The pipeline(s) above will be things like :browser, handling common tasks as
preventing request forgery, <code>:fetch_session</code>, <code>:fetch_flash</code>.</p>
<p>The controller is where business logic specific to your application lives,
making data available for a view. The view subsequently substitutes values
inside a template which is rendered.</p>
<h2>Ch.2</h2>
<p>Start an interactive Elixir shell within an Elixir application project:</p>
<pre><code>iex -S mix
</code></pre>
<p>Structs add a constraint to Maps in that they offer protection from bad or
misspelled keynames at compilation time, while Maps only throw errors during
runtime when the key is first accessed. Under the covers, a Struct is just a Map
with a <strong>struct</strong> key.</p>
<h2>Ch3</h2>
<p>A view is a module containing a rendering function that converts data into a
format the user will consume, like HTML or JSON. A template is a function on the
view module, compiled from a file containing raw markup language and embedded
Elixir code to process substitutions and loops. Views are modules responsible
for rendering, and templates are the compiled functions that have static markup
and native code to build pages.</p>
<p>EEx executes Elixir code within <code>&#x3C;%= %></code> tags, injecting the result into the
template. EEx evalutes code within <code>&#x3C;% %></code> tags, but without injecting the
result.</p>
<p>The expression <code>&#x3C;%= for user &#x3C;- @users do %></code> walks through the users, rendering
each user using the template block between the <code>do ... end</code> block, rolling the
result up into the template.</p>
<p>In Phoenix, after compilation, templates are just functions. These templates are
just linked lists (rather than other web frameworks that use string
concatenation)– this means Phoenix doesn't have to make huge copies of large
strings, resulting in performance improvements. This lets Elixir leverage modern
CPU optimization since it only has a single copy of the largest strings in your
application.</p>
<p>When a template is added to Phoenix, the view module extracts the template from
the filesystem and adds it as a function on the module itself, thats why the
view module is required in the first place.</p>
<h2>Ch.4</h2>
<p>Ecto uses a DSL that specifies how the fields in a struct map to database tables
and columns. The DSL is built using Elixir macros. Virtual fields are not
persisted to the database.</p>
<p>Models (called contexts &#x26; schemas in Phoenix 1.3) are layers of functions to
transform data according to business logic. A schema is the native database form
of the data, and a struct is the data once pulled out of the database.
Changesets in Ecto decouple the authentication update policy separately from the
database schema. This policy segregation makes handling change over time.</p>
<p>Ecto helps you write code to make the minimal required database operation to
update a record. Ecto can enforce constraints without hitting the database.</p>
<h2>Ch.5</h2>
<p><code>Ecto.Changeset.cast</code> converts a bare Map to a changeset, for security it limits
the incoming parameters to those specified.</p>
<p>Module plugs vs function plugs: A function plug is a single function, a module
plug is a module that provides two functions with some configuration details,
otherwise they work the same. <code>lib/rumbl_web/endpoint.ex</code> is an example of a
module plug while <code>lib/rumbl_web/router.ex</code> is an example of a function plug.
You specify a module plug by providing a module name plug Plug.Logger while a
function plug is specified by its name as an atom plug <code>:protect_from_forgery</code>.</p>
<p>To share a Plug across more than one module, you want to use a Module Plug. All
plugs take a connection and return a connection.</p>
<p>Any values stored in conn.assigns.* is automatically available with the same
name in our view. So if you assign to current_user, then @current_user can be
used directly in a view.</p>
<h2>Ch.6 Generators &#x26; Relationships</h2>
<p>Generating resources, phx.gen.html generates simple HTML based scaffolds,
phx.gen.json generates a REST-based API using JSON. They create resources for
CRUD operations, creating migrations, controllers, and templates as well as
tests.</p>
<p>An example:</p>
<pre><code>mix phx.gen.html Medias \
  Video videos \
  user_id:references:users \
  url:string \
  title:string \
  description:text
</code></pre>
<p>This includes the name of the context that holds the module, the module defines
the model. The plural form of the model name, and each field with some type
information. Phoenix consistently uses singular forms in models, controllers,
and views. At the application boundry (URLs, table names you provide a bit more
information because you can use pluralized names). Instead of using inflectors,
you just explicitly set the pluralized form yourself.</p>
<p>Primary keys identify rows for each item in a table. Foreign keys point from one
table to the primary key in another table. Foreign key lets the database get in
on the act of maintaining consistency across our two relationships.</p>
<p>The change function handles two database functions, one for migrating up and one
for migrating down. A migration up applies a migration, and a migration down
reverts it. If you make a mistake a need to move a single migration up or down,
you can do so.</p>
<p>If you meant to add a <code>view_count</code> field to your create_video migration, but
before you migrated your database up. You would create a new migration adding
the view_count field, since you haven't pushed your changes to production yet,
you can roll back, make your changes, and then migrate up again. First you'd
roll back your changes: mix ecto.rollback Verify the undo of the create_video
migration. At this point, add the missing view_count field and migrate forwards
with mix ecto.migrate.</p>
<p>A schema is responsible for tying the database fields to a field in the Elixir
struct, these are defined in <code>lib/&#x3C;appname>/&#x3C;contextname>/&#x3C;model>.ex</code>.</p>
<p>Ecto associations are explicit, you need to ask to fetch associated records
specifically. Most persistence frameworks often fetch rows you don't need or
fetch them in inefficient ways, over time these inefficiencies add up to major
performance problems. Ecto forces the developer to pay down these performance
hits early so they don't add up over time, when they're more difficult to fix.</p>
<pre><code>iex> user = Repo.get_by!(User, username: "josevalim")
%{Rumbl.User{...}
iex> user.videos
#Ecto.Association.NotLoaded&#x3C;association :videos is not loaded>
iex> user = Repo.preload(user, :videos)
%{Rumbl.User{...}
iex> user.videos
[] # loaded, but none are associated! yay
</code></pre>
<p>Repo.preload accepts one or a collection of association names, and fetches all
associated data.</p>
<pre><code>iex> user = Repo.get_by!(User, username: "josevalim")
iex> attrs = %{title: "hi", description: "says hi", url: "example.com"}
iex> video = Ecto.build_assoc(user, :videos, attrs)
iex> video = Repo.insert!(video)
</code></pre>
<p>Ecto.build_assoc builds a struct, with the proper relationship fields already
set.</p>
<p>To fetch videos associated with a user, without storing them in a user struct:</p>
<pre><code>iex> query = Ecto.assoc(user, :videos)
iex> Repo.all(query)
[%Rumbl.Video{...}]
</code></pre>
<p>assoc is convenient in that it returns an Ecto.Query all videos scoped to a
specific user, or a list of users, we convert this query into data by calling
Repo.all.</p>
<h2>Ch.7 Ecto Queries and Constraints</h2>
<p>Seeds are small scripts that populate the database with values every time the
script is run. Phoenix stores seed scripts in priv/repo/seeds.ex. Mix tasks will
run these scripts. Use mix run priv/repo/seeds.exs to add the seed data to the
database.</p>
<pre><code>Repo.all from c in Category, select: c.name
</code></pre>
<p><code>Repo.all</code> means return all rows, <code>from</code> is a macro that builds a query,
<code>c in Category</code> means we're pulling rows (labeled c) from the Category schema.
<code>select: c.name</code> means we're going to return only the name field.</p>
<pre><code>Repo.all from c in Category, order_by: c.name, select: c.name
</code></pre>
<p>will order the results by name and return a tuple containing the name and the id
fields.</p>
<p>Ecto queries are composable, you don't need to define the entire query at once,
you can combine them bit-by-bit.</p>
<pre><code>iex> query = Category
iex> query = from c in query, order_by: c.name
iex> query = from c in query, select: {c.name, c.id}
#Ecto.Query&#x3C;>
iex> Repo.all(query)
[...]
</code></pre>
<p>This works because Ecto defines a queryable protocol. from receives a queryable,
and you can use any queryable as a base for a new query. An Elixir protocol
defines an API for specific language features, this one defines the API for
something that can be queried. This is why we can invoke Repo.all(Category) or
Repo.all(query) because both Category and query implement the <code>Ecto.Queryable</code>
protocol. By abiding to the protocol, developer can quickly layer together
sophisticaed queries with Ecto.Query, maintaining sophistication without
complexity.</p>
<p>Code that builds and transforms queries, and code that interacts with the
repository should belong to the context. Code that makes requests for the data
should belong to the controller—because the controller is where the web logic
should live, and the database layer should be hidden within the application
context with the rest of the business logic.</p>
<pre><code>iex> username = "josevalim"
iex> Repo.one(from u in User, where: u.username == ^username)
%Rumbl.User{...}
</code></pre>
<p>Repo.one returns one row, from u in user means read from the User schema. where
u.username == ^username means return the row <code>where u.username == ^username</code>,
using the pin <code>^</code> operator means we don't want to assign the username but use
its value.</p>
<p><code>Repo.one</code> does not mean "return the first result" but "one result is expected",
so if there is more, it fails. The Ecto Query API is not about composing query
strings, it uses Elixir macros such that Ecto knows where user-defined variables
are located, it's easier to protect the user from security flaws like
SQL-injection attacks. It also helps a bit with query normalization and
leverages the data types as defined in the schema for casting values at runtime.</p>
<p>Any functions with side effects—the ones that change the world—should remain in
the controllers, while the context, model, and view layers remain side effect
free. The controller receives data, either from a traditional web request,
reading data from a socket, and this data is passed from the controller to
various functions that transform it as it moves through the functions to the
shape of our business-model requirements. Finally it makes changes to the world
around us, either delivering emails, adding entries to a database, or invoking a
view which is again written to the connection (another side effect), any of
which can result in a business operation.</p>
<p>The query API supports many operators:
<code>==, !=, &#x3C;=, >=, &#x3C;, >, and, or, not, in, like, ilike, is_nil, count, avg, sum, min, max, datetime_add, date_add, fragment, field, type</code>.</p>
<p>Keyword syntax uses a keyword list to express a query.</p>
<pre><code>iex> Repo.one from u in User,
              select: count(u.id),
              where: ilike(u.username, ^"j%") or
                     ilike(u.username, ^"c%")
</code></pre>
<p>the u variable is bound as part of Ecto's from macro, representing entries from
the User schema. Each join in a query gets a special binding.</p>
<pre><code># count users
iex> users_count = from u in User, select: count(u.id)
#Ecto.Query&#x3C;from u in Rumbl.User, select: count(u.id)>
# count usernames with a j
iex> j_users = from u in users_count, where: ilike(u.username, ^"%j%")
#Ecto.Query&#x3C;from u in Rumbl.User, where: ilike(u.username, ^"%j%"), select: count(u.id)>
</code></pre>
<p>This query builds up a new query, normalizing as it builds, upon the saved
query, we even built the query using the same bound variable name, u, but we
didn't have to.</p>
<p>The pipe syntax allows developer to build queries by piping through query
macros. Each pipe takes a queryable and returns a queryable.</p>
<pre><code>iex> User |>
     select([u], count(u.id)) |>
     where([u], ilike(u.username, ^"j%") or ilike(u.username, ^"c%")) |>
     Repo.one()
[debug] QUERY OK source="users" db=4.5ms
SELECT count(u0."id") FROM "users" AS u0 WHERE ((u0."username" ILIKE $1) OR (u0."username" ILIKE $2)) ["j%", "c%"]
5
</code></pre>
<p>Because each segment of the pipe works independently of the others, we need to
specify the binding manually for each one.</p>
<p>Fragments offer an escape hatch from Ecto's Query API. The best abstractions
offer an escape hatch, and since Ecto's Query API doesn't represent every query
the database layer can provide, Ecto's query fragments send part of the query
directly to the database but allows you to construct it in a safe way, like
this: iex> from(u in User, where: fragment("lower(username) = ?",
^String.downcase(uname)))</p>
<p>When all else fails, you can directly run SQL statements with
Ecto.Adapters.SQL.Query:</p>
<pre><code>iex> Ecto.Adapters.SQL.query(Rumbl.Repo, "SELECT power($1, $2)", [2, 10])
SELECT power($1, $2) [2, 10]
{:ok, %Postgrex.Result{ columns: ["power"], command: :select, connection_id: 5979, num_rows: 1, rows: [[1024.0]] }}
</code></pre>
<p>Ecto relationships are explicit:</p>
<pre><code>iex> user = Repo.one from(u in User, limit: 1)
iex> user.videos
#Ecto.Association.NotLoaded&#x3C;association :videos is not loaded>
iex> user = Repo.preload(user, :videos)
iex> []
</code></pre>
<p>Ecto allows us to preload associations directly as part of a query:</p>
<blockquote>
<p>iex> Repo.all from u in User, join: v in assoc(u, :videos), join: c in
assoc(v, :category), where: c.name == "Comedy", select: {u, v}
[{Rumbl.User{...}, %Rumbl.User{...}}]</p>
</blockquote>
<p>Constraints allow developers to use underlying relational, and can solve
potential race conditions:</p>
<ol>
<li>User sends a category ID through the form.</li>
<li>Perform a query to check if the category ID exists in the DB.</li>
<li>If the category ID exists, add the video with the category ID to the
database.</li>
</ol>
<p>However, someone could delete the category ID between steps 2 and 3, allowing a
video insertion without and existing category. This will lead to inconsistent
data over time. Phoenix uses constrains to manage change in a way that combines
the harsh protections of the database with Ecto's gentle guiding hand to report
errors without crashing. Some terms:</p>
<p>constraint: An explicit database constraint, a uniqueness constrain, or an
index, or and integrity constraint between primary and foreign keys.</p>
<pre><code>constraint error: The Ecto.ConstraintError.
changeset constraint: A constraint annotation allowing Ecto to convert constraint errors into changeset error messages.
changeset error messages: Beautiful error messages for the consumption of people.
</code></pre>
<p>Ecto allows the application layer (and web server) to use database services like
referential integrity and transactions to strike a balance between the needs of
the application layer and the needs of the database. Ecto rewards developers the
many guarantees databases offer with data integrity in the short term, by
transforming constraint errors into user feedback, and in the longer term by
guaranteeing you wont be awake at 3am fixing bugs caused by inconsistent data.</p>
<p>IEx allows us to fetch a previous value using v(n), where n is the number of the
expression, you can alsop ass negative values to grab the last nth expression.</p>
<pre><code>iex> alias Rumbl.Repo
Rumbl.Repo
iex> category = Repo.get_by(Rumbl.Medias.Category, name: "Drama")
%Rumbl.Medias.Category{
  __meta__: #Ecto.Schema.Metadata&#x3C;:loaded, "categories">,
  id: 2,
  inserted_at: ~N[2017-11-05 18:02:12.006642],
  name: "Drama",
  updated_at: ~N[2017-11-05 18:02:12.006653]
}
iex> Repo.delete(category)
[debug] QUERY ERROR db=6.5ms
DELETE FROM "categories" WHERE "id" = $1 [2]
** (Ecto.ConstraintError) constraint error when attempting to delete struct: * foreign_key: videos_category_id_fkey
</code></pre>
<p>We previously added a video using the Drama category, so the database prevents
deletion of the category as it is tied to the video and would create orphaned
records. Use foreign_key_constraint, which is like assoc_constraint used
earlier, except it doesn't inflect the foreign key from the relationship.</p>
<p>This is particularly useful if you want to provide reasons why a category can
not be deleted.</p>
<pre><code>iex> import Ecto.Changeset
Ecto.Changeset
iex(22)> changeset = Ecto.Changeset.change(category)
#Ecto.Changeset&#x3C;action: nil, changes: %{}, errors: [], data: #Rumbl.Medias.Category&#x3C;>, valid?: true>
iex> changeset = foreign_key_constraint(changeset, :videos, name: :videos_category_id_fkey, message: "still exists")
#Ecto.Changeset&#x3C;action: nil, changes: %{}, errors: [],
            data: #Rumbl.Medias.Category&#x3C;>, valid?: true>
iex> Repo.delete(changeset)
[debug] QUERY ERROR db=8.8ms
DELETE FROM "categories" WHERE "id" = $1 [2]
{
  :error,
  #Ecto.Changeset&#x3C;
    action: :delete,
    changes: %{},
    errors: [videos: {"still exists", []}],
    data: #Rumbl.Medias.Category&#x3C;>,
    valid?: false
  >
}
</code></pre>
<p>because the foreign key is established in the videos table, we need to
explicitly call this out in the call to <code>foreign_key_constraint</code>. The work best
suited for the database must be done in the database.</p>
<p>The <code>constraint</code> changeset functions are useful when the constraint being mapped
is triggered by external data, often as part of the user request. Using
changeset constraints only makes sense if the error message can be something the
user can take useful action on.</p>
<h2>Ch.8 Testing MVC</h2>
<p>Testing principals</p>
<ul>
<li>Fast — tests should run quickly and concurrently if possible</li>
<li>Isolated — Tests that are too isolated wont have enough context to be useful,
tests that aren't isolated enough are difficult to understand and maintain</li>
<li>DRY — Strive to eliminate repetition in tests</li>
<li>Repeatable — The same test on the same code should always yield the same
result</li>
<li>unit test — exercises a function for one layer of your application</li>
<li>integration test — focuses on the way different layers of an application fit
together. An example is a controller test that makes a request to and
endpoint, flows through the pipelines, reads from the database, and renders
through views just like a Phoenix request would</li>
<li>acceptance test — test how multiple actions work together. A single acceptance
test case may sign the user on, perform several calculations that build upon
each other, then sign off.</li>
</ul>
<p>ExUnit has three main macros, setup macro specifies setup code that runs before
each test is run. test macro defines a single isolated test, it defines a
hypothesis about code. Finally, assert macro specifies something we believe to
be true about code, if true, the test passes.</p>
<pre><code>defmodule MyTest do
  use ExUnit.Case, async: true
  setup do
    :ok
  end
  test "pass" do
    assert true
  end
  test "fail" do
    assert false
  end
end
</code></pre>
<p>This code runs two tests, first setup is run, then the "pass" test case. Next
setup is run, and the "fail" test case. The output is a passing test case and a
failing test case.</p>
<p>Tests will generally use <code>&#x3C;AppName>.ConnCase</code> meaning you get
<code>&#x3C;AppName>.Router.Helpers</code>, and Ecto imports along for free.</p>
<p>ExUnit calls tests with</p>
<pre><code>test "GET /", %{conn: conn} do
  conn = get(conn, "/")
  assert html_response(conn, 200) =~ "Welcome to Phoenix!"
end
</code></pre>
<p>by calling get(conn, "/") rather than calling the index action on the controller
directly. This practice gives the test the right level of isolation because
we're using the controller the same way Phoenix does. Phoenix also provides test
helpers to make testing responses easier.</p>
<pre><code>assert html_response(conn, 200) ~= "Welcome to Phoenix!"
</code></pre>
<p>which does the following:</p>
<pre><code>asserts the response was 200
content-type was text/html
return the response body, allowing us to match on the contents
</code></pre>
<p>this also exists for JSON called json_response that allow for assertions like</p>
<pre><code>assert %{user_id: user.id} = json_response(conn, 200)
</code></pre>
<p>ExUnit tags help when setup is different from test to test. When specifying a
tag, ExUnit makes that information available within the setup block via
callbacks.</p>
<pre><code>setup %{conn: conn} = config do
  if username = config[:login_as] do
    user = insert_user(username: username)
    conn = assign(conn, :current_user, user)
    {:ok, conn: conn, user: user}
  else
    :ok
  end
end
@tag login_as: "max" # this is the tag
test "list all user's videos on index", %{conn: conn, user: user} do
# ...
</code></pre>
<p>tags take either a keyword list (above) or an atom as arguments. The config map
contains the conn and tag (either atom or keyword list). In this case if
login_as has a value, we use it to login the user and return an updated conn
with the user, otherwise we return an :ok.</p>
<p>Writing unit tests directly against a function like the Auth plug will result in
unexpected errors during the testing/coding cycle because calls directly to the
function will get results that have not gone through the endpoint, router or
other pipelines. The data has not been fully transformed, so the results will
not reflect the state that the implementation sees when running in a server. The
bypass_through helper that ConnCase provides allows developer to send a
connection through the endpoint, router, and desired pipelines but bypass the
route dispatch. This approach gives developer a connection wired up with all the
transformations the specific tests require, such as fetching the session and
adding flash messages.</p>
<pre><code>setup %{conn: conn} do
  conn =
    conn
    |> bypass_through(RumblWeb.Router, :browser)
    |> get("/")

  {:ok, %{conn, conn}}
end
test "authenticate_user halts when no current_user exists", %{conn: conn} do
  conn = Auth.authenticate_user([])
  assert conn.halted
end
</code></pre>
<p>The setup block calls <code>bypass_through</code>, passing the router and the :browser
pipeline to invoke. When the get request is invoked, it accesses the endpoint
and stops at the browser pipeline, as requested. The path "/" given to the get
isn't used by the router when bypassing, its just stored in the connection, this
provides all the requirements for a plug with a valid session and flash message
support.</p>
<p>Since most repository-related functionality will be tested with integration
tests as they insert and update records, but we want to be sure to catch some
error conditions as close to the breaking point as possible. One example is the
uniqueness constraint checks in the changeset. It has side effects because we're
going to need to create a record and then test against it.</p>
<h2>Ch. 9: Watching Videos</h2>
<pre><code>alter table(:videos) do
  add :slug, :string
end
</code></pre>
<p>the alter macro changes the schema for both up and down migrations. The premise
of a slug is so you can permanently generate a name from the data in other
fields, some of which may be updatable.</p>
<p>Because Ecto separates changesets from the definition of a database record
(model), this gives developer the ability to have different change policies for
any type of change to the record. Changesets act as a funnel to the database,
filtering and casting incoming data, making sure sensitive fields like user role
cannot be set externally, while casting them to the type defined in the schema.
Changesets can validate data, for example the length of a field— on the fly, but
validations that depend on database integrity are left to the database, in the
form of constraints. Changesets make the code easier to reason about, can be
composed, allowing each part of a data transformation to be a more easily
comprehend–able and testable function.</p>
<p>In viws you may see a <code>video_path</code>, or <code>watch_path</code>, or <code>x_path()</code> helpers.
These helper functions are generated by RumblWeb.Router and imported into
controllers and views by rumble_web/web.ex. <code>watch_path(@conn, :show, video)</code>
takes the Video struct, and by conventions it generates a URL by extracting the
ID from the Video struct. This works because Phoenix knows to use the ID field
to generated the URL because Phoenix implements the Phoenix.Param protocol.
Developer can customize this behavior by implementing our own functions to
override the default protocol implementation to generate the URL using the slug.</p>
<pre><code>defimpl Phoenix.Param, for: Rumbl.video do
  def to_param(%{slug: slug, id: id}) do
    "#{id}-#{slug}"
  end
end
</code></pre>
<p>The advantage of implementing this behavior change as a Protocol implementation
is the clean polymorphism we get by extending Phoenix parameters without having
to change Phoenix itself or the Video module itself.</p>
<h2>Ch. 10: Using Channels</h2>
<p>A client connects to a channel over a socket, then sends and receives messages,
that's it. A Phoenix channel is a conversation, the channel sends messages,
receives messages, and keeps state. Just as controllers receive requests,
channels receive events.</p>
<p>A Phoenix conversation (messages sent, messages received) is about a topic, and
the topic maps onto application concepts like a chat room, local map, or
annotations to a video. Each user's conversation on a topic has its own
isolated, dedicated process.</p>
<p>Whereas the traditional web-based request/response interactions are stateless,
conversations in an Elixir process can be stateful. This means for sophisticated
interactions like games and more interactive pages, developer doesn't need to
use local storage, cookies, or databases to keep track of the state of the
conversation. Each call to the channel simply picks up where it left off. This
only works if the framework supports true isolation and concurrency: one process
must be isolated, not affect others, and true concurrency means lightweight
abstractions that don't bleed into one another. Channels must scale in the
dimensions developer is most concerned about: code complexity, performance, and
manageability.</p>
<p>The application channel is simple, but it must manage three things:</p>
<pre><code>making and breaking connections
sending messages
receiving messages
</code></pre>
<p>A socket establishes a new connection with a socket. After a connection is made,
that same socket will be transformed through the life of the connection. The
socket is a representation of a continuous ongoing conversation between the
client and the server. When a connection is made, the initial socket is created,
and that same socket is transformed with each new received event, through the
whole life of the conversation.</p>
<p>The socket is an abstraction over a websocket, or to handle older clients, a
longpoll. The socket abstracts the transport layer, and Phoenix handles the
rest.</p>
<p>A channel is a conversation about a topic, the topic has an identifier of
videos:<video id> where video_id is a dynamic id matching a record in the
database. Topic identifiers generally take the form topic:subtopic where topic
is often a resource name and subtopic is the instance ID that usually identifies
a row in the database.</p>
<p>Topics are organizing concepts, they are used as parameters to functions, and
used in URLs to identify conversations. Just like a URL passes an :id parameter
to represent a request for a resource from a controller; with channels,
developer can use the topic ID to scope a channel connection.</p>
<p>Phoenix channels define "callbacks", or functions that respond to channel events
such as join below:</p>
<pre><code>def join("videos:" &#x3C;> video_id, _params, socket) do
  {:ok, assign(socket, :video_id, String.to_integer(video_id))}
end
</code></pre>
<p>in the above code, assign(socket...) returns a socket, resulting in a {:ok,
socket} returning for authorized users or {:error, socket} which will deny
joining the channel. Sockets maintain state in a socket.assigns map, and the
above adds the video id to the assigns map using the helper function
<code>assign(socket, :video_id, String.to_integer(video_id))</code>.</p>
<p>Socket state is maintained for the duration of the connection. The socket is
transformed in a loop rather than a single pipeline. The socket state changed in
the above "assign()" call will be accessible later as events come into and out
of the channel. This small difference leads to enormous differences in
efficiency between the channels API and the controllers API.</p>
<p>Channels receive events with an event name, such as new_message, and include a
payload of arbitrary data. Each channel module can receive events in three ways</p>
<pre><code>handle_in receives direct channel events
handle_out intercepts broadcast events
handle_info receives OTP messages
</code></pre>
<p><code>handle_info</code> is invoked whenever an Elixir message reaches the channel.</p>
<p>Session-based authentication makes sense for request/response–type applications,
but for channels, token authentication works better because the connection is a
long-duration connection. With token authentication, each user gets a unique
token. Tokens allow for a secure authentication mechanism that doesn't reply on
any specific transport. Once the user is already authenticated using the
request/response traditional approach, the application can expose the token to
the frontend, and this token can be used by the channel.</p>
<h2>Ch. 11: OTP</h2>
<p>OTP is a way to think about concurrency and distribution, using patterns that
allow developer to use concurrency to build state without language features that
rely on mutability.</p>
<p>OTP is the name of the library encapsulating concurrent state and behavior, and
the abstractions is called generic server, or GenServer.</p>
<p>Supervisors need to be able to restart processes in the right way, according to
policies that are best for the application. For example, suppose a database was
upgraded, and the connection stopped. The application would need to
automatically kill and restart the connection pool. This policy should not
impact the code that uses the database. If developer can replace a simple
supervisor process with a supervisor tree, we can build much more robust
fault-tolerance and recovery software.</p>
<p>In Phoenix, there is little code attempting to deal with every possible error
exception. Instead, we trust the error reporting to log the errors so developer
can fix what's broken, and in the meantime, supervisor tree can automatically
restart services in a last good state. OTP captures these clean abstractions in
a coherent library, allowing developer to declare the supervision properties
that most interest us. This allows developer to build robust self-healing
software without building complex self-healing software.</p>
<p>A supervision strategy is what policy is used when a process dies, for example,
the :one_for_one policy will restart a child process when the child dies. If all
resources depend on a common service, developer could specify :one_for_all to
kill and restart all child processes if any child dies. Developer doesn't need
to add any code to fully supervise every process, only configure a policy to
tell OTP how to handle each crash.</p>
<p>The default restart strategy is :permanent, the supervisor always restarts child
processes with this strategy. With <code>:temporary</code>, the child process is never
restarted. Under :transient, the child is restarted of if it terminates
abnormally, with an exit reason other than :normal, :shutdown, or {:shutdown,
term}.</p>
<p><code>:temporary</code> is useful when a restart is unlikely to resolve a worker crash, or
when restarting doesn't make sense based on the flow of the application.</p>
<p>Child processes can have different restart strategies, supervisors have
configurable supervision strategies.</p>
<pre><code>:one_for_one if a child terminates, the supervisor restarts only that process
:one_for_all if a child terminates, a supervisor terminates all remaining children, the restarts all children
:rest_for_one if a child terminates, a supervisor terminates all children defined after the one that died, then restarts all terminated processes
:simple_on_for_one Similar to :one_for_one but used when a supervisor needs to dynamically supervise processes. For example, a web server would use it to supervise web requests, which may be 10, 10,000, or 100,000 processes.
</code></pre>
<p>A channel in Phoenix is an OTP process built to serve a single user in the
context of a single conversation on a topic.</p>
<p>When fetching real-time (pp 212) data about a video that will be synched in
real-time, a network or failure by the 3rd party can occur, but since we're
making multiple requests, we can ignore the failure and use the responses from
the successful requests. The restart strategy spawns multiple processes and
shouldn't restart the failures because its time sensitive, the errors can simply
be moved past, we'll accept any successful requests that return, so we'll use
the :temporary restart strategy.</p>
<p>OTP applications protect in both directions, that is, if the Phoenix server
itself crashes, we bring down the children processes and restart them so no
resources are leaked. When a child process crashes, we bring down that process
and restart it so we can attempt clean recovery. When building a supervisor and
children, the process of defining the restart strategy and supervision strategy
lets developer focus on the main application logic, as any unknowable error
handling is already taken care of. Let the errors crash gracefully, and OTP will
handle the rest.</p>
<p>Because GenServer's are meant to be generic servers, they hold both computation
and state. However, in many cases, we want a process to store state only, or
only execute a particular function. An agent is a simple GenServer</p>
<h2>Chapter 12: Observer and Umbrellas</h2>
<p>Umbrella projects allow developer to develop and test multiple child
applications in isolation side by side while providing conveniences for
configuring and building them only once. Instead of breaking the application
into multiple applications in distinct source-code repositories, which adds too
much overhead to the development workflow, you can use Umbrella projects. The
combination of Mix and OTP make this abstraction a good one for separating core
concerns and dealing with complexity.</p>
<p>Observer is an Erlang tool for understanding all running processes in an
application. To start:</p>
<pre><code>iex -S mix
> :observer.start
</code></pre>
<p>The tool visualizes different aspects of the application, lets developer see a
list of all running processes, how much memory, messages the system is using.
Developer can more easily see where bottlenecks occur by finding processes with
large numbers of messages.</p>
<p>Each Umbrella has a parent directory that defines</p>
<pre><code>The shared configuration of the project
The dependencies for that project
the apps directory with child applications
</code></pre>
<p>to create new Umbrella projects, use mix new <name> --umbrella outside of your
application, which stubs out a simpler Phoenix app, this project can then be
moved inside the parent project.</p>
<h2>Chapter 13: Testing Channels and OTP</h2>
<p>Stubs and mocks are both testing fixtures that replace real world
implementations. A stub replaces real-world libraries with simpler, predictable
behavior. With a stub, developer can bypass code that would be difficult to
test. Stubs should have nothing to say whether a test should pass or fail. Stubs
are a simple scaffold implementation standing in for a more complex real-world
implementation.</p>
<p>A mock is similar, but replaces real-world behavior just as a stub does, but it
allows developer to specify expectations and results, playing back those
expectations at runtime. A mock will fail a test if the test code doesn't
receive the expected function calls. A mock is an implementation that records
expected behavior at definition time and plays it back at runtime, expecting
those results.</p>
<p>Within the Elixir community, avoid mocking whenever possible. Most mocking
libraries end up changing global behavior—for example, by replacing a function
in the HTTP client library to return a particular result. These function
replacements are global, so a change in one place would change all code running
at the same time. That means tests written in this way can no longer run
concurrently. These strategies will snowball, requiring more and more mocking
until the dependencies among components are completely hidden.</p>
<p>A better strategy is to identify code that's difficult to test live, and to
build a configurable, replaceable testing implementation rather than a dynamic
mock. The development and production code will use the simple :httpc client, and
the testing code will use a stub that is called during the tests.</p>
<p>A major advantage of writing asynchronous tests in OTP is the tests run
concurrently, meaning they can be run in parallel, so the entire suite of tests
can finish more quickly than their synchronous counterparts.</p>
<h2>References</h2>
<p>McCord, Chris. Tate, Bruce. Valim, José. (2016). <em>Programming Phoenix</em>. The
Pragmatic Programmers. Print.</p>
</div><footer class="license_copyright__KOT7N">This is licensed under a Creative Commons <!-- -->cc-by-nc-sa<!-- --> International License</footer></article></main><div class="layout_layout__Xf50c"><footer class="footer_footer__OT4Wn layout_layout__Xf50c"><p>© 2003-<!-- -->2023<!-- --> 0xADADA (unless otherwise noted.)<br/><a title="0xADADA" href="/">Home</a> <a href="/rss.xml">RSS</a> <span class="h-card"><a class="c-Meta u-email" rel="me" title="0xADADA" href="mailto:0xadada.pub@protonmail.com">Email</a> </span><a rel="me nofollow external noopener" title="0xADADA on Mastodon" href="https://freeradical.zone/@0xadada">Mastodon</a> <a rel="me nofollow external noopener" title="0xADADA on Twitter" href="https://twitter.com/0xadada">Twitter</a> <a rel="me nofollow external noopener" title="0xADADA on GitHub" href="https://github.com/0xadada">GitHub</a> <a rel="me nofollow external noopener" title="0xADADA on Goodreads" href="https://www.goodreads.com/review/list/60524683-0xadada?shelf=wanted">Goodreads</a> </p></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"slugs":{"year":"2017","month":"12","day":"22","slug":"programming-phoenix"},"title":"Notes on Programming Phoenix for Elixir","displayTitle":"Notes on Programming Phoenix for Elixir","metaDescription":"Notes and observations on the Phoenix web framework for Elixir","metaKeywords":"elixir, phoenix, web development, programming, software engineering","date":1513958040000,"author":"0xADADA","content":"\u003cp\u003eA summary of my notes taken while reading\n\u003ca href=\"https://pragprog.com/book/phoenix/programming-phoenix\"\u003eProgramming Phoenix\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eCh.1\u003c/h2\u003e\n\u003cp\u003eThe layers of phoenix, the endpoint is where the HTTP connection contacts\nphoenix, from there it goes to the router which directs a request to the\nappropriate controller, passing through a series of pipelines. Pipelines chain\nfunctions together to handle tasks that span across multiple controllers, such\nas browser requests vs API requests.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econnection\n|\u003e endpoint\n|\u003e router\n|\u003e pipelines\n|\u003e controller\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe purpose of the web server is to route a request to a function that performs\nthe right task, called an action. These functions are grouped into controllers.\nA controller is also a series of pipelines\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econnection\n|\u003e controller\n|\u003e common_services\n|\u003e action\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAn action to show a user would look like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econnection\n|\u003e find_user\n|\u003e view\n|\u003e template\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRun phoenix inside an interactive shell: iex -S mix phx.server\u003c/p\u003e\n\u003cp\u003eIn controllers, parameters come in as a Map of keys with string names:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edef world(conn, %{\"name\" =\u003e name}) do    # string key\n  render conn, \"world.html\", name: name  # atom key\nend\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ePhoenix specifically converts these from String keys to atoms once inside the\napplication boundry, because external data can't safely be converted to atoms.\u003c/p\u003e\n\u003cp\u003ePhoenix encourages breaking big functions down into smaller ones. It provides a\nplace to explicitly register each smaller function in a way that makes it easy\nto understand and replace. These small pieces are orchestrated with the Plug\nlibrary. The Plug library is a specification for building applications that\nconnect with the web. Plug produces and consumes a struct called Plug.Conn,\ndescribing the entire request/response cycle of a web application, containing\nthe request, protocol, parameters, etc. Each individual Plug should do one-thing\nand do it well, take the conn struct, transform it, and return it.\u003c/p\u003e\n\u003cp\u003ePlugs are just functions, and Phoenix web applications are just pipelines of\nPlugs.\u003c/p\u003e\n\u003cp\u003eIf you look at endpoint.ex you'll see that its essentially a pipeline of\nfunctions chained together at the beginning of each request:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econnection\n|\u003e Plug.Static.call\n|\u003e Plug.RequestId.call\n|\u003e Plug.Logger.call\n|\u003e Plug.Parsers.call\n|\u003e Plug.MethodOverride.call\n|\u003e Plug.Head.call\n|\u003e Plug.Session.call\n|\u003e \u0026#x3C;your app\u003e.Router.call\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe last call, to \u003cyour app\u003e.Router.call is where endpoint.ex defers to your\napplications router.ex to define which controller, and subsequently, which\nfunction is invoked next in the pipeline chain. The router defines a set of\npipelines for dealing with which Plugs to use prior to your conroller being\ncalled, typically:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econnection\n|\u003e endpoint\n|\u003e router\n|\u003e pipeline(s)\n|\u003e controller\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe pipeline(s) above will be things like :browser, handling common tasks as\npreventing request forgery, \u003ccode\u003e:fetch_session\u003c/code\u003e, \u003ccode\u003e:fetch_flash\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThe controller is where business logic specific to your application lives,\nmaking data available for a view. The view subsequently substitutes values\ninside a template which is rendered.\u003c/p\u003e\n\u003ch2\u003eCh.2\u003c/h2\u003e\n\u003cp\u003eStart an interactive Elixir shell within an Elixir application project:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eiex -S mix\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eStructs add a constraint to Maps in that they offer protection from bad or\nmisspelled keynames at compilation time, while Maps only throw errors during\nruntime when the key is first accessed. Under the covers, a Struct is just a Map\nwith a \u003cstrong\u003estruct\u003c/strong\u003e key.\u003c/p\u003e\n\u003ch2\u003eCh3\u003c/h2\u003e\n\u003cp\u003eA view is a module containing a rendering function that converts data into a\nformat the user will consume, like HTML or JSON. A template is a function on the\nview module, compiled from a file containing raw markup language and embedded\nElixir code to process substitutions and loops. Views are modules responsible\nfor rendering, and templates are the compiled functions that have static markup\nand native code to build pages.\u003c/p\u003e\n\u003cp\u003eEEx executes Elixir code within \u003ccode\u003e\u0026#x3C;%= %\u003e\u003c/code\u003e tags, injecting the result into the\ntemplate. EEx evalutes code within \u003ccode\u003e\u0026#x3C;% %\u003e\u003c/code\u003e tags, but without injecting the\nresult.\u003c/p\u003e\n\u003cp\u003eThe expression \u003ccode\u003e\u0026#x3C;%= for user \u0026#x3C;- @users do %\u003e\u003c/code\u003e walks through the users, rendering\neach user using the template block between the \u003ccode\u003edo ... end\u003c/code\u003e block, rolling the\nresult up into the template.\u003c/p\u003e\n\u003cp\u003eIn Phoenix, after compilation, templates are just functions. These templates are\njust linked lists (rather than other web frameworks that use string\nconcatenation)– this means Phoenix doesn't have to make huge copies of large\nstrings, resulting in performance improvements. This lets Elixir leverage modern\nCPU optimization since it only has a single copy of the largest strings in your\napplication.\u003c/p\u003e\n\u003cp\u003eWhen a template is added to Phoenix, the view module extracts the template from\nthe filesystem and adds it as a function on the module itself, thats why the\nview module is required in the first place.\u003c/p\u003e\n\u003ch2\u003eCh.4\u003c/h2\u003e\n\u003cp\u003eEcto uses a DSL that specifies how the fields in a struct map to database tables\nand columns. The DSL is built using Elixir macros. Virtual fields are not\npersisted to the database.\u003c/p\u003e\n\u003cp\u003eModels (called contexts \u0026#x26; schemas in Phoenix 1.3) are layers of functions to\ntransform data according to business logic. A schema is the native database form\nof the data, and a struct is the data once pulled out of the database.\nChangesets in Ecto decouple the authentication update policy separately from the\ndatabase schema. This policy segregation makes handling change over time.\u003c/p\u003e\n\u003cp\u003eEcto helps you write code to make the minimal required database operation to\nupdate a record. Ecto can enforce constraints without hitting the database.\u003c/p\u003e\n\u003ch2\u003eCh.5\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003eEcto.Changeset.cast\u003c/code\u003e converts a bare Map to a changeset, for security it limits\nthe incoming parameters to those specified.\u003c/p\u003e\n\u003cp\u003eModule plugs vs function plugs: A function plug is a single function, a module\nplug is a module that provides two functions with some configuration details,\notherwise they work the same. \u003ccode\u003elib/rumbl_web/endpoint.ex\u003c/code\u003e is an example of a\nmodule plug while \u003ccode\u003elib/rumbl_web/router.ex\u003c/code\u003e is an example of a function plug.\nYou specify a module plug by providing a module name plug Plug.Logger while a\nfunction plug is specified by its name as an atom plug \u003ccode\u003e:protect_from_forgery\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eTo share a Plug across more than one module, you want to use a Module Plug. All\nplugs take a connection and return a connection.\u003c/p\u003e\n\u003cp\u003eAny values stored in conn.assigns.* is automatically available with the same\nname in our view. So if you assign to current_user, then @current_user can be\nused directly in a view.\u003c/p\u003e\n\u003ch2\u003eCh.6 Generators \u0026#x26; Relationships\u003c/h2\u003e\n\u003cp\u003eGenerating resources, phx.gen.html generates simple HTML based scaffolds,\nphx.gen.json generates a REST-based API using JSON. They create resources for\nCRUD operations, creating migrations, controllers, and templates as well as\ntests.\u003c/p\u003e\n\u003cp\u003eAn example:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emix phx.gen.html Medias \\\n  Video videos \\\n  user_id:references:users \\\n  url:string \\\n  title:string \\\n  description:text\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis includes the name of the context that holds the module, the module defines\nthe model. The plural form of the model name, and each field with some type\ninformation. Phoenix consistently uses singular forms in models, controllers,\nand views. At the application boundry (URLs, table names you provide a bit more\ninformation because you can use pluralized names). Instead of using inflectors,\nyou just explicitly set the pluralized form yourself.\u003c/p\u003e\n\u003cp\u003ePrimary keys identify rows for each item in a table. Foreign keys point from one\ntable to the primary key in another table. Foreign key lets the database get in\non the act of maintaining consistency across our two relationships.\u003c/p\u003e\n\u003cp\u003eThe change function handles two database functions, one for migrating up and one\nfor migrating down. A migration up applies a migration, and a migration down\nreverts it. If you make a mistake a need to move a single migration up or down,\nyou can do so.\u003c/p\u003e\n\u003cp\u003eIf you meant to add a \u003ccode\u003eview_count\u003c/code\u003e field to your create_video migration, but\nbefore you migrated your database up. You would create a new migration adding\nthe view_count field, since you haven't pushed your changes to production yet,\nyou can roll back, make your changes, and then migrate up again. First you'd\nroll back your changes: mix ecto.rollback Verify the undo of the create_video\nmigration. At this point, add the missing view_count field and migrate forwards\nwith mix ecto.migrate.\u003c/p\u003e\n\u003cp\u003eA schema is responsible for tying the database fields to a field in the Elixir\nstruct, these are defined in \u003ccode\u003elib/\u0026#x3C;appname\u003e/\u0026#x3C;contextname\u003e/\u0026#x3C;model\u003e.ex\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eEcto associations are explicit, you need to ask to fetch associated records\nspecifically. Most persistence frameworks often fetch rows you don't need or\nfetch them in inefficient ways, over time these inefficiencies add up to major\nperformance problems. Ecto forces the developer to pay down these performance\nhits early so they don't add up over time, when they're more difficult to fix.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eiex\u003e user = Repo.get_by!(User, username: \"josevalim\")\n%{Rumbl.User{...}\niex\u003e user.videos\n#Ecto.Association.NotLoaded\u0026#x3C;association :videos is not loaded\u003e\niex\u003e user = Repo.preload(user, :videos)\n%{Rumbl.User{...}\niex\u003e user.videos\n[] # loaded, but none are associated! yay\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRepo.preload accepts one or a collection of association names, and fetches all\nassociated data.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eiex\u003e user = Repo.get_by!(User, username: \"josevalim\")\niex\u003e attrs = %{title: \"hi\", description: \"says hi\", url: \"example.com\"}\niex\u003e video = Ecto.build_assoc(user, :videos, attrs)\niex\u003e video = Repo.insert!(video)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eEcto.build_assoc builds a struct, with the proper relationship fields already\nset.\u003c/p\u003e\n\u003cp\u003eTo fetch videos associated with a user, without storing them in a user struct:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eiex\u003e query = Ecto.assoc(user, :videos)\niex\u003e Repo.all(query)\n[%Rumbl.Video{...}]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eassoc is convenient in that it returns an Ecto.Query all videos scoped to a\nspecific user, or a list of users, we convert this query into data by calling\nRepo.all.\u003c/p\u003e\n\u003ch2\u003eCh.7 Ecto Queries and Constraints\u003c/h2\u003e\n\u003cp\u003eSeeds are small scripts that populate the database with values every time the\nscript is run. Phoenix stores seed scripts in priv/repo/seeds.ex. Mix tasks will\nrun these scripts. Use mix run priv/repo/seeds.exs to add the seed data to the\ndatabase.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eRepo.all from c in Category, select: c.name\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ccode\u003eRepo.all\u003c/code\u003e means return all rows, \u003ccode\u003efrom\u003c/code\u003e is a macro that builds a query,\n\u003ccode\u003ec in Category\u003c/code\u003e means we're pulling rows (labeled c) from the Category schema.\n\u003ccode\u003eselect: c.name\u003c/code\u003e means we're going to return only the name field.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eRepo.all from c in Category, order_by: c.name, select: c.name\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ewill order the results by name and return a tuple containing the name and the id\nfields.\u003c/p\u003e\n\u003cp\u003eEcto queries are composable, you don't need to define the entire query at once,\nyou can combine them bit-by-bit.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eiex\u003e query = Category\niex\u003e query = from c in query, order_by: c.name\niex\u003e query = from c in query, select: {c.name, c.id}\n#Ecto.Query\u0026#x3C;\u003e\niex\u003e Repo.all(query)\n[...]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis works because Ecto defines a queryable protocol. from receives a queryable,\nand you can use any queryable as a base for a new query. An Elixir protocol\ndefines an API for specific language features, this one defines the API for\nsomething that can be queried. This is why we can invoke Repo.all(Category) or\nRepo.all(query) because both Category and query implement the \u003ccode\u003eEcto.Queryable\u003c/code\u003e\nprotocol. By abiding to the protocol, developer can quickly layer together\nsophisticaed queries with Ecto.Query, maintaining sophistication without\ncomplexity.\u003c/p\u003e\n\u003cp\u003eCode that builds and transforms queries, and code that interacts with the\nrepository should belong to the context. Code that makes requests for the data\nshould belong to the controller—because the controller is where the web logic\nshould live, and the database layer should be hidden within the application\ncontext with the rest of the business logic.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eiex\u003e username = \"josevalim\"\niex\u003e Repo.one(from u in User, where: u.username == ^username)\n%Rumbl.User{...}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRepo.one returns one row, from u in user means read from the User schema. where\nu.username == ^username means return the row \u003ccode\u003ewhere u.username == ^username\u003c/code\u003e,\nusing the pin \u003ccode\u003e^\u003c/code\u003e operator means we don't want to assign the username but use\nits value.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eRepo.one\u003c/code\u003e does not mean \"return the first result\" but \"one result is expected\",\nso if there is more, it fails. The Ecto Query API is not about composing query\nstrings, it uses Elixir macros such that Ecto knows where user-defined variables\nare located, it's easier to protect the user from security flaws like\nSQL-injection attacks. It also helps a bit with query normalization and\nleverages the data types as defined in the schema for casting values at runtime.\u003c/p\u003e\n\u003cp\u003eAny functions with side effects—the ones that change the world—should remain in\nthe controllers, while the context, model, and view layers remain side effect\nfree. The controller receives data, either from a traditional web request,\nreading data from a socket, and this data is passed from the controller to\nvarious functions that transform it as it moves through the functions to the\nshape of our business-model requirements. Finally it makes changes to the world\naround us, either delivering emails, adding entries to a database, or invoking a\nview which is again written to the connection (another side effect), any of\nwhich can result in a business operation.\u003c/p\u003e\n\u003cp\u003eThe query API supports many operators:\n\u003ccode\u003e==, !=, \u0026#x3C;=, \u003e=, \u0026#x3C;, \u003e, and, or, not, in, like, ilike, is_nil, count, avg, sum, min, max, datetime_add, date_add, fragment, field, type\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eKeyword syntax uses a keyword list to express a query.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eiex\u003e Repo.one from u in User,\n              select: count(u.id),\n              where: ilike(u.username, ^\"j%\") or\n                     ilike(u.username, ^\"c%\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ethe u variable is bound as part of Ecto's from macro, representing entries from\nthe User schema. Each join in a query gets a special binding.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# count users\niex\u003e users_count = from u in User, select: count(u.id)\n#Ecto.Query\u0026#x3C;from u in Rumbl.User, select: count(u.id)\u003e\n# count usernames with a j\niex\u003e j_users = from u in users_count, where: ilike(u.username, ^\"%j%\")\n#Ecto.Query\u0026#x3C;from u in Rumbl.User, where: ilike(u.username, ^\"%j%\"), select: count(u.id)\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis query builds up a new query, normalizing as it builds, upon the saved\nquery, we even built the query using the same bound variable name, u, but we\ndidn't have to.\u003c/p\u003e\n\u003cp\u003eThe pipe syntax allows developer to build queries by piping through query\nmacros. Each pipe takes a queryable and returns a queryable.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eiex\u003e User |\u003e\n     select([u], count(u.id)) |\u003e\n     where([u], ilike(u.username, ^\"j%\") or ilike(u.username, ^\"c%\")) |\u003e\n     Repo.one()\n[debug] QUERY OK source=\"users\" db=4.5ms\nSELECT count(u0.\"id\") FROM \"users\" AS u0 WHERE ((u0.\"username\" ILIKE $1) OR (u0.\"username\" ILIKE $2)) [\"j%\", \"c%\"]\n5\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBecause each segment of the pipe works independently of the others, we need to\nspecify the binding manually for each one.\u003c/p\u003e\n\u003cp\u003eFragments offer an escape hatch from Ecto's Query API. The best abstractions\noffer an escape hatch, and since Ecto's Query API doesn't represent every query\nthe database layer can provide, Ecto's query fragments send part of the query\ndirectly to the database but allows you to construct it in a safe way, like\nthis: iex\u003e from(u in User, where: fragment(\"lower(username) = ?\",\n^String.downcase(uname)))\u003c/p\u003e\n\u003cp\u003eWhen all else fails, you can directly run SQL statements with\nEcto.Adapters.SQL.Query:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eiex\u003e Ecto.Adapters.SQL.query(Rumbl.Repo, \"SELECT power($1, $2)\", [2, 10])\nSELECT power($1, $2) [2, 10]\n{:ok, %Postgrex.Result{ columns: [\"power\"], command: :select, connection_id: 5979, num_rows: 1, rows: [[1024.0]] }}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eEcto relationships are explicit:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eiex\u003e user = Repo.one from(u in User, limit: 1)\niex\u003e user.videos\n#Ecto.Association.NotLoaded\u0026#x3C;association :videos is not loaded\u003e\niex\u003e user = Repo.preload(user, :videos)\niex\u003e []\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eEcto allows us to preload associations directly as part of a query:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eiex\u003e Repo.all from u in User, join: v in assoc(u, :videos), join: c in\nassoc(v, :category), where: c.name == \"Comedy\", select: {u, v}\n[{Rumbl.User{...}, %Rumbl.User{...}}]\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eConstraints allow developers to use underlying relational, and can solve\npotential race conditions:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eUser sends a category ID through the form.\u003c/li\u003e\n\u003cli\u003ePerform a query to check if the category ID exists in the DB.\u003c/li\u003e\n\u003cli\u003eIf the category ID exists, add the video with the category ID to the\ndatabase.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eHowever, someone could delete the category ID between steps 2 and 3, allowing a\nvideo insertion without and existing category. This will lead to inconsistent\ndata over time. Phoenix uses constrains to manage change in a way that combines\nthe harsh protections of the database with Ecto's gentle guiding hand to report\nerrors without crashing. Some terms:\u003c/p\u003e\n\u003cp\u003econstraint: An explicit database constraint, a uniqueness constrain, or an\nindex, or and integrity constraint between primary and foreign keys.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econstraint error: The Ecto.ConstraintError.\nchangeset constraint: A constraint annotation allowing Ecto to convert constraint errors into changeset error messages.\nchangeset error messages: Beautiful error messages for the consumption of people.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eEcto allows the application layer (and web server) to use database services like\nreferential integrity and transactions to strike a balance between the needs of\nthe application layer and the needs of the database. Ecto rewards developers the\nmany guarantees databases offer with data integrity in the short term, by\ntransforming constraint errors into user feedback, and in the longer term by\nguaranteeing you wont be awake at 3am fixing bugs caused by inconsistent data.\u003c/p\u003e\n\u003cp\u003eIEx allows us to fetch a previous value using v(n), where n is the number of the\nexpression, you can alsop ass negative values to grab the last nth expression.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eiex\u003e alias Rumbl.Repo\nRumbl.Repo\niex\u003e category = Repo.get_by(Rumbl.Medias.Category, name: \"Drama\")\n%Rumbl.Medias.Category{\n  __meta__: #Ecto.Schema.Metadata\u0026#x3C;:loaded, \"categories\"\u003e,\n  id: 2,\n  inserted_at: ~N[2017-11-05 18:02:12.006642],\n  name: \"Drama\",\n  updated_at: ~N[2017-11-05 18:02:12.006653]\n}\niex\u003e Repo.delete(category)\n[debug] QUERY ERROR db=6.5ms\nDELETE FROM \"categories\" WHERE \"id\" = $1 [2]\n** (Ecto.ConstraintError) constraint error when attempting to delete struct: * foreign_key: videos_category_id_fkey\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe previously added a video using the Drama category, so the database prevents\ndeletion of the category as it is tied to the video and would create orphaned\nrecords. Use foreign_key_constraint, which is like assoc_constraint used\nearlier, except it doesn't inflect the foreign key from the relationship.\u003c/p\u003e\n\u003cp\u003eThis is particularly useful if you want to provide reasons why a category can\nnot be deleted.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eiex\u003e import Ecto.Changeset\nEcto.Changeset\niex(22)\u003e changeset = Ecto.Changeset.change(category)\n#Ecto.Changeset\u0026#x3C;action: nil, changes: %{}, errors: [], data: #Rumbl.Medias.Category\u0026#x3C;\u003e, valid?: true\u003e\niex\u003e changeset = foreign_key_constraint(changeset, :videos, name: :videos_category_id_fkey, message: \"still exists\")\n#Ecto.Changeset\u0026#x3C;action: nil, changes: %{}, errors: [],\n            data: #Rumbl.Medias.Category\u0026#x3C;\u003e, valid?: true\u003e\niex\u003e Repo.delete(changeset)\n[debug] QUERY ERROR db=8.8ms\nDELETE FROM \"categories\" WHERE \"id\" = $1 [2]\n{\n  :error,\n  #Ecto.Changeset\u0026#x3C;\n    action: :delete,\n    changes: %{},\n    errors: [videos: {\"still exists\", []}],\n    data: #Rumbl.Medias.Category\u0026#x3C;\u003e,\n    valid?: false\n  \u003e\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ebecause the foreign key is established in the videos table, we need to\nexplicitly call this out in the call to \u003ccode\u003eforeign_key_constraint\u003c/code\u003e. The work best\nsuited for the database must be done in the database.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003econstraint\u003c/code\u003e changeset functions are useful when the constraint being mapped\nis triggered by external data, often as part of the user request. Using\nchangeset constraints only makes sense if the error message can be something the\nuser can take useful action on.\u003c/p\u003e\n\u003ch2\u003eCh.8 Testing MVC\u003c/h2\u003e\n\u003cp\u003eTesting principals\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFast — tests should run quickly and concurrently if possible\u003c/li\u003e\n\u003cli\u003eIsolated — Tests that are too isolated wont have enough context to be useful,\ntests that aren't isolated enough are difficult to understand and maintain\u003c/li\u003e\n\u003cli\u003eDRY — Strive to eliminate repetition in tests\u003c/li\u003e\n\u003cli\u003eRepeatable — The same test on the same code should always yield the same\nresult\u003c/li\u003e\n\u003cli\u003eunit test — exercises a function for one layer of your application\u003c/li\u003e\n\u003cli\u003eintegration test — focuses on the way different layers of an application fit\ntogether. An example is a controller test that makes a request to and\nendpoint, flows through the pipelines, reads from the database, and renders\nthrough views just like a Phoenix request would\u003c/li\u003e\n\u003cli\u003eacceptance test — test how multiple actions work together. A single acceptance\ntest case may sign the user on, perform several calculations that build upon\neach other, then sign off.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eExUnit has three main macros, setup macro specifies setup code that runs before\neach test is run. test macro defines a single isolated test, it defines a\nhypothesis about code. Finally, assert macro specifies something we believe to\nbe true about code, if true, the test passes.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edefmodule MyTest do\n  use ExUnit.Case, async: true\n  setup do\n    :ok\n  end\n  test \"pass\" do\n    assert true\n  end\n  test \"fail\" do\n    assert false\n  end\nend\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis code runs two tests, first setup is run, then the \"pass\" test case. Next\nsetup is run, and the \"fail\" test case. The output is a passing test case and a\nfailing test case.\u003c/p\u003e\n\u003cp\u003eTests will generally use \u003ccode\u003e\u0026#x3C;AppName\u003e.ConnCase\u003c/code\u003e meaning you get\n\u003ccode\u003e\u0026#x3C;AppName\u003e.Router.Helpers\u003c/code\u003e, and Ecto imports along for free.\u003c/p\u003e\n\u003cp\u003eExUnit calls tests with\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003etest \"GET /\", %{conn: conn} do\n  conn = get(conn, \"/\")\n  assert html_response(conn, 200) =~ \"Welcome to Phoenix!\"\nend\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eby calling get(conn, \"/\") rather than calling the index action on the controller\ndirectly. This practice gives the test the right level of isolation because\nwe're using the controller the same way Phoenix does. Phoenix also provides test\nhelpers to make testing responses easier.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eassert html_response(conn, 200) ~= \"Welcome to Phoenix!\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ewhich does the following:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003easserts the response was 200\ncontent-type was text/html\nreturn the response body, allowing us to match on the contents\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ethis also exists for JSON called json_response that allow for assertions like\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eassert %{user_id: user.id} = json_response(conn, 200)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eExUnit tags help when setup is different from test to test. When specifying a\ntag, ExUnit makes that information available within the setup block via\ncallbacks.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esetup %{conn: conn} = config do\n  if username = config[:login_as] do\n    user = insert_user(username: username)\n    conn = assign(conn, :current_user, user)\n    {:ok, conn: conn, user: user}\n  else\n    :ok\n  end\nend\n@tag login_as: \"max\" # this is the tag\ntest \"list all user's videos on index\", %{conn: conn, user: user} do\n# ...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003etags take either a keyword list (above) or an atom as arguments. The config map\ncontains the conn and tag (either atom or keyword list). In this case if\nlogin_as has a value, we use it to login the user and return an updated conn\nwith the user, otherwise we return an :ok.\u003c/p\u003e\n\u003cp\u003eWriting unit tests directly against a function like the Auth plug will result in\nunexpected errors during the testing/coding cycle because calls directly to the\nfunction will get results that have not gone through the endpoint, router or\nother pipelines. The data has not been fully transformed, so the results will\nnot reflect the state that the implementation sees when running in a server. The\nbypass_through helper that ConnCase provides allows developer to send a\nconnection through the endpoint, router, and desired pipelines but bypass the\nroute dispatch. This approach gives developer a connection wired up with all the\ntransformations the specific tests require, such as fetching the session and\nadding flash messages.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esetup %{conn: conn} do\n  conn =\n    conn\n    |\u003e bypass_through(RumblWeb.Router, :browser)\n    |\u003e get(\"/\")\n\n  {:ok, %{conn, conn}}\nend\ntest \"authenticate_user halts when no current_user exists\", %{conn: conn} do\n  conn = Auth.authenticate_user([])\n  assert conn.halted\nend\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe setup block calls \u003ccode\u003ebypass_through\u003c/code\u003e, passing the router and the :browser\npipeline to invoke. When the get request is invoked, it accesses the endpoint\nand stops at the browser pipeline, as requested. The path \"/\" given to the get\nisn't used by the router when bypassing, its just stored in the connection, this\nprovides all the requirements for a plug with a valid session and flash message\nsupport.\u003c/p\u003e\n\u003cp\u003eSince most repository-related functionality will be tested with integration\ntests as they insert and update records, but we want to be sure to catch some\nerror conditions as close to the breaking point as possible. One example is the\nuniqueness constraint checks in the changeset. It has side effects because we're\ngoing to need to create a record and then test against it.\u003c/p\u003e\n\u003ch2\u003eCh. 9: Watching Videos\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003ealter table(:videos) do\n  add :slug, :string\nend\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ethe alter macro changes the schema for both up and down migrations. The premise\nof a slug is so you can permanently generate a name from the data in other\nfields, some of which may be updatable.\u003c/p\u003e\n\u003cp\u003eBecause Ecto separates changesets from the definition of a database record\n(model), this gives developer the ability to have different change policies for\nany type of change to the record. Changesets act as a funnel to the database,\nfiltering and casting incoming data, making sure sensitive fields like user role\ncannot be set externally, while casting them to the type defined in the schema.\nChangesets can validate data, for example the length of a field— on the fly, but\nvalidations that depend on database integrity are left to the database, in the\nform of constraints. Changesets make the code easier to reason about, can be\ncomposed, allowing each part of a data transformation to be a more easily\ncomprehend–able and testable function.\u003c/p\u003e\n\u003cp\u003eIn viws you may see a \u003ccode\u003evideo_path\u003c/code\u003e, or \u003ccode\u003ewatch_path\u003c/code\u003e, or \u003ccode\u003ex_path()\u003c/code\u003e helpers.\nThese helper functions are generated by RumblWeb.Router and imported into\ncontrollers and views by rumble_web/web.ex. \u003ccode\u003ewatch_path(@conn, :show, video)\u003c/code\u003e\ntakes the Video struct, and by conventions it generates a URL by extracting the\nID from the Video struct. This works because Phoenix knows to use the ID field\nto generated the URL because Phoenix implements the Phoenix.Param protocol.\nDeveloper can customize this behavior by implementing our own functions to\noverride the default protocol implementation to generate the URL using the slug.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edefimpl Phoenix.Param, for: Rumbl.video do\n  def to_param(%{slug: slug, id: id}) do\n    \"#{id}-#{slug}\"\n  end\nend\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe advantage of implementing this behavior change as a Protocol implementation\nis the clean polymorphism we get by extending Phoenix parameters without having\nto change Phoenix itself or the Video module itself.\u003c/p\u003e\n\u003ch2\u003eCh. 10: Using Channels\u003c/h2\u003e\n\u003cp\u003eA client connects to a channel over a socket, then sends and receives messages,\nthat's it. A Phoenix channel is a conversation, the channel sends messages,\nreceives messages, and keeps state. Just as controllers receive requests,\nchannels receive events.\u003c/p\u003e\n\u003cp\u003eA Phoenix conversation (messages sent, messages received) is about a topic, and\nthe topic maps onto application concepts like a chat room, local map, or\nannotations to a video. Each user's conversation on a topic has its own\nisolated, dedicated process.\u003c/p\u003e\n\u003cp\u003eWhereas the traditional web-based request/response interactions are stateless,\nconversations in an Elixir process can be stateful. This means for sophisticated\ninteractions like games and more interactive pages, developer doesn't need to\nuse local storage, cookies, or databases to keep track of the state of the\nconversation. Each call to the channel simply picks up where it left off. This\nonly works if the framework supports true isolation and concurrency: one process\nmust be isolated, not affect others, and true concurrency means lightweight\nabstractions that don't bleed into one another. Channels must scale in the\ndimensions developer is most concerned about: code complexity, performance, and\nmanageability.\u003c/p\u003e\n\u003cp\u003eThe application channel is simple, but it must manage three things:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emaking and breaking connections\nsending messages\nreceiving messages\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eA socket establishes a new connection with a socket. After a connection is made,\nthat same socket will be transformed through the life of the connection. The\nsocket is a representation of a continuous ongoing conversation between the\nclient and the server. When a connection is made, the initial socket is created,\nand that same socket is transformed with each new received event, through the\nwhole life of the conversation.\u003c/p\u003e\n\u003cp\u003eThe socket is an abstraction over a websocket, or to handle older clients, a\nlongpoll. The socket abstracts the transport layer, and Phoenix handles the\nrest.\u003c/p\u003e\n\u003cp\u003eA channel is a conversation about a topic, the topic has an identifier of\nvideos:\u003cvideo id\u003e where video_id is a dynamic id matching a record in the\ndatabase. Topic identifiers generally take the form topic:subtopic where topic\nis often a resource name and subtopic is the instance ID that usually identifies\na row in the database.\u003c/p\u003e\n\u003cp\u003eTopics are organizing concepts, they are used as parameters to functions, and\nused in URLs to identify conversations. Just like a URL passes an :id parameter\nto represent a request for a resource from a controller; with channels,\ndeveloper can use the topic ID to scope a channel connection.\u003c/p\u003e\n\u003cp\u003ePhoenix channels define \"callbacks\", or functions that respond to channel events\nsuch as join below:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edef join(\"videos:\" \u0026#x3C;\u003e video_id, _params, socket) do\n  {:ok, assign(socket, :video_id, String.to_integer(video_id))}\nend\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ein the above code, assign(socket...) returns a socket, resulting in a {:ok,\nsocket} returning for authorized users or {:error, socket} which will deny\njoining the channel. Sockets maintain state in a socket.assigns map, and the\nabove adds the video id to the assigns map using the helper function\n\u003ccode\u003eassign(socket, :video_id, String.to_integer(video_id))\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eSocket state is maintained for the duration of the connection. The socket is\ntransformed in a loop rather than a single pipeline. The socket state changed in\nthe above \"assign()\" call will be accessible later as events come into and out\nof the channel. This small difference leads to enormous differences in\nefficiency between the channels API and the controllers API.\u003c/p\u003e\n\u003cp\u003eChannels receive events with an event name, such as new_message, and include a\npayload of arbitrary data. Each channel module can receive events in three ways\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ehandle_in receives direct channel events\nhandle_out intercepts broadcast events\nhandle_info receives OTP messages\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ccode\u003ehandle_info\u003c/code\u003e is invoked whenever an Elixir message reaches the channel.\u003c/p\u003e\n\u003cp\u003eSession-based authentication makes sense for request/response–type applications,\nbut for channels, token authentication works better because the connection is a\nlong-duration connection. With token authentication, each user gets a unique\ntoken. Tokens allow for a secure authentication mechanism that doesn't reply on\nany specific transport. Once the user is already authenticated using the\nrequest/response traditional approach, the application can expose the token to\nthe frontend, and this token can be used by the channel.\u003c/p\u003e\n\u003ch2\u003eCh. 11: OTP\u003c/h2\u003e\n\u003cp\u003eOTP is a way to think about concurrency and distribution, using patterns that\nallow developer to use concurrency to build state without language features that\nrely on mutability.\u003c/p\u003e\n\u003cp\u003eOTP is the name of the library encapsulating concurrent state and behavior, and\nthe abstractions is called generic server, or GenServer.\u003c/p\u003e\n\u003cp\u003eSupervisors need to be able to restart processes in the right way, according to\npolicies that are best for the application. For example, suppose a database was\nupgraded, and the connection stopped. The application would need to\nautomatically kill and restart the connection pool. This policy should not\nimpact the code that uses the database. If developer can replace a simple\nsupervisor process with a supervisor tree, we can build much more robust\nfault-tolerance and recovery software.\u003c/p\u003e\n\u003cp\u003eIn Phoenix, there is little code attempting to deal with every possible error\nexception. Instead, we trust the error reporting to log the errors so developer\ncan fix what's broken, and in the meantime, supervisor tree can automatically\nrestart services in a last good state. OTP captures these clean abstractions in\na coherent library, allowing developer to declare the supervision properties\nthat most interest us. This allows developer to build robust self-healing\nsoftware without building complex self-healing software.\u003c/p\u003e\n\u003cp\u003eA supervision strategy is what policy is used when a process dies, for example,\nthe :one_for_one policy will restart a child process when the child dies. If all\nresources depend on a common service, developer could specify :one_for_all to\nkill and restart all child processes if any child dies. Developer doesn't need\nto add any code to fully supervise every process, only configure a policy to\ntell OTP how to handle each crash.\u003c/p\u003e\n\u003cp\u003eThe default restart strategy is :permanent, the supervisor always restarts child\nprocesses with this strategy. With \u003ccode\u003e:temporary\u003c/code\u003e, the child process is never\nrestarted. Under :transient, the child is restarted of if it terminates\nabnormally, with an exit reason other than :normal, :shutdown, or {:shutdown,\nterm}.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e:temporary\u003c/code\u003e is useful when a restart is unlikely to resolve a worker crash, or\nwhen restarting doesn't make sense based on the flow of the application.\u003c/p\u003e\n\u003cp\u003eChild processes can have different restart strategies, supervisors have\nconfigurable supervision strategies.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e:one_for_one if a child terminates, the supervisor restarts only that process\n:one_for_all if a child terminates, a supervisor terminates all remaining children, the restarts all children\n:rest_for_one if a child terminates, a supervisor terminates all children defined after the one that died, then restarts all terminated processes\n:simple_on_for_one Similar to :one_for_one but used when a supervisor needs to dynamically supervise processes. For example, a web server would use it to supervise web requests, which may be 10, 10,000, or 100,000 processes.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eA channel in Phoenix is an OTP process built to serve a single user in the\ncontext of a single conversation on a topic.\u003c/p\u003e\n\u003cp\u003eWhen fetching real-time (pp 212) data about a video that will be synched in\nreal-time, a network or failure by the 3rd party can occur, but since we're\nmaking multiple requests, we can ignore the failure and use the responses from\nthe successful requests. The restart strategy spawns multiple processes and\nshouldn't restart the failures because its time sensitive, the errors can simply\nbe moved past, we'll accept any successful requests that return, so we'll use\nthe :temporary restart strategy.\u003c/p\u003e\n\u003cp\u003eOTP applications protect in both directions, that is, if the Phoenix server\nitself crashes, we bring down the children processes and restart them so no\nresources are leaked. When a child process crashes, we bring down that process\nand restart it so we can attempt clean recovery. When building a supervisor and\nchildren, the process of defining the restart strategy and supervision strategy\nlets developer focus on the main application logic, as any unknowable error\nhandling is already taken care of. Let the errors crash gracefully, and OTP will\nhandle the rest.\u003c/p\u003e\n\u003cp\u003eBecause GenServer's are meant to be generic servers, they hold both computation\nand state. However, in many cases, we want a process to store state only, or\nonly execute a particular function. An agent is a simple GenServer\u003c/p\u003e\n\u003ch2\u003eChapter 12: Observer and Umbrellas\u003c/h2\u003e\n\u003cp\u003eUmbrella projects allow developer to develop and test multiple child\napplications in isolation side by side while providing conveniences for\nconfiguring and building them only once. Instead of breaking the application\ninto multiple applications in distinct source-code repositories, which adds too\nmuch overhead to the development workflow, you can use Umbrella projects. The\ncombination of Mix and OTP make this abstraction a good one for separating core\nconcerns and dealing with complexity.\u003c/p\u003e\n\u003cp\u003eObserver is an Erlang tool for understanding all running processes in an\napplication. To start:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eiex -S mix\n\u003e :observer.start\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe tool visualizes different aspects of the application, lets developer see a\nlist of all running processes, how much memory, messages the system is using.\nDeveloper can more easily see where bottlenecks occur by finding processes with\nlarge numbers of messages.\u003c/p\u003e\n\u003cp\u003eEach Umbrella has a parent directory that defines\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eThe shared configuration of the project\nThe dependencies for that project\nthe apps directory with child applications\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eto create new Umbrella projects, use mix new \u003cname\u003e --umbrella outside of your\napplication, which stubs out a simpler Phoenix app, this project can then be\nmoved inside the parent project.\u003c/p\u003e\n\u003ch2\u003eChapter 13: Testing Channels and OTP\u003c/h2\u003e\n\u003cp\u003eStubs and mocks are both testing fixtures that replace real world\nimplementations. A stub replaces real-world libraries with simpler, predictable\nbehavior. With a stub, developer can bypass code that would be difficult to\ntest. Stubs should have nothing to say whether a test should pass or fail. Stubs\nare a simple scaffold implementation standing in for a more complex real-world\nimplementation.\u003c/p\u003e\n\u003cp\u003eA mock is similar, but replaces real-world behavior just as a stub does, but it\nallows developer to specify expectations and results, playing back those\nexpectations at runtime. A mock will fail a test if the test code doesn't\nreceive the expected function calls. A mock is an implementation that records\nexpected behavior at definition time and plays it back at runtime, expecting\nthose results.\u003c/p\u003e\n\u003cp\u003eWithin the Elixir community, avoid mocking whenever possible. Most mocking\nlibraries end up changing global behavior—for example, by replacing a function\nin the HTTP client library to return a particular result. These function\nreplacements are global, so a change in one place would change all code running\nat the same time. That means tests written in this way can no longer run\nconcurrently. These strategies will snowball, requiring more and more mocking\nuntil the dependencies among components are completely hidden.\u003c/p\u003e\n\u003cp\u003eA better strategy is to identify code that's difficult to test live, and to\nbuild a configurable, replaceable testing implementation rather than a dynamic\nmock. The development and production code will use the simple :httpc client, and\nthe testing code will use a stub that is called during the tests.\u003c/p\u003e\n\u003cp\u003eA major advantage of writing asynchronous tests in OTP is the tests run\nconcurrently, meaning they can be run in parallel, so the entire suite of tests\ncan finish more quickly than their synchronous counterparts.\u003c/p\u003e\n\u003ch2\u003eReferences\u003c/h2\u003e\n\u003cp\u003eMcCord, Chris. Tate, Bruce. Valim, José. (2016). \u003cem\u003eProgramming Phoenix\u003c/em\u003e. The\nPragmatic Programmers. Print.\u003c/p\u003e\n","slug":"2017-12-22-programming-phoenix","license":"cc-by-nc-sa"}},"__N_SSG":true},"page":"/[year]/[month]/[day]/[slug]","query":{"year":"2017","month":"12","day":"22","slug":"programming-phoenix"},"buildId":"8lVdZxFQRcnwEza8kfMUH","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>