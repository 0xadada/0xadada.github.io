3:I[5613,[],""]
5:I[1778,[],""]
6:I[5250,["250","static/chunks/250-d7e0a94ebe194dac.js","931","static/chunks/app/page-84b663e6d1625d16.js"],""]
7:I[1749,["250","static/chunks/250-d7e0a94ebe194dac.js","749","static/chunks/749-1aefd436964833c3.js","185","static/chunks/app/layout-27c6969c51c79394.js"],"Image"]
4:["slug","2003/12/14/speech-segmentation-in-japanese-and-english-bilinguals","c"]
0:["VbQdpIM5ri7XIRJDcVd1I",[[["",{"children":[["slug","2003/12/14/speech-segmentation-in-japanese-and-english-bilinguals","c"],{"children":["__PAGE__?{\"slug\":[\"2003\",\"12\",\"14\",\"speech-segmentation-in-japanese-and-english-bilinguals\"]}",{}]}]},"$undefined","$undefined",true],["",{"children":[["slug","2003/12/14/speech-segmentation-in-japanese-and-english-bilinguals","c"],{"children":["__PAGE__",{},["$L1","$L2",null]]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","$4","children"],"loading":"$undefined","loadingStyles":"$undefined","loadingScripts":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/01f63581d77a7b07.css","precedence":"next","crossOrigin":""}]]}]]},[null,["$","html",null,{"lang":"en-US","children":["$","body",null,{"children":[["$","main",null,{"className":"layout_layout__dCqca","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"loading":"$undefined","loadingStyles":"$undefined","loadingScripts":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[],"styles":null}]}],["$","footer",null,{"className":"footer_footer__woFMY layout_layout__dCqca","children":[["$","nav",null,{"children":["$","p",null,{"children":["¬© 2003-","2024"," 0xADADA (unless otherwise noted.)",["$","br",null,{}],["$","span",null,{"className":"h-card","children":[["$","span",null,{"className":"p-note","hidden":true,"children":"0xADADA is a Software Engineer / Writer / Motorsports Driver exploring the impact of the attention economy on idleness, time, and lived experience. üßë‚Äçüíªüîßüöó‚úçÔ∏èüîí"}],["$","$L6",null,{"className":"u-url u-uid","href":"https://0xadada.pub/","title":"0xADADA","children":"0xADADA"}]," ",["$","$L6",null,{"href":"mailto:0xadada.pub@protonmail.com","className":"c-Meta u-email","rel":"me","title":"0xADADA","children":"Email"}]," ",["$","$L6",null,{"href":"https://warpcast.com/0xadada","className":"u-url","rel":"me nofollow external noopener","title":"0xADADA on Warpcast","children":"Farcaster"}]," ",["$","$L6",null,{"href":"https://mastodon.cloud/@0xADADA","className":"u-url","rel":"me nofollow external noopener","title":"0xADADA on Mastodon","children":"Mastodon"}]," ",["$","$L6",null,{"href":"https://bsky.app/profile/0xadada.bsky.social","className":"u-url","rel":"me nofollow external noopener","title":"0xADADA on Bluesky","children":"Bluesky"}]," ",["$","$L6",null,{"href":"https://twitter.com/0xadada","className":"u-url","rel":"me nofollow external noopener","title":"0xADADA on Twitter","children":["$","del",null,{"children":"Twitter"}]}]," ",["$","$L6",null,{"href":"https://github.com/0xadada","className":"u-url","rel":"me nofollow external noopener","title":"0xADADA on GitHub","children":"GitHub"}]," ",["$","$L6",null,{"href":"https://www.goodreads.com/review/list/60524683-0xadada?shelf=wanted","className":"u-url","rel":"me nofollow external noopener","title":"0xADADA on Goodreads","children":"Goodreads"}]," ",["$","$L6",null,{"href":"/colophon/","children":"Colophon"}]," ",["$","$L6",null,{"href":"/rss.xml","children":"RSS"}]," ",["$","$L7",null,{"className":"u-photo","src":"/static/images/meta/avatar.svg","alt":"0xADADA icon","width":"20","height":"20","style":{"marginBottom":"-0.125rem"}}]]}]]}]}],["$","form",null,{"action":"https://buttondown.email/api/emails/embed-subscribe/0xadada","method":"post","target":"popupwindow","children":[["$","label",null,{"htmlFor":"email","children":["Sign up to get emailed when I write new things:"," "]}],["$","input",null,{"type":"email","name":"email","id":"email"}],["$","input",null,{"type":"submit","value":"Subscribe"}]]}]]}]]}]}],null]],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/04cd9c7ad2e403ea.css","precedence":"next","crossOrigin":""}]],"$L8"]]]]
9:T471,
(Otake, Hatano, Cutler & Mehler, 1993). Japanese is a ‚Äúmora-timed‚Äù language,
where each mora represents a rhythmic unit; in comparison to English which is
‚Äústress-timed‚Äù (Beckman, 1982). There has been much research into the field of
speech segmentation, specifically into the types listed above including
stressed-base and syllable-based (Cutler, Mehler, Norris & Segui, 1986), as well
as mora-based (Cutler & Otake, 1994). There has also been a fair amount of study
on phoneme discrimination of /r/ and /l/ in native speakers of Japanese and
English. One particular study (Miyawaki, Strange & Verbrugge, Liberman, Jenkins
& Fujimura, 1975) used synthesized speech to compare Americans and Japanese at
discriminating /ra/ and /la/. A follow-up study a few years later reinforced
previous findings that Americans can categorically discriminate the phonemes,
and that Japanese are at a near-chance level of perception (Strange & Dittmann,
1984). This study also showed that after training, the native Japanese
performance on those phones increased, and therefore categorical perception is
indeed possible for non-native speakers.a:T45a, (Miyawaki et al., 1975). The /r/ and /l/ phones don‚Äôt constitute a
phonemic contrast in Japanese, and therefore would provide a good base to
conduct tests on the differences in native and non-native discrimination tasks.
This research paper used a speech sound generator to create a series of sounds
between two phones in order to see where and how categorical perception would
occur. The parallel-resonance synthesizer generated 15 3-formant sounds that
would be used in the tests. The third formant (F3) was varied in frequency in
steps between the /ra/ and /la/ sounds. From this set of 15 sounds, there would
be two types of tests conducted on each subject group, an identification test
and an oddity discrimination test. The subjects of this research consisted of 39
native American speakers and 21 native Japanese speakers. The discrimination
task was to listen to a series of three sounds, and make note of which of the
three was different. The results showed that the Americans could easily
discriminate the target sound, only getting low scores if the sounds were
ambiguous as to if it was /r/ or /l/, b:T41e,In a study of brain plasticity and non-native lexical and segmentation
processes, Sanders et al. in 2002 came up with interesting methods for probing
non-native speakers to see if learning the L2 early in life or later in life
would effect the ability to learn non-native lexical and segmentation processes.
This study would research various language subsystems such as lexico-semantic,
syntactic, and prosodic information processing. Using the earlier proposed
theories that each language uses different methods of speech segmentation
(English: Stress-based, French: Syllable-based, Japanese: Mora-based) they tried
to find if non-native speakers used lexico-syntactic information processes when
segmenting a non-native language (Sanders et al., 2002). Four groups were
obtained to study this hypothesis, a group of native English speakers as a
control group (E), early English Japanese native speakers (JE), early English
Spanish native speakers (SE), late English Japanese native speakers (J), and a
group of late English Spanish native speakers (S). c:T427,In studies such as Otake (1993), findings were made that support the idea that
non-native speakers segment a foreign language using their native segmentation
processes. Future research could focus on bilinguals learning an L2 that has a
totally different segmentation process. An example would be how a Japanese
learning English or an American learning French begins to learn a new
segmentation style, and how specific training on segmentation may make learning
more efficient. A similar study design that could be used would be Strange &
Dittmanns‚Äô (1984) study on the /r/ - /l/ distinction of Japanese speaking
English. This study could be applied to learning a new segmentation style. The
time of acquisition of a new non-native speech segmentation process would also
be of interest to this field. Also of interest to future research is the topics
of multiple segmentation cues, such as lexico-semantic, and prosody, and how
these can be used interchangeably along with more native segmentation processes
such as mora, syllable, or stress-based segmentation.2:["$","article",null,{"className":"hentry h-entry","children":[["$","header",null,{"children":[["$","h1",null,{"className":"entry-title p-name","children":"Speech Segmentation in Japanese and English Bilinguals"}],"$undefined",["$","time",null,{"className":"display-date_published__A_L_d dt-published","dateTime":"2003-12-14T00:00:00.000Z","children":"Sunday December 14, 2003"}],["$","span",null,{"className":"h-card page_byline__wbXsN","children":[" by ",["$","span",null,{"className":"fn p-author p-name","children":"0xADADA"}]]}]]}],["$","div",null,{"className":"entry-content e-content","children":[["$","center",null,{"children":["Colleges of Computer Sciences and Science (Psychology)",["$","br",null,{}]," Northeastern University Boston, MA",["$","br",null,{}]," Topics Concerning Japanese and English Bilinguals "]}],"\n",["$","h3",null,{"id":"a-literature-review","children":"A Literature Review"}],"\n",["$","hr",null,{}],"\n",["$","p",null,{"children":"The purpose of this paper is to provide an overview of the existing literature concerning speech segmentation, categorical perception, and some other issues concerning bilinguals of English and Japanese. It will touch on some of the differences in the two languages and how they affect learning the L2. The paper will start by providing background information about some of the two languages and some of the current issues involved in speech processing. The paper will then delve into the most current research, what the issues are, how it was done, and the results they have found. It will then go on to discuss the possible future directions of this research and end with references."}],"\n",["$","hr",null,{}],"\n",["$","h2",null,{"id":"i-introduction","children":"I. Introduction"}],"\n",["$","p",null,{"children":"People trying to learn a second language always have a difficult task ahead of\nthem. Learning a new grammar and lexicon takes both time and practice. Some\nlanguages are diverse enough that on top of a new lexicon and grammar, there is\nalso an entirely new segmentation system to be learned. Japanese and English are\ndiverse in the way the speech streams are segmented into parts, with Japanese\nusing the mora as the basic unit of perception (McQueen, Otake & Cutler, 2001)\nand English using stress (Cutler & Butterfield, 1992). Along with learning a new\nlexicon, grammar, and segmentation system is the problems with categorical\nperception. How can bilinguals learn new sounds that aren‚Äôt in their native\nlanguage? Another important issue is if there are multiple segmentation cues,\ncan there be universal segmentation cues besides the rhythm-based mora, syllable\nand stress-based processes? This paper will try to uncover these questions."}],"\n",["$","h3",null,{"id":"background","children":"Background"}],"\n",["$","p",null,{"children":["When participating in conversation the listener must do a myriad of tasks to\ncomprehend what the other speaker is saying. On the physical side of things, the\ncomprehended must process the sound waves from the air into electrical signals\ninto the brain. From there he/she must then begin the process of turning sound\nsignals into words, phrases, sentences, and finally into a complete dialog. The\naspect of changing the raw speech stream into words is called speech\nsegmentation (Cutler, Hehler, Norris & Segui, 1986). Speech segmentation is\nusually accomplished in different ways according to the language, the major\nrhythm-based segmentation types are mora in Japanese, syllable in French\n(Cutler, Hehler, Norris & Segui, 1986), and stress in English. The mora is the\nsmallest Japanese unit of perception, it is subsyllabic. ",["$","em",null,{"children":"‚ÄúIt can be a vocalic\nnucleus, a nucleus plus syllable onset, or, as in the second and fourth morae of\nshinshinto, it can be the postvocalic portion of a syllable, i.e., the coda.‚Äù"}],"$9"]}],"\n",["$","h2",null,{"id":"ii-review-of-the-existing-literature","children":"II. Review of the existing literature"}],"\n",["$","h3",null,{"id":"1-discrimination-of-r-and-l-by-native-speakers-of-japanese-and-english","children":"1. Discrimination of /r/ and /l/ by native speakers of Japanese and English"}],"\n",["$","p",null,{"children":["Categorical perception of speech sounds is an important aspect of speech\nsegmentation. Without the ability to properly hear differences in non-native\nspeech sounds, then segmentation and comprehension will be negatively impaired.\nOne of the seminal research papers written comparing the abilities of English\nand Japanese natives to discriminate the /r/ and /l/ sound was by Miyawaki,\nStrange, Verbrugge, Liberman, Jenkins and Fujimura in 1975. The research here\nconfirms earlier findings that Japanese subjects cannot distinguish between /r/\nand /l/ (Goto, 1971). This paper investigated the effects of linguistic use on\nthe ability to discriminate the class of liquid phones in English and Japanese\nnatives. They focused on the phonemes /l/ and /r/ in syllable-initial position.\nThe choice for the liquid /l/ and /r/ phones were made due to the fact that\n",["$","em",null,{"children":"‚Äúthe distinction between these phones is phonemic in English but not in\nJapnanese.‚Äù"}],"$a",["$","em",null,{"children":"‚Äúpairs whose members were labeled as the\nsame phoneme was considerable less accurate, although still above the 33% chance\nlevel‚Äù"}]," (Miyawaki, et al. 1975). The Japanese however, showed a near-chance\nlevel of discrimination. An interesting aspect of this study was the finding\nthat when speech sound context was not included, the two groups behaved almost\nidentically, ",["$","em",null,{"children":"‚Äúwe see very clearly that the Japanese do not differ from the\nAmericans on any of the comparison pairs. The nonspeech discrimination functions\nare virtually identical for the two groups of subjects.‚Äù"}]," (Miyawaki, et at.,\n1975). Both groups were able to discriminate isolated F3 patterns quite\naccurately, which indicates that both groups are able to hear sound differences\nphysical sub-contextual level."]}],"\n",["$","p",null,{"children":["A later study on the /r/ and /l/ perception task using a synthetic speech\ngeneration process similar to the above study focused on if linguistic\nexperience had an impact on categorical perception of the /r/ and /l/ phonemes.\nThey were suggesting that ",["$","em",null,{"children":"‚Äú‚Ä¶ native Japanese adults learning English as a\nsecond language are capable of categorical perception of /r/ and l/l.‚Äù"}],"\n(MacKain, et al., 1981). This study was similar to the above, but had a few\ndifferences. The first was how they varied the acoustic values both temporally\nand spectrally ",["$","em",null,{"children":"‚Äúto optimize the Japanese subjects‚Äô opportunity to show\nperceptual differentiation of the /r/-/l/ contrast.‚Äù"}]," (MacKain, et al., 1981).\nThe second was the inclusion of an AXB oddity discrimination task which is\nthought to provide a better opportunity to let the subjects detect auditory\ndifferences. The third was an identification task for both the American and\nJapanese groups. The Americans, as expected, displayed a strong category\nboundary with strong identification scores. The not-experienced Japanese\nsubjects displayed poor categorization of the /r/ and /l/ with near chance\nlevels in all stimuli types. The results in the non-experienced Japanese group\nextended the Miyawaki results. The experienced group ",["$","em",null,{"children":"‚Äúhad intensive English\nconversation training with native American-English speakers and as a group spent\na larger percentage of the average day conversing in English than did the\nnot-experienced Japanese.‚Äù"}]," (MacKain, et at., 1981). This group displayed\nsimilar results to the Americans on both the identification tasks and the\ndiscrimination tasks. These results suggests ",["$","em",null,{"children":"‚Äúthat the occurrence and\nabruptness of an /r/-/l/ category boundary for the experience Japanese might be\nrelated to their grater conversational English experience‚Ä¶‚Äù"}]," (MacKain et al.,\n1981). This particular study had added the AXB oddity task because it is less\nmemory demanding and ",["$","em",null,{"children":"‚Äúthey could use nonphoenetic auditory memory to aid\nperformance‚Äù"}]," (MacKain et al., 1981) in the hopes that it would allow the\nnon-experienced Japanese to obtain better results, but this particular task did\nnot achieve the hopes of its design intentions. Overall, their research\nsuggested that Japanese native speakers can obtain categorical perception of /r/\nand /l/ with some practice and more experience, which is good news to aspiring\nbilinguals."]}],"\n",["$","p",null,{"children":["Research into the /r/ and /l/ phonemes is interesting particularly to Japanese\nbecause of the lack of contrast between these sounds. Distinctive contrast in\nparticular speech sounds allows speakers of that language to discriminate where\nother language speakers would not. The above studies examined this aspect as\nwell, but in a 1984 study by Strange and Dittmann focused on how the ability to\ndiscriminate sounds not available in the L1 can change with explicit training,\n",["$","em",null,{"children":"‚ÄúWe were interested in whether we could modify the perception of AE\nword-initial /r/ and /l/ by adult Japanese learners of English, in the\nlaboratory, using the psychophysical training task successfully employed by\nCarney et al. (1977).‚Äù"}]," (Strange & Dittmann, 1984). The design of this study was\nto test the abilities of eight female Japanese native speakers before training\nand then again after training to examine the effects of training on\ndiscrimination of the /r/ and /l/ series of synthetic speech sounds. The initial\npre-training tests consisted of a minimal pairs test, an identification of the\nrock-lock series and an oddity discrimination task. The training was done\nindividually over a three week period that totaled 14 to 18 sessions, it\nconsisted of an AX discrimination task with immediate feedback. At the end of\nthe training the post-training tests were given, which were the same as the\npre-training tests. Pre-training results were similar to the results found in by\nMacKain in 1981, with near-chance levels of accuracy. The training task\nperformance showed ",["$","em",null,{"children":"‚Äúgradual improvement over session with the greatest\nimprovement in the first several sessions‚Äù"}]," (Strange & Dittmann, 1984). All\nsubjects showed increased performance as the training sessions progressed. After\nthe training, the post-training tests showed that ",["$","em",null,{"children":"‚Äúpretraining versus\nposttraining categorical perception tests for each of the eight subjects of the\nrock-lock series revealed that seven of the eight subjects improved as a\nfunction of the training.‚Äù"}]," (Strange & Dittmann, 1984). Post-training test\nresults also showed improvement in their discrimination of cross-category pairs\nwith over 75% correct (Strange & Dittmann, 1984). Overall, training did indicate\nimprovement as performance on post-training tests showed better performance\nresults (Strange & Dittmann, 1984). ",["$","em",null,{"children":"‚ÄúWe can thus conclude that training with a\nfixed-standard AX discrimination task resulted in improved (categorical)\nperception of the training stimuli, as tested by the (more demanding)\nidentification and oddity discrimination tasks‚Äù"}]," (Strange & Dittmann, 1984).\nThis study also tested differences in sound according to acoustic properties.\nThey wanted to test if training in the /rock/ - /lock/ set would transfer over\nto good performance in a similar /rake/ - /lake/ test. Their results showed that\nit was indeed the case, and training did help this test achieve improved\nperformance, this also supports the idea that bilinguals can indeed learn to\nperceive new sounds in a second language."]}],"\n",["$","h3",null,{"id":"2-speech-segmentation-using-the-mora","children":"2. Speech segmentation using the Mora"}],"\n",["$","p",null,{"children":"One of the major differences in a language is how it is timed, or the rhythm of\nthe language. Bilinguals not only have to learn a new lexicon and grammar\nsystem, but sometimes must also learn a new rhythm to speak their L2. An example\nof this would be a native English speaker trying to learn Japanese or\nvise-versa. English is a stress-timed language whereas Japanese is mora-timed.\nResearch of the mora can help lead to further understanding of how languages of\ndifferent timing style can be learned more efficiently for bilinguals as well as\nhow differences in timing can affect different aspects of speech segmentation."}],"\n",["$","p",null,{"children":["In a study by Otake et al., 1993, of how Japanese words are segmented by native\nand non-native listeners, results found that Japanese responses were consistent\nwith moraic segmentation, while non-native listeners responded differently. The\nmora is a uniquely Japanese timing mechanism; it is smaller than a syllable, and\nis considered the basic unit of perception. This study set up four experiments\nwith native and non-native speakers to examine the effects of native timing type\non Japanese words. In the first experiment 40 native Japanese speakers were used\nto listen to a series of 3 to 6 words. When a word was heard with the target\nletters on a printed card, the subject was to press a button. Results showed\nthat the native Japanese listeners responses better supported the mora\nhypothesis than the syllable hypotheses, confirming their initial hypothesis\nthat Japanese speakers will segment words best with the mora-timed segmentation\nstyle. ",["$","em",null,{"children":"‚ÄúThe pattern of results in this experiment thus appears to offer strong\nsupport for the mora hypothesis but none to the syllable hypothesis.‚Äù"}]," (Otake et\nal, 1993). The second experiment tested English speaking subjects on Japanese\nwords. The design was similar to the previous task on the Japanese subjects,\nwhere a series of Japanese words would be played, and the subject was to press a\nbutton once the sound on the printed card was heard. The results of this\nexperiment are in stark contrast with that of experiment 1. The findings in this\nexperiment support that ",["$","em",null,{"children":"‚Äúwe many now conclude that English listeners do not\nexploit mora structure in the same way.‚Äù"}]," (Otake et al., 1993) [as Japanese\nnative listeners]. A note about experiment 1, the targets were presented on\nprinted card in Roman text orthography, this can introduce confounds due to how\nJapanese naturally represent sounds with kana characters. Experiment 3 was\nexactly the same design as experiment 1 with the exception that the target word\nwas played through the headphones before the word sequence rather than being\nprinted on a card. Experiment 3 had 40 native Japanese speaking subjects. The\ntarget sound was played first, followed by the sequences of test words. The\nsubject would press a button when they heard the target word in the sequence.\nThe results of experiment 3 replicated those of experiment 1. ",["$","em",null,{"children":"‚ÄúThe replication\nof experiment 1‚Äôs results strongly confirms our conclusions from the preceding\nexperiments: Japanese listeners do not naturally segment speech syllable by\nsyllable; they do naturally segment it mora by mora.‚Äù"}]," (Otake et al., 1993).\nThese findings also cleared up the problem of orthography of Japanese speech\nsounds mentioned earlier. Experiment 4 was identical to experiment 3 with the\nexception of the subjects being 33 native French speakers. The results of\nexperiment 4 were as expected, ",["$","em",null,{"children":"‚Äúthe response patterns of French listeners are,\nas predicted, best accounted for by the syllabic hypothesis‚Ä¶‚Äù"}]," (Otake et al.,\n1993). These results also support findings by Cutler et al., 1986. Finally,\nthese results support the predictions made initially, that non-native listeners\nwill ",["$","em",null,{"children":"‚Äúnot replicate the pattern of results shown with the same materials by\nnative Japanese listeners‚Äù"}]," (Otake et al., 1993). These results support that\nnon-native listeners will try to segment the sounds of a non-native language by\napplying their native speech segmentation system, whether its mora-based,\nstress-based or syllable-based. How could that affect learning a second language\nwhere the segmentation process is different from the native language?"]}],"\n",["$","p",null,{"children":["Taking from some of the findings from the previous study, Cutler and Otake\nstarted a new research project in 1994 that would focus on whether subjects\nwould apply their native segmentation processes to a foreign language. This\nresearch could show that inappropriate use of a segmentation process could\ninhibit the processing of a non-native language in bilinguals. ",["$","em",null,{"children":"‚ÄúThis suggests\nthat segmentation procedures may indeed be highly similar to phonological\ncategorization procedures: they effectively aid processing of the native\nlanguage, but they may reduce processing efficiency for input in a foreign\nlanguage.‚Äù"}]," (Cutler & Otake, 1994). This study also hoped to further support the\nmora hypothesis put forth by Otake in 1993. The first experiment was to test\nJapanese native speakers on if moraic targets ",["$","em",null,{"children":"‚Äúwill be easier to detect than\nnonmoraic‚Äù"}]," (Cutler & Otake, 1994). This should prove to be the case under the\nmora hypothesis. This experiment will also test for whether phoneme detection\n",["$","em",null,{"children":"‚Äúwill be differentially difficult for vowels versus consonants‚Äù"}]," (Cutler &\nOtake, 1994). Experiment 1 was designed with 40 native Japanese speakers, they\nwould listen for a target sound (O or N) in a series of Japanese words, and\npress a response key as soon as they detected the sound. The results from this\nexperiment showed the same results seen in Otake (1993): ",["$","em",null,{"children":"‚Äúmora structure is\ncrucially involved in the process by which Japanese listeners convert spoken\ninput in lexically accessible representation.‚Äù"}]," (Cutler & Otake, 1993), as well\nas showing that quick responses to moraic input is not only restricted to CV\ninput as seen in Otakes‚Äô 1993 experiment. It should be noted that there were no\nsignificant findings concerning the differences in vowel and consonant response\ntimes mentioned earlier. Experiment 2 was conducted in the same way as\nexperiment 1, with the exception that the subjects were 24 English native\nspeakers. As expected, the non-native speakers did not demonstrate the moraic\neffects shown in experiment 1. Interestingly, ",["$","em",null,{"children":"‚ÄúThe main effect of vowel versus\nconsonant target was, however, significant; consonants were detected both faster\nand more accurately than vowels‚Äù"}]," (Cutler & Otake, 1994). This supports the\nearlier findings that native listeners apply their native segmentation\nprocedures to a non-native language. Experiment 3 was similar to experiment 2\nexcept with English words as both target and sequence words. The subjects were\n24 native English speakers. Findings in experiment 3 showed that ",["$","em",null,{"children":"‚ÄúThese\nfindings are in line with previous failures to find significant phoneme\ndetection differences between targets in stressed versus unstressed position\nwith English listeners and laboratory-read speech.‚Äù"}]," (Cutler & Otake, 1994).\nThese results were similar to experiment 2. Experiment 4 used the same materials\nas experiment 3, English words, while it used the same design, as well as the\nsame subjects as in experiment 1, using Japanese native speakers. Results in\nthis experiment found the vowel sounds were much more difficult to find than the\nconsonantal targets (Cutler & Otake, 1994). The findings of experiment 4 also\nhelp support the mora hypothesis, namely ",["$","em",null,{"children":"‚Äúthat mismatch between in the input\nand the native language phonemic repertoire plays a role in phoneme detection in\na foreign language.‚Äù"}]," (Cutler & Otake, 1994). Experiment 5 was designed with 20\nnative Japanese speakers listening to a played back words of native Japanese.\nThe procedure was that of experiment 1. The subjects were to press a button when\nthey heard a target phoneme in the played back word sequence. The results of\nthis experiment demonstrated what the experimenters had hoped for, ",["$","em",null,{"children":"‚Äúa new\neffect in Japanese phoneme-monitoring: targets in word-initial position are\ndetected faster than targets in word-medial position.‚Äù"}]," (Cutler & Otake, 1994).\nThis experiment showed the mora effect was significant for all four phoneme\ntargets. Experiment 6 involved doing the same as experiment 5 but with English\nsubjects. This experiment was designed to show the English speakers advantage of\ndetection of vowels over consonant targets, as well as observing a difference in\nRTs in A and O due to ",["$","em",null,{"children":"‚Äúphoneme repertoire mismatch‚Äù"}]," (Cutler & Otake, 1994).\nThis experiment used the same materials in experiment 5 with 23 native English\nspeakers using the procedures from experiment 2. The results of this experiment\nshowed that ",["$","em",null,{"children":"‚Äúthere was again a significant overall advantage for consonant over\nvowel targets‚Äù"}]," (Cutler & Otake, 1994). Overall these experiments had shown that\nthe ",["$","em",null,{"children":"‚Äúthe moraic effect which Japanese listeners show in phoneme detection in\ntheir native language appears in world-initial well as in word-medial position\nand with a variety of phoneme targets.‚Äù"}]," (Cutler & Otake, 1994). These\nexperiments also go to explain that the consonant and vowel detection abilities\nof English speakers appear when listening to Japanese, due to native English\nsegmentation process advantages in this particular task."]}],"\n",["$","p",null,{"children":["In terms of bilinguals, the above studies used Japanese native speakers, some of\nwhom had some English experience. Even with experience, it was shown that they\nwere still applying mora segmentation to English input, as well as English\nspeakers applying stress-based segmentation when it is inappropriate in Japanese\ninput. As quoted from the above study ",["$","em",null,{"children":"‚ÄúWe believe that this finding has\npotentially important implication for understanding the processes of acquisition\nof a second language.‚Äù"}]," (Cutler & Otake, 1994). Although this research seems to\ncast a dim picture on those trying to learn a second language, all is not lost,\nin a study by Cutler et al., 1992, showed that more than one segmentation\nprocess is available, and even if not available, these rhythm based processes\nare heuristics for processing a non-native language, are not necessary for\ncomprehension. (Cutler et al., 1992)."]}],"\n",["$","h3",null,{"id":"3-rhythmic-cues-and-the-lexicon","children":"3. Rhythmic cues and the Lexicon"}],"\n",["$","p",null,{"children":["An important area of study is on how rhythmic cues in the speech stream affect\nlexical decisions in Japanese. A learner of Japanese would be affected by this\nin that many languages don‚Äôt use rhythmic cues, and so in learning Japanese,\nrhythmic cues may not serve as an available heuristic. Japanese provides a good\ntest platform to study rhythmic cues and speech segmentation because Japanese\n",["$","em",null,{"children":"‚Äúrhythm is based neither on syllables nor on stress. Instead, it is based on\nthe mora, a subsyllabic unit which can be of five different types‚Äù"}]," (McQueen et\nal, 2001)."]}],"\n",["$","p",null,{"children":["In a study by Cutler and Otake, examining old Japanese wood-block prints which\ncontained a word-based joke system called ‚ÄúGoroawase‚Äù to examine if there\nexisted sub-moraic information processing in the speech stream. This joke system\nis used by substituting a single mora in a word with another mora to create a\nsimilar sounding word with a different meaning, thus creating a word-pun. Their\nfindings suggest that ",["$","em",null,{"children":"‚Äúmora substitution is more often than would be expected\nby chance in effect phoneme substitution because two words which overlap in all\nbut a single consonant or vowel form a better target-pun pair than two words\nwhich overlap in all but a CV mora.‚Äù"}]," (Cutler & Otake, 2002). Another experiment\nin the same paper uses word reconstruction. They had subjects listen to a word\nwith a replaced mora, and were told which mora was changed. The subjects were\nthen to tell which word had been intended. This experiment would use mora as the\ncue to the target word, which would then be accessed via the lexicon. 45 native\nJapanese speaking subjects partook in this experiment and the results were as\nexpected. 4 mora words were more easily accessed than 3 mora words. Their\nresults suggested that ",["$","em",null,{"children":"‚Äúword reconstruction was significantly easier when the\ninitial mora had been replaced by another mora sharing with it either C or V.‚Äù"}],"\n(Cutler & Otake, 2002). In their second experiment, they distorted the final\nmora of the word, and found that identification was faster and more accurate,\nwhich suggests ",["$","em",null,{"children":"‚Äúthat this information can be exploited continuously rather than\nonly on a mora-by-mora basis‚Äù"}]," (Cutler & Otake, 2002). This indicates that\nlexical access can occur without complete moraic information. In experiments 3\nand 4 of their research, they tested replacing both third and fourth mora and\nplaced a focus on consonant versus vowel replacement, they found similar results\nto experiment two. ",["$","em",null,{"children":"‚ÄúAlthough both the V-replacement and C-replacement condition\nproved easier than M-replacements, there was also a difference between the first\ntwo: Replacement of a vowel proved easier than replacement of a consonant.‚Äù"}],"\n(Cutler & Otake, 2002). On their 5th experiment, they used a Yes/No type\nresponse to the design of the previous experiment, their results helped to\ncement their conclusion that there is continuous speech processing, even below\nthe level of the mora. (Cutler & Otake, 2002). This does not suggest that there\nis no mora, but only that continuous speech sounds are helpful to the\nsegmentation of the continuous speech stream (Cutler & Otake, 2002). Their\nresults do however suggest, and support the findings of Norris et al., in 1997,\n",["$","em",null,{"children":"‚Äúthat the contribution of rhythmic categories in word recognition is the same\nfor all languages‚Äù"}]," (Cutler & Otake, 2002). As rhythmic categories are universal\nto all languages (Norris et al., 1997), this would provide some relief to\nlearners of Japanese a second language, as mora are not the end-all to\nsegmentation, but a heuristic function that helps to make segmentation more\nefficient."]}],"\n",["$","p",null,{"children":["If mora and meter are both parts of speech segmentation, how can we combine the\ntwo? A study by McQueen, Otake and Cutler in 2001 tries to answer that. Their\nexperiments use the PWC, or Possible Word Constraint developed by Norris et al.,\nin 1997 to test if Japanese speakers use the PWC just as English and Dutch\nspeakers. The PWC is another type of heuristic function used in speech\nsegmentation particular to the Shortlist model developed by Norris et al.,\n(1997). Their first experiment used 54 native Japanese speakers to listen to a\nnative speaker of Japanese pronounce nonwords with Japanese words embedded\nwithin them. The subjects were then to press a button and pronounce the embedded\nword once they heard it. The results of the experiment showed that indeed,\n",["$","em",null,{"children":"‚Äúlisteners find it harder to spot words in impossible word contexts than in\npossible word contexts. Japanese listeners therefore appear to use the PWC when\nsegmenting speech.‚Äù"}]," (McQueen, Otake & Cutler, 2001). Other tests done in this\nstudy all supported the PWC and show similar results done with English speakers.\n(McQueen, Otake & Cutler, 2001). The PWC suggests the speech segmentation is\nmore universal than previously devised models. This model suggests that words\nare activated and no particular lexical decision is 100% decided, until all are\ndecided. ",["$","em",null,{"children":"‚ÄúThe present experiments therefore support the theory of lexical\nsegmentation that the PWC offers. On this view, candidate words are activated by\nthe incoming speech stream and compete with each other until a lexical parse is\nsettled upon.‚Äù"}]," (McQueen, Otake & Cutler, 2001). This model also allows for the\nrhythmic segmentation process to exist and help add to the parsing of lexical\nand segmental structure. ",["$","em",null,{"children":"‚ÄúFurthermore, just as the rhythmic structure of\nEnglish or Dutch provides English and Dutch listeners with cues to the location\nof likely word boundaries (Cutler & Norris, 1988, Vroomen et at., 1996), so too\ndoes the characteristic rhythm of Japanese provide Japanese listeners with a\nsegmentation cue.‚Äù"}]," (McQueen, Otake & Cutler, 2001)."]}],"\n",["$","p",null,{"children":"Rhythmic segmentation provides another type of segmentation heuristic along with\nmora-based segmentation as seen in the above papers. Although bilinguals have\ntrouble when inappropriately applying their native language segmentation style\nto a non-native language, rhythmic segmentation is a universal heuristic which\nexists to compensate when L1 segmentation style fails to properly segment the\nnon-native language."}],"\n",["$","h3",null,{"id":"4-segmentation-in-non-native-languages","children":"4. Segmentation in non-native languages"}],"\n",["$","p",null,{"children":"An important area of study concerning bilingual speech segmentation is how\nnon-native speakers segment their second language (L2). If non-native speakers\ncan use any of the non-native language rhythmic, prosodic, lexico-syntatic, and\nsyntactic segmentation processes, then they will have a much easier time\nsegmenting the L2. The ability to learn a non-native segmentation process would\nbe beneficial to any bilingual as it would facilitate comprehension of the L2.\nResearch into brain plasticity for learning non-native language processes would\ndirectly benefit the existing body of knowledge."}],"\n",["$","p",null,{"children":["$b",["$","em",null,{"children":"‚ÄúIf non-native speakers fail\nto use rhythmic segmentation cues other than the rhythmic cues relevant to their\nL1, native speakers of Japanese (mora-timed) and Spanish (syllable-timed) would\nnot be expected to use stress pattern as a segmentation cue when listening to\nEnglish. Alternatively, native Japanese and native Spanish speakers might differ\nin their abilities to use stress pattern as a segmentation cue in English.‚Äù"}],"\n(Sanders et al., 2002). For this experiment they created 5 groups of 3 sentences\neach. The 5 groups were ",["$","em",null,{"children":"‚ÄúStrong stress, initial position (SI), strong stress,\nmedial position (SM), weak stress, initial position (WI), weak stress, medial\nposition (WM), and target absent (TA).‚Äù"}]," (Sanders et al., 2002). Each group\ncontained 3 sentences each of which were a semantic, syntactic or acoustic\ntarget sentence. The semantic words were normal English sentences, the syntactic\nsentences replaced all open-class words with non-words, and the acoustic words\nonly retained the original prosody. Participants were asked to listen to a\ntarget, and then were asked to press one button if that target was heard in the\nbeginning of the sentence, another button if it was heard in the middle, and the\nthird button if it was not heard. Results indicated that ",["$","em",null,{"children":"‚Äúthe fact that both\ngroups of late-learners were able to use the lexical information supports the\nhypothesis that the lexico-semantic system remains relatively plastic beyond the\nage of 12.‚Äù"}]," (Sanders et al., 2002). These results support the idea that late\nlearners can learn non-native language processes later in life, which is a\nreassuring fact for late-bilinguals, however ",["$","em",null,{"children":"‚ÄúNo group of non-native speakers\nused syntactic information to the same extent as native speakers.‚Äù"}]," (Sanders et\nal., 2002). These results, as well as other indicate that syntactic information\nprocesses are not as easily learned later in life (Sanders et al., 2002). An\ninteresting finding was how both early and late learners of English\nJapanese-natives were able to use some segmentation cues that are seemingly\neffective in both English and Japanese; ",["$","em",null,{"children":"‚Äúthis study could either indicate that\nboth groups were applying a Japanese segmentation cue that happens to co-occur\nwith stress in English or that both groups had enough exposure to English to\nlearn a new segmentation cue.‚Äù"}]," (Sanders et al., 2002). Overall the findings of\nthis study show that both lexical and semantic segmentation subsystems retain\nthe ",["$","em",null,{"children":"‚Äúability to change to a greater degree than do syntactic subsystems‚Äù"}],"\n(Sanders et al., 2002)."]}],"\n",["$","p",null,{"children":["In terms of bilinguals, some very reassuring findings from the above study is\nthat some segmentation processes are still learnable later in life, and they are\nalso interchangeable, one or more can be used when another is unavailable or not\napplicable. ",["$","em",null,{"children":"‚ÄúThe findings also indicate that segmentation cues can be used\nflexibly by both native and non-native speakers, such that cues that are both\navailable in the speech stream and usable by the listener are employed to a\ngreater extent when other segmentation cues are either absent or not accessible\nto the listener.‚Äù"}]," (Sanders et al., 2002)."]}],"\n",["$","h2",null,{"id":"iii-possible-directions-for-future-research","children":"III. Possible directions for future research"}],"\n",["$","p",null,{"children":"$c"}],"\n",["$","h2",null,{"id":"bibliography","children":"Bibliography"}],"\n",["$","div",null,{"className":"hanging-indent","children":[["$","cite",null,{"children":["$","p",null,{"children":["Beckman, M. (1982). Segment Duration and the ‚ÄòMora‚Äô in Japanese. ",["$","em",null,{"children":"Phonetica"}],",\n39, 113-135."]}]}],["$","cite",null,{"children":["$","p",null,{"children":["Cutler, A., & Butterfield, S. (1992). Rhythmic cues to speech segmentation:\nEvidence from juncture misperception. ",["$","em",null,{"children":"Journal of Memory and Langauge"}],", 31,\n218-236."]}]}],["$","cite",null,{"children":["$","p",null,{"children":["Cutler, A., Mehler, J., Norris, D., Segui, J., (1986). The Syllable‚Äôs Differing\nRole in the Segmentation of French and English. ",["$","em",null,{"children":"Journal of Memory and\nLanguage"}],", 25, 385-400."]}]}],["$","cite",null,{"children":["$","p",null,{"children":["Cutler, A., & Norris, D. (1988). The role of strong syllables in segmentation\nfor lexical access. ",["$","em",null,{"children":"Journal of Experimental Psychology: Human Perception and\nPerformance"}],", 14, 113-121."]}]}],["$","cite",null,{"children":["$","p",null,{"children":["Cutler, A., Otake, T. (2002). Rhythmic Categories in Spoken-Word Recognition.\n",["$","em",null,{"children":"Journal of Memory and Language"}],", 46, 296-322."]}]}],["$","cite",null,{"children":["$","p",null,{"children":["Cutler, A., Otake, T. (1994). Mora or Phoneme? Further Evidence for\nLangauge-Specific Listening. ",["$","em",null,{"children":"Journal of Memory and Language"}],", 33, 824-844."]}]}],["$","cite",null,{"children":["$","p",null,{"children":["Goto, H. (1971). Auditory perception by normal Japanese adults of the sounds ‚ÄúL‚Äù\nand ‚ÄúR‚Äù. ",["$","em",null,{"children":"Neuropsychologia"}],", 9, 317-323."]}]}],["$","cite",null,{"children":["$","p",null,{"children":["MacKain, K. S., Best, C. T., Strange, W. (1981). Categorical perception of\nEnglish /r/ and /l/ by Japanese bilinguals. ",["$","em",null,{"children":"Applied Psycholinguistics"}],", 2,\n369-390."]}]}],["$","cite",null,{"children":["$","p",null,{"children":["McQueen, M. J., Otake, T., Cutler, A., (2001). Rhythmic Cues and Possible-Word\nConstraints in Japanese Speech Segmentation. ",["$","em",null,{"children":"Journal of Memory and Language"}],",\n45, 103-132."]}]}],["$","cite",null,{"children":["$","p",null,{"children":["Miyawaki, K., Strange, W., Verbrugge, R., Liberman, A. M., Jenkins, J. J.,\nFujimura, O. (1975). An effect of linguistic experience: The discrimination of\n[r] and [l] by native speakers of Japanese and English, ",["$","em",null,{"children":"Perception &\nPsychophysics"}],", 18, 331-340."]}]}],["$","cite",null,{"children":["$","p",null,{"children":["Norris, D. G., McQueen, J. M., Cutler, A., & Butterfield, S. (1997). The\npossible-word constraint in the segmentation of continuous speech. ",["$","em",null,{"children":"Cognitive\nPsychology"}],", 34 191-243"]}]}],["$","cite",null,{"children":["$","p",null,{"children":["Otake, T., Hatano, G., Cutler, A., Mehler, J., (1993) Mora or Syllable? Speech\nSegmentation in Japanese. ",["$","em",null,{"children":"Journal of Memory and Language"}],", 32, 258-278."]}]}],["$","cite",null,{"children":["$","p",null,{"children":["Sanders, L. D., Neville, H. J., Woldorff, M. G., (2002). Speech Segmentation by\nNative and Non-Native Speakers: The Use of Lexical, Syntactic, and\nStress-Pattern Cues. ",["$","em",null,{"children":"Journal of Speech, Language, and Hearing Research"}],", 45,\n519-530."]}]}],["$","cite",null,{"children":["$","p",null,{"children":["Strange, W., Dittmann, S., (1984).\nEffects of discrimination training on the\nperception of /r-l/ by Japanese adults learning English.\n",["$","em",null,{"children":"Perception & Psychophysics"}],",\n36(2), 131-145."]}]}]]}]]}],["$","footer",null,{"className":"license_copyright__EpMnZ","children":["This is licensed under a Creative Commons ","cc-by-nc-sa"," International License"]}]]}]
8:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"Speech Segmentation in Japanese and English Bilinguals"}],["$","meta","3",{"name":"description","content":"The purpose of this paper is to provide an overview of the existing literature concerning speech segmentation, categorical perception, and some other issues concerning bilinguals of English and Japanese"}],["$","link","4",{"rel":"author","href":"https://0xadada.pub"}],["$","meta","5",{"name":"author","content":"0xADADA"}],["$","meta","6",{"name":"keywords","content":"essays,Japanese,sociology"}],["$","link","7",{"rel":"alternate","type":"application/rss+xml","href":"https://0xadada.pub/rss.xml"}],["$","meta","8",{"property":"og:title","content":"Speech Segmentation in Japanese and English Bilinguals"}],["$","meta","9",{"property":"og:description","content":"The purpose of this paper is to provide an overview of the existing literature concerning speech segmentation, categorical perception, and some other issues concerning bilinguals of English and Japanese"}],["$","meta","10",{"property":"og:url","content":"https://0xadada.pub/2003/12/14/speech-segmentation-in-japanese-and-english-bilinguals/"}],["$","meta","11",{"property":"og:site_name","content":"0xADADA"}],["$","meta","12",{"property":"og:locale","content":"en_US"}],["$","meta","13",{"property":"og:image","content":"https://0xadada.pub/static/images/meta/avatar.svg"}],["$","meta","14",{"property":"og:image:width","content":"660"}],["$","meta","15",{"property":"og:image:height","content":"660"}],["$","meta","16",{"property":"og:type","content":"article"}],["$","meta","17",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","18",{"name":"twitter:title","content":"Speech Segmentation in Japanese and English Bilinguals"}],["$","meta","19",{"name":"twitter:description","content":"The purpose of this paper is to provide an overview of the existing literature concerning speech segmentation, categorical perception, and some other issues concerning bilinguals of English and Japanese"}],["$","meta","20",{"name":"twitter:image","content":"https://0xadada.pub/static/images/meta/avatar.svg"}],["$","meta","21",{"name":"twitter:image:width","content":"660"}],["$","meta","22",{"name":"twitter:image:height","content":"660"}],["$","link","23",{"rel":"icon","href":"/favicon.ico","type":"image/x-icon","sizes":"48x48"}],["$","link","24",{"rel":"apple-touch-icon","href":"/apple-icon.png?b764b3a1dbf00a82","type":"image/png","sizes":"180x180"}]]
1:null
